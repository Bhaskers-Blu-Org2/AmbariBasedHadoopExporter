{
  "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/",
  "Clusters" : {
    "cluster_id" : 2,
    "cluster_name" : "sparkpoc",
    "health_report" : {
      "Host/stale_config" : 1,
      "Host/maintenance_state" : 6,
      "Host/host_state/HEALTHY" : 7,
      "Host/host_state/UNHEALTHY" : 0,
      "Host/host_state/HEARTBEAT_LOST" : 0,
      "Host/host_state/INIT" : 0,
      "Host/host_status/HEALTHY" : 7,
      "Host/host_status/UNHEALTHY" : 0,
      "Host/host_status/UNKNOWN" : 0,
      "Host/host_status/ALERT" : 0
    },
    "provisioning_state" : "INSTALLED",
    "security_type" : "NONE",
    "total_hosts" : 7,
    "version" : "HDP-2.6",
    "credential_store_properties" : {
      "storage.persistent" : "false",
      "storage.temporary" : "true"
    },
    "desired_configs" : {
      "ams-env" : {
        "tag" : "version1547647211880",
        "version" : 6
      },
      "ams-grafana-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ams-grafana-ini" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ams-hbase-env" : {
        "tag" : "version1547647211881",
        "version" : 3
      },
      "ams-hbase-log4j" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ams-hbase-policy" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ams-hbase-security-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ams-hbase-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ams-log4j" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ams-logsearch-conf" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ams-site" : {
        "tag" : "version1547648902632",
        "version" : 11
      },
      "ams-ssl-client" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ams-ssl-server" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "beeline-log4j2" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "capacity-scheduler" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "cluster-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "core-site" : {
        "tag" : "iocachedeactivateversion20180829144422",
        "version" : 3
      },
      "hadoop-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hadoop-metrics2.properties" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hadoop-policy" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hcat-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hdfs-log4j" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hdfs-logsearch-conf" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hdfs-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hive-atlas-application.properties" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hive-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hive-exec-log4j" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hive-exec-log4j2" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hive-interactive-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hive-interactive-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hive-log4j" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hive-log4j2" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hive-logsearch-conf" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hive-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hivemetastore-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hiveserver2-interactive-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "hiveserver2-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "iocache-bks-log4j-properties" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "iocache-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "iocache-lds-log4j-properties" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "jupyter-site" : {
        "tag" : "INITIAL",
        "version" : 1
      },
      "livy2-conf" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "livy2-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "livy2-log4j-properties" : {
        "tag" : "version1551265682812",
        "version" : 4
      },
      "livy2-spark-blacklist" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "llap-cli-log4j2" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "llap-daemon-log4j" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "mapred-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "mapred-logsearch-conf" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "mapred-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "oozie-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "oozie-log4j" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "oozie-logsearch-conf" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "oozie-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "parquet-logging" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "pig-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "pig-log4j" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "pig-properties" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-hdfs-audit" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-hdfs-plugin-properties" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-hdfs-policymgr-ssl" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-hdfs-security" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-hive-audit" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-hive-plugin-properties" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-hive-policymgr-ssl" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-hive-security" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-yarn-audit" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-yarn-plugin-properties" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-yarn-policymgr-ssl" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ranger-yarn-security" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "spark2-defaults" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "spark2-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "spark2-hive-site-override" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "spark2-log4j-properties" : {
        "tag" : "version1551266302458",
        "version" : 13
      },
      "spark2-logsearch-conf" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "spark2-metrics-properties" : {
        "tag" : "version1551196373065",
        "version" : 22
      },
      "spark2-thrift-fairscheduler" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "spark2-thrift-sparkconf" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "sqoop-atlas-application.properties" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "sqoop-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "sqoop-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ssl-client" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "ssl-server" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "tez-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "tez-interactive-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "tez-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "webhcat-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "webhcat-log4j" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "webhcat-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "yarn-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "yarn-log4j" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "yarn-logsearch-conf" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "yarn-site" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "zeppelin-config" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "zeppelin-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "zeppelin-log4j-properties" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "zeppelin-logsearch-conf" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "zeppelin-shiro-ini" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "zoo.cfg" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "zookeeper-env" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "zookeeper-log4j" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      },
      "zookeeper-logsearch-conf" : {
        "tag" : "TOPOLOGY_RESOLVED",
        "version" : 2
      }
    },
    "desired_service_config_versions" : {
      "AMBARI_METRICS" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "AMBARI_METRICS",
          "service_config_version" : 16,
          "createtime" : 1547648902817,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "admin",
          "service_config_version_note" : "Removing HDI",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "HDFS" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "HDFS",
          "service_config_version" : 3,
          "createtime" : 1535553862661,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "hdinsightwatchdog",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "HIVE" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "HIVE",
          "service_config_version" : 2,
          "createtime" : 1535553552935,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "internal",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "IOCACHE" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "IOCACHE",
          "service_config_version" : 2,
          "createtime" : 1535553554623,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "internal",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "JUPYTER" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "JUPYTER",
          "service_config_version" : 1,
          "createtime" : 1535553516179,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "hdinsightwatchdog",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "MAPREDUCE2" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "MAPREDUCE2",
          "service_config_version" : 2,
          "createtime" : 1535553549661,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "internal",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "OOZIE" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "OOZIE",
          "service_config_version" : 2,
          "createtime" : 1535553548271,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "internal",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "PIG" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "PIG",
          "service_config_version" : 2,
          "createtime" : 1535553550663,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "internal",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "SPARK2" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "SPARK2",
          "service_config_version" : 34,
          "createtime" : 1551266302657,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "admin",
          "service_config_version_note" : "Updating to WARN (wdatp loggeR)",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        },
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "SPARK2",
          "service_config_version" : 3,
          "createtime" : 1535610629472,
          "group_id" : 2,
          "group_name" : "overridetry",
          "user" : "admin",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "SQOOP" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "SQOOP",
          "service_config_version" : 2,
          "createtime" : 1535553547995,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "internal",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "TEZ" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "TEZ",
          "service_config_version" : 2,
          "createtime" : 1535553549485,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "internal",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "YARN" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "YARN",
          "service_config_version" : 2,
          "createtime" : 1535553550470,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "internal",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "ZEPPELIN" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "ZEPPELIN",
          "service_config_version" : 2,
          "createtime" : 1535553554453,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "internal",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ],
      "ZOOKEEPER" : [
        {
          "hosts" : [ ],
          "compatibleWithCurrentStack" : true,
          "cluster_name" : "sparkpoc",
          "service_name" : "ZOOKEEPER",
          "service_config_version" : 2,
          "createtime" : 1535553549890,
          "group_id" : -1,
          "group_name" : "Default",
          "user" : "internal",
          "stack_id" : "HDP-2.6",
          "is_current" : true,
          "is_cluster_compatible" : true
        }
      ]
    }
  },
  "alerts_summary" : {
    "CRITICAL" : 2,
    "MAINTENANCE" : 55,
    "OK" : 78,
    "UNKNOWN" : 2,
    "WARNING" : 0
  },
  "alerts_summary_hosts" : {
    "CRITICAL" : 1,
    "OK" : 6,
    "UNKNOWN" : 0,
    "WARNING" : 0
  },
  "privileges" : [ ],
  "config_groups" : [
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/config_groups/2",
      "ConfigGroup" : {
        "cluster_name" : "sparkpoc",
        "id" : 2
      }
    }
  ],
  "kerberos_descriptors" : [
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/kerberos_descriptors/COMPOSITE",
      "KerberosDescriptor" : {
        "cluster_name" : "sparkpoc",
        "type" : "COMPOSITE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/kerberos_descriptors/STACK",
      "KerberosDescriptor" : {
        "cluster_name" : "sparkpoc",
        "type" : "STACK"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/kerberos_descriptors/USER",
      "KerberosDescriptor" : {
        "cluster_name" : "sparkpoc",
        "type" : "USER"
      }
    }
  ],
  "hosts" : [
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/hosts/hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
      "Hosts" : {
        "cluster_name" : "sparkpoc",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/hosts/hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
      "Hosts" : {
        "cluster_name" : "sparkpoc",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/hosts/wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
      "Hosts" : {
        "cluster_name" : "sparkpoc",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/hosts/wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
      "Hosts" : {
        "cluster_name" : "sparkpoc",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/hosts/zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
      "Hosts" : {
        "cluster_name" : "sparkpoc",
        "host_name" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/hosts/zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
      "Hosts" : {
        "cluster_name" : "sparkpoc",
        "host_name" : "zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/hosts/zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
      "Hosts" : {
        "cluster_name" : "sparkpoc",
        "host_name" : "zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net"
      }
    }
  ],
  "configurations" : [
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-env&tag=version1547566054222",
      "tag" : "version1547566054222",
      "type" : "ams-env",
      "version" : 3,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-env&tag=version1547566893452",
      "tag" : "version1547566893452",
      "type" : "ams-env",
      "version" : 4,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-env&tag=version1547567058886",
      "tag" : "version1547567058886",
      "type" : "ams-env",
      "version" : 5,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-env&tag=version1547647211880",
      "tag" : "version1547647211880",
      "type" : "ams-env",
      "version" : 6,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-grafana-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-grafana-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-grafana-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-grafana-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-grafana-ini&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-grafana-ini",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-grafana-ini&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-grafana-ini",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-hbase-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-hbase-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-hbase-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-hbase-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-hbase-env&tag=version1547647211881",
      "tag" : "version1547647211881",
      "type" : "ams-hbase-env",
      "version" : 3,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-hbase-log4j&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-hbase-log4j",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-hbase-log4j&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-hbase-log4j",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-hbase-policy&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-hbase-policy",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-hbase-policy&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-hbase-policy",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-hbase-security-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-hbase-security-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-hbase-security-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-hbase-security-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-hbase-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-hbase-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-hbase-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-hbase-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-log4j&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-log4j",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-log4j&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-log4j",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-logsearch-conf&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-logsearch-conf",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-logsearch-conf&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-logsearch-conf",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-site&tag=version1547129087042",
      "tag" : "version1547129087042",
      "type" : "ams-site",
      "version" : 3,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-site&tag=version1547393896244",
      "tag" : "version1547393896244",
      "type" : "ams-site",
      "version" : 4,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-site&tag=version1547564509034",
      "tag" : "version1547564509034",
      "type" : "ams-site",
      "version" : 5,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-site&tag=version1547566424767",
      "tag" : "version1547566424767",
      "type" : "ams-site",
      "version" : 6,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-site&tag=version1547567058887",
      "tag" : "version1547567058887",
      "type" : "ams-site",
      "version" : 7,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-site&tag=version1547642491417",
      "tag" : "version1547642491417",
      "type" : "ams-site",
      "version" : 8,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-site&tag=version1547647623968",
      "tag" : "version1547647623968",
      "type" : "ams-site",
      "version" : 9,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-site&tag=version1547648616845",
      "tag" : "version1547648616845",
      "type" : "ams-site",
      "version" : 10,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-site&tag=version1547648902632",
      "tag" : "version1547648902632",
      "type" : "ams-site",
      "version" : 11,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-ssl-client&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-ssl-client",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-ssl-client&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-ssl-client",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-ssl-server&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ams-ssl-server",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ams-ssl-server&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ams-ssl-server",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=beeline-log4j2&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "beeline-log4j2",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=beeline-log4j2&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "beeline-log4j2",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=capacity-scheduler&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "capacity-scheduler",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=capacity-scheduler&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "capacity-scheduler",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=cluster-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "cluster-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=cluster-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "cluster-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=core-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "core-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=core-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "core-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=core-site&tag=iocachedeactivateversion20180829144422",
      "tag" : "iocachedeactivateversion20180829144422",
      "type" : "core-site",
      "version" : 3,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=global&tag=iocachedeactivateversion20180829144422",
      "tag" : "iocachedeactivateversion20180829144422",
      "type" : "global",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hadoop-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hadoop-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hadoop-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hadoop-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hadoop-metrics2.properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hadoop-metrics2.properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hadoop-metrics2.properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hadoop-metrics2.properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hadoop-policy&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hadoop-policy",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hadoop-policy&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hadoop-policy",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hcat-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hcat-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hcat-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hcat-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hdfs-log4j&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hdfs-log4j",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hdfs-log4j&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hdfs-log4j",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hdfs-logsearch-conf&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hdfs-logsearch-conf",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hdfs-logsearch-conf&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hdfs-logsearch-conf",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hdfs-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hdfs-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hdfs-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hdfs-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-atlas-application.properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hive-atlas-application.properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-atlas-application.properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hive-atlas-application.properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hive-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hive-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-exec-log4j&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hive-exec-log4j",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-exec-log4j&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hive-exec-log4j",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-exec-log4j2&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hive-exec-log4j2",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-exec-log4j2&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hive-exec-log4j2",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-interactive-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hive-interactive-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-interactive-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hive-interactive-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-interactive-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hive-interactive-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-interactive-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hive-interactive-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-log4j&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hive-log4j",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-log4j&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hive-log4j",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-log4j2&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hive-log4j2",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-log4j2&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hive-log4j2",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-logsearch-conf&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hive-logsearch-conf",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-logsearch-conf&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hive-logsearch-conf",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hive-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hive-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hive-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hivemetastore-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hivemetastore-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hivemetastore-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hivemetastore-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hiveserver2-interactive-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hiveserver2-interactive-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hiveserver2-interactive-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hiveserver2-interactive-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hiveserver2-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "hiveserver2-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=hiveserver2-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "hiveserver2-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=iocache-bks-log4j-properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "iocache-bks-log4j-properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=iocache-bks-log4j-properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "iocache-bks-log4j-properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=iocache-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "iocache-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=iocache-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "iocache-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=iocache-lds-log4j-properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "iocache-lds-log4j-properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=iocache-lds-log4j-properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "iocache-lds-log4j-properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=jupyter-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "jupyter-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=livy2-conf&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "livy2-conf",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=livy2-conf&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "livy2-conf",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=livy2-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "livy2-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=livy2-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "livy2-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=livy2-log4j-properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "livy2-log4j-properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=livy2-log4j-properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "livy2-log4j-properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=livy2-log4j-properties&tag=version1551256191921",
      "tag" : "version1551256191921",
      "type" : "livy2-log4j-properties",
      "version" : 3,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=livy2-log4j-properties&tag=version1551265682812",
      "tag" : "version1551265682812",
      "type" : "livy2-log4j-properties",
      "version" : 4,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=livy2-spark-blacklist&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "livy2-spark-blacklist",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=livy2-spark-blacklist&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "livy2-spark-blacklist",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=llap-cli-log4j2&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "llap-cli-log4j2",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=llap-cli-log4j2&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "llap-cli-log4j2",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=llap-daemon-log4j&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "llap-daemon-log4j",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=llap-daemon-log4j&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "llap-daemon-log4j",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=mapred-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "mapred-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=mapred-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "mapred-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=mapred-logsearch-conf&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "mapred-logsearch-conf",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=mapred-logsearch-conf&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "mapred-logsearch-conf",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=mapred-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "mapred-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=mapred-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "mapred-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=oozie-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "oozie-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=oozie-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "oozie-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=oozie-log4j&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "oozie-log4j",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=oozie-log4j&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "oozie-log4j",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=oozie-logsearch-conf&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "oozie-logsearch-conf",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=oozie-logsearch-conf&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "oozie-logsearch-conf",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=oozie-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "oozie-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=oozie-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "oozie-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=parquet-logging&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "parquet-logging",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=parquet-logging&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "parquet-logging",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=pig-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "pig-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=pig-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "pig-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=pig-log4j&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "pig-log4j",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=pig-log4j&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "pig-log4j",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=pig-properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "pig-properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=pig-properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "pig-properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hdfs-audit&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-hdfs-audit",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hdfs-audit&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-hdfs-audit",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hdfs-plugin-properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-hdfs-plugin-properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hdfs-plugin-properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-hdfs-plugin-properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hdfs-policymgr-ssl&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-hdfs-policymgr-ssl",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hdfs-policymgr-ssl&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-hdfs-policymgr-ssl",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hdfs-security&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-hdfs-security",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hdfs-security&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-hdfs-security",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hive-audit&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-hive-audit",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hive-audit&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-hive-audit",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hive-plugin-properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-hive-plugin-properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hive-plugin-properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-hive-plugin-properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hive-policymgr-ssl&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-hive-policymgr-ssl",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hive-policymgr-ssl&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-hive-policymgr-ssl",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hive-security&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-hive-security",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-hive-security&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-hive-security",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-yarn-audit&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-yarn-audit",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-yarn-audit&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-yarn-audit",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-yarn-plugin-properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-yarn-plugin-properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-yarn-plugin-properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-yarn-plugin-properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-yarn-policymgr-ssl&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-yarn-policymgr-ssl",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-yarn-policymgr-ssl&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-yarn-policymgr-ssl",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-yarn-security&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ranger-yarn-security",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ranger-yarn-security&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ranger-yarn-security",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-defaults&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "spark2-defaults",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-defaults&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "spark2-defaults",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "spark2-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "spark2-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-hive-site-override&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "spark2-hive-site-override",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-hive-site-override&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "spark2-hive-site-override",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "spark2-log4j-properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "spark2-log4j-properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=version1543940937418",
      "tag" : "version1543940937418",
      "type" : "spark2-log4j-properties",
      "version" : 3,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=version1543947192130",
      "tag" : "version1543947192130",
      "type" : "spark2-log4j-properties",
      "version" : 4,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=version1543948303421",
      "tag" : "version1543948303421",
      "type" : "spark2-log4j-properties",
      "version" : 5,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=version1545641441013",
      "tag" : "version1545641441013",
      "type" : "spark2-log4j-properties",
      "version" : 6,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=version1550505579087",
      "tag" : "version1550505579087",
      "type" : "spark2-log4j-properties",
      "version" : 7,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=version1551196433465",
      "tag" : "version1551196433465",
      "type" : "spark2-log4j-properties",
      "version" : 8,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=version1551248583866",
      "tag" : "version1551248583866",
      "type" : "spark2-log4j-properties",
      "version" : 9,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=version1551256191922",
      "tag" : "version1551256191922",
      "type" : "spark2-log4j-properties",
      "version" : 10,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=version1551265471478",
      "tag" : "version1551265471478",
      "type" : "spark2-log4j-properties",
      "version" : 11,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=version1551265682813",
      "tag" : "version1551265682813",
      "type" : "spark2-log4j-properties",
      "version" : 12,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-log4j-properties&tag=version1551266302458",
      "tag" : "version1551266302458",
      "type" : "spark2-log4j-properties",
      "version" : 13,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-logsearch-conf&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "spark2-logsearch-conf",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-logsearch-conf&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "spark2-logsearch-conf",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "spark2-metrics-properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "spark2-metrics-properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1538660319758",
      "tag" : "version1538660319758",
      "type" : "spark2-metrics-properties",
      "version" : 3,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1538663917348",
      "tag" : "version1538663917348",
      "type" : "spark2-metrics-properties",
      "version" : 4,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1538664301170",
      "tag" : "version1538664301170",
      "type" : "spark2-metrics-properties",
      "version" : 5,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1538664638666",
      "tag" : "version1538664638666",
      "type" : "spark2-metrics-properties",
      "version" : 6,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1538665066871",
      "tag" : "version1538665066871",
      "type" : "spark2-metrics-properties",
      "version" : 7,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1538912530677",
      "tag" : "version1538912530677",
      "type" : "spark2-metrics-properties",
      "version" : 8,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1538920750443",
      "tag" : "version1538920750443",
      "type" : "spark2-metrics-properties",
      "version" : 9,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1539178670041",
      "tag" : "version1539178670041",
      "type" : "spark2-metrics-properties",
      "version" : 10,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1541919413205",
      "tag" : "version1541919413205",
      "type" : "spark2-metrics-properties",
      "version" : 11,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1541923016001",
      "tag" : "version1541923016001",
      "type" : "spark2-metrics-properties",
      "version" : 12,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1541923927288",
      "tag" : "version1541923927288",
      "type" : "spark2-metrics-properties",
      "version" : 13,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1542715161034",
      "tag" : "version1542715161034",
      "type" : "spark2-metrics-properties",
      "version" : 14,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1542897487233",
      "tag" : "version1542897487233",
      "type" : "spark2-metrics-properties",
      "version" : 15,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1543239907706",
      "tag" : "version1543239907706",
      "type" : "spark2-metrics-properties",
      "version" : 16,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1545560444568",
      "tag" : "version1545560444568",
      "type" : "spark2-metrics-properties",
      "version" : 17,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1545563690124",
      "tag" : "version1545563690124",
      "type" : "spark2-metrics-properties",
      "version" : 18,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1545567225415",
      "tag" : "version1545567225415",
      "type" : "spark2-metrics-properties",
      "version" : 19,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1545667159755",
      "tag" : "version1545667159755",
      "type" : "spark2-metrics-properties",
      "version" : 20,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1545819857213",
      "tag" : "version1545819857213",
      "type" : "spark2-metrics-properties",
      "version" : 21,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-metrics-properties&tag=version1551196373065",
      "tag" : "version1551196373065",
      "type" : "spark2-metrics-properties",
      "version" : 22,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-thrift-fairscheduler&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "spark2-thrift-fairscheduler",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-thrift-fairscheduler&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "spark2-thrift-fairscheduler",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-thrift-sparkconf&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "spark2-thrift-sparkconf",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=spark2-thrift-sparkconf&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "spark2-thrift-sparkconf",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=sqoop-atlas-application.properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "sqoop-atlas-application.properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=sqoop-atlas-application.properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "sqoop-atlas-application.properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=sqoop-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "sqoop-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=sqoop-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "sqoop-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=sqoop-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "sqoop-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=sqoop-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "sqoop-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ssl-client&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ssl-client",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ssl-client&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ssl-client",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ssl-server&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "ssl-server",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=ssl-server&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "ssl-server",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=tez-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "tez-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=tez-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "tez-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=tez-interactive-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "tez-interactive-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=tez-interactive-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "tez-interactive-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=tez-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "tez-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=tez-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "tez-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=webhcat-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "webhcat-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=webhcat-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "webhcat-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=webhcat-log4j&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "webhcat-log4j",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=webhcat-log4j&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "webhcat-log4j",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=webhcat-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "webhcat-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=webhcat-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "webhcat-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=yarn-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "yarn-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=yarn-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "yarn-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=yarn-log4j&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "yarn-log4j",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=yarn-log4j&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "yarn-log4j",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=yarn-logsearch-conf&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "yarn-logsearch-conf",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=yarn-logsearch-conf&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "yarn-logsearch-conf",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=yarn-site&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "yarn-site",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=yarn-site&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "yarn-site",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zeppelin-config&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "zeppelin-config",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zeppelin-config&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "zeppelin-config",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zeppelin-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "zeppelin-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zeppelin-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "zeppelin-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zeppelin-log4j-properties&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "zeppelin-log4j-properties",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zeppelin-log4j-properties&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "zeppelin-log4j-properties",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zeppelin-logsearch-conf&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "zeppelin-logsearch-conf",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zeppelin-logsearch-conf&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "zeppelin-logsearch-conf",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zeppelin-shiro-ini&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "zeppelin-shiro-ini",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zeppelin-shiro-ini&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "zeppelin-shiro-ini",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zoo.cfg&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "zoo.cfg",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zoo.cfg&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "zoo.cfg",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zookeeper-env&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "zookeeper-env",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zookeeper-env&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "zookeeper-env",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zookeeper-log4j&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "zookeeper-log4j",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zookeeper-log4j&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "zookeeper-log4j",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zookeeper-logsearch-conf&tag=INITIAL",
      "tag" : "INITIAL",
      "type" : "zookeeper-logsearch-conf",
      "version" : 1,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations?type=zookeeper-logsearch-conf&tag=TOPOLOGY_RESOLVED",
      "tag" : "TOPOLOGY_RESOLVED",
      "type" : "zookeeper-logsearch-conf",
      "version" : 2,
      "Config" : {
        "cluster_name" : "sparkpoc",
        "stack_id" : "HDP-2.6"
      }
    }
  ],
  "services" : [
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/AMBARI_METRICS",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/HDFS",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/HIVE",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "HIVE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/IOCACHE",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "IOCACHE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/JUPYTER",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "JUPYTER"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/MAPREDUCE2",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "MAPREDUCE2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/OOZIE",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "OOZIE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/PIG",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "PIG"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/SPARK2",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "SPARK2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/SQOOP",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "SQOOP"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/TEZ",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "TEZ"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/YARN",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/ZEPPELIN",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "ZEPPELIN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/services/ZOOKEEPER",
      "ServiceInfo" : {
        "cluster_name" : "sparkpoc",
        "service_name" : "ZOOKEEPER"
      }
    }
  ],
  "workflows" : [ ],
  "requests" : [
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/188",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 188
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/189",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 189
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/190",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 190
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/191",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 191
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/192",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 192
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/193",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 193
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/194",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 194
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/195",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 195
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/196",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 196
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/197",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 197
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/198",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 198
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/199",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 199
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/200",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 200
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/201",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 201
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/202",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 202
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/203",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 203
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/204",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 204
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/205",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 205
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/206",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 206
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/requests/207",
      "Requests" : {
        "cluster_name" : "sparkpoc",
        "id" : 207
      }
    }
  ],
  "alerts" : [
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/123",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 75,
        "definition_name" : "ambari_server_stale_alerts",
        "host_name" : null,
        "id" : 123,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/124",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 74,
        "definition_name" : "ambari_server_component_version",
        "host_name" : null,
        "id" : 124,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/125",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 73,
        "definition_name" : "ambari_server_performance",
        "host_name" : null,
        "id" : 125,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/24",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 59,
        "definition_name" : "metrics_monitor_process_percent",
        "host_name" : null,
        "id" : 24,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/26",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 20,
        "definition_name" : "journalnode_process_percent",
        "host_name" : null,
        "id" : 26,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/35",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 7,
        "definition_name" : "namenode_ha_health",
        "host_name" : null,
        "id" : 35,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/71",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 33,
        "definition_name" : "datanode_process_percent",
        "host_name" : null,
        "id" : 71,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/73",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 23,
        "definition_name" : "datanode_storage_percent",
        "host_name" : null,
        "id" : 73,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/72",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 42,
        "definition_name" : "yarn_nodemanager_webui_percent",
        "host_name" : null,
        "id" : 72,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/25",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 1,
        "definition_name" : "zookeeper_server_process_percent",
        "host_name" : null,
        "id" : 25,
        "service_name" : "ZOOKEEPER"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/3",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 71,
        "definition_name" : "ambari_agent_disk_usage",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 3,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/8",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 72,
        "definition_name" : "ambari_server_agent_heartbeat",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 8,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/19",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 70,
        "definition_name" : "ambari_agent_version_select",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 19,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/81",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 56,
        "definition_name" : "ams_metrics_collector_process",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 81,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/85",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 60,
        "definition_name" : "ams_metrics_collector_autostart",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 85,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/99",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 58,
        "definition_name" : "ams_metrics_collector_hbase_master_cpu",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 99,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/105",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 55,
        "definition_name" : "ams_metrics_monitor_process",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 105,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/107",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 57,
        "definition_name" : "ams_metrics_collector_hbase_master_process",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 107,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/75",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 15,
        "definition_name" : "namenode_webui",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 75,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/76",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 25,
        "definition_name" : "upgrade_finalized_state",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 76,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/77",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 14,
        "definition_name" : "namenode_hdfs_blocks_health",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 77,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/78",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 34,
        "definition_name" : "namenode_hdfs_capacity_utilization",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 78,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/79",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 35,
        "definition_name" : "namenode_rpc_latency",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 79,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/83",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 5,
        "definition_name" : "namenode_hdfs_pending_deletion_blocks",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 83,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/87",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 3,
        "definition_name" : "namenode_cpu",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 87,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/94",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 29,
        "definition_name" : "hdfs_zookeeper_failover_controller_process",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 94,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/96",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 36,
        "definition_name" : "namenode_last_checkpoint",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 96,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/97",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 9,
        "definition_name" : "datanode_health_summary",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 97,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/109",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 30,
        "definition_name" : "namenode_directory_status",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 109,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/127",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 37,
        "definition_name" : "increase_nn_heap_usage_daily",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 127,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/129",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 32,
        "definition_name" : "increase_nn_heap_usage_weekly",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 129,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/132",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 26,
        "definition_name" : "namenode_client_rpc_processing_latency_hourly",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 132,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/133",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 22,
        "definition_name" : "namenode_client_rpc_queue_latency_hourly",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 133,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/147",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 13,
        "definition_name" : "namenode_client_rpc_processing_latency_daily",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 147,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/148",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 6,
        "definition_name" : "namenode_client_rpc_queue_latency_daily",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 148,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/149",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 21,
        "definition_name" : "namenode_increase_in_storage_capacity_usage_daily",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 149,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/150",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 27,
        "definition_name" : "namenode_increase_in_storage_capacity_usage_weekly",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 150,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/98",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 54,
        "definition_name" : "hive_webhcat_server_status",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 98,
        "service_name" : "HIVE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/121",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 53,
        "definition_name" : "hive_server_process",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 121,
        "service_name" : "HIVE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/122",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 50,
        "definition_name" : "hive_metastore_process",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 122,
        "service_name" : "HIVE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/100",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 62,
        "definition_name" : "cache_data_transfer_server_status",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 100,
        "service_name" : "IOCACHE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/103",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 63,
        "definition_name" : "cache_metadata_server_status",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 103,
        "service_name" : "IOCACHE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/80",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 66,
        "definition_name" : "mapreduce_history_server_process",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 80,
        "service_name" : "MAPREDUCE2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/86",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 69,
        "definition_name" : "mapreduce_history_server_cpu",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 86,
        "service_name" : "MAPREDUCE2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/89",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 68,
        "definition_name" : "mapreduce_history_server_rpc_latency",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 89,
        "service_name" : "MAPREDUCE2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/95",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 67,
        "definition_name" : "mapreduce_history_server_webui",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 95,
        "service_name" : "MAPREDUCE2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/91",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 64,
        "definition_name" : "oozie_server_webui",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 91,
        "service_name" : "OOZIE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/114",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 65,
        "definition_name" : "oozie_server_status",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 114,
        "service_name" : "OOZIE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/102",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 39,
        "definition_name" : "SPARK2_JOBHISTORYSERVER_PROCESS",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 102,
        "service_name" : "SPARK2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/111",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 38,
        "definition_name" : "livy2_server_status",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 111,
        "service_name" : "SPARK2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/120",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 40,
        "definition_name" : "spark2_thriftserver_status",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 120,
        "service_name" : "SPARK2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/82",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 44,
        "definition_name" : "yarn_resourcemanager_webui",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 82,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/84",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 47,
        "definition_name" : "yarn_resourcemanager_rpc_latency",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 84,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/90",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 45,
        "definition_name" : "yarn_resourcemanager_cpu",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 90,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/92",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 48,
        "definition_name" : "nodemanager_health_summary",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 92,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/93",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 49,
        "definition_name" : "yarn_app_timeline_server_webui",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 93,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/88",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 41,
        "definition_name" : "zeppelin_server_status",
        "host_name" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 88,
        "service_name" : "ZEPPELIN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/1",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 71,
        "definition_name" : "ambari_agent_disk_usage",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 1,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/11",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 72,
        "definition_name" : "ambari_server_agent_heartbeat",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 11,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/16",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 70,
        "definition_name" : "ambari_agent_version_select",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 16,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/52",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 55,
        "definition_name" : "ams_metrics_monitor_process",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 52,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/27",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 15,
        "definition_name" : "namenode_webui",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 27,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/29",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 25,
        "definition_name" : "upgrade_finalized_state",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 29,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/31",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 14,
        "definition_name" : "namenode_hdfs_blocks_health",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 31,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/33",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 34,
        "definition_name" : "namenode_hdfs_capacity_utilization",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 33,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/37",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 35,
        "definition_name" : "namenode_rpc_latency",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 37,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/39",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 5,
        "definition_name" : "namenode_hdfs_pending_deletion_blocks",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 39,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/41",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 3,
        "definition_name" : "namenode_cpu",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 41,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/46",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 29,
        "definition_name" : "hdfs_zookeeper_failover_controller_process",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 46,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/47",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 36,
        "definition_name" : "namenode_last_checkpoint",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 47,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/48",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 9,
        "definition_name" : "datanode_health_summary",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 48,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/53",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 30,
        "definition_name" : "namenode_directory_status",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 53,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/126",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 37,
        "definition_name" : "increase_nn_heap_usage_daily",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 126,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/128",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 32,
        "definition_name" : "increase_nn_heap_usage_weekly",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 128,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/130",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 26,
        "definition_name" : "namenode_client_rpc_processing_latency_hourly",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 130,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/131",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 22,
        "definition_name" : "namenode_client_rpc_queue_latency_hourly",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 131,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/30",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 54,
        "definition_name" : "hive_webhcat_server_status",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 30,
        "service_name" : "HIVE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/101",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 53,
        "definition_name" : "hive_server_process",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 101,
        "service_name" : "HIVE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/119",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 50,
        "definition_name" : "hive_metastore_process",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 119,
        "service_name" : "HIVE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/49",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 62,
        "definition_name" : "cache_data_transfer_server_status",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 49,
        "service_name" : "IOCACHE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/51",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 63,
        "definition_name" : "cache_metadata_server_status",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 51,
        "service_name" : "IOCACHE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/34",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 67,
        "definition_name" : "mapreduce_history_server_webui",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 34,
        "service_name" : "MAPREDUCE2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/38",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 66,
        "definition_name" : "mapreduce_history_server_process",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 38,
        "service_name" : "MAPREDUCE2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/40",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 69,
        "definition_name" : "mapreduce_history_server_cpu",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 40,
        "service_name" : "MAPREDUCE2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/42",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 68,
        "definition_name" : "mapreduce_history_server_rpc_latency",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 42,
        "service_name" : "MAPREDUCE2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/32",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 65,
        "definition_name" : "oozie_server_status",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 32,
        "service_name" : "OOZIE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/36",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 64,
        "definition_name" : "oozie_server_webui",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 36,
        "service_name" : "OOZIE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/74",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 40,
        "definition_name" : "spark2_thriftserver_status",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 74,
        "service_name" : "SPARK2"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/28",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 44,
        "definition_name" : "yarn_resourcemanager_webui",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 28,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/43",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 45,
        "definition_name" : "yarn_resourcemanager_cpu",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 43,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/44",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 48,
        "definition_name" : "nodemanager_health_summary",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 44,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/45",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 49,
        "definition_name" : "yarn_app_timeline_server_webui",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 45,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/50",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 47,
        "definition_name" : "yarn_resourcemanager_rpc_latency",
        "host_name" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 50,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/6",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 71,
        "definition_name" : "ambari_agent_disk_usage",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 6,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/12",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 72,
        "definition_name" : "ambari_server_agent_heartbeat",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 12,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/21",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 70,
        "definition_name" : "ambari_agent_version_select",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 21,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/116",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 55,
        "definition_name" : "ams_metrics_monitor_process",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 116,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/104",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 10,
        "definition_name" : "datanode_unmounted_data_dir",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 104,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/108",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 12,
        "definition_name" : "datanode_process",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 108,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/110",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 17,
        "definition_name" : "datanode_storage",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 110,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/115",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 16,
        "definition_name" : "datanode_webui",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 115,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/118",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 8,
        "definition_name" : "datanode_heap_usage",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 118,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/106",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 62,
        "definition_name" : "cache_data_transfer_server_status",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 106,
        "service_name" : "IOCACHE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/113",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 63,
        "definition_name" : "cache_metadata_server_status",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 113,
        "service_name" : "IOCACHE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/112",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 46,
        "definition_name" : "yarn_nodemanager_webui",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 112,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/117",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 43,
        "definition_name" : "yarn_nodemanager_health",
        "host_name" : "wn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 117,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/134",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 71,
        "definition_name" : "ambari_agent_disk_usage",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 134,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/135",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 72,
        "definition_name" : "ambari_server_agent_heartbeat",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 135,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/137",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 70,
        "definition_name" : "ambari_agent_version_select",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 137,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/144",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 55,
        "definition_name" : "ams_metrics_monitor_process",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 144,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/136",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 10,
        "definition_name" : "datanode_unmounted_data_dir",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 136,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/139",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 12,
        "definition_name" : "datanode_process",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 139,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/140",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 17,
        "definition_name" : "datanode_storage",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 140,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/143",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 16,
        "definition_name" : "datanode_webui",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 143,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/146",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 8,
        "definition_name" : "datanode_heap_usage",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 146,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/138",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 62,
        "definition_name" : "cache_data_transfer_server_status",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 138,
        "service_name" : "IOCACHE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/142",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 63,
        "definition_name" : "cache_metadata_server_status",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 142,
        "service_name" : "IOCACHE"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/141",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 46,
        "definition_name" : "yarn_nodemanager_webui",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 141,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/145",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 43,
        "definition_name" : "yarn_nodemanager_health",
        "host_name" : "wn4-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 145,
        "service_name" : "YARN"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/2",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 71,
        "definition_name" : "ambari_agent_disk_usage",
        "host_name" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 2,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/13",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 72,
        "definition_name" : "ambari_server_agent_heartbeat",
        "host_name" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 13,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/22",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 70,
        "definition_name" : "ambari_agent_version_select",
        "host_name" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 22,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/18",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 55,
        "definition_name" : "ams_metrics_monitor_process",
        "host_name" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 18,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/20",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 28,
        "definition_name" : "journalnode_process",
        "host_name" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 20,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/23",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 2,
        "definition_name" : "zookeeper_server_process",
        "host_name" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 23,
        "service_name" : "ZOOKEEPER"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/4",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 71,
        "definition_name" : "ambari_agent_disk_usage",
        "host_name" : "zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 4,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/10",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 72,
        "definition_name" : "ambari_server_agent_heartbeat",
        "host_name" : "zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 10,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/17",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 70,
        "definition_name" : "ambari_agent_version_select",
        "host_name" : "zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 17,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/54",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 55,
        "definition_name" : "ams_metrics_monitor_process",
        "host_name" : "zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 54,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/55",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 28,
        "definition_name" : "journalnode_process",
        "host_name" : "zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 55,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/56",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 2,
        "definition_name" : "zookeeper_server_process",
        "host_name" : "zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 56,
        "service_name" : "ZOOKEEPER"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/5",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 71,
        "definition_name" : "ambari_agent_disk_usage",
        "host_name" : "zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 5,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/9",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 72,
        "definition_name" : "ambari_server_agent_heartbeat",
        "host_name" : "zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 9,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/15",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 70,
        "definition_name" : "ambari_agent_version_select",
        "host_name" : "zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 15,
        "service_name" : "AMBARI"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/57",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 55,
        "definition_name" : "ams_metrics_monitor_process",
        "host_name" : "zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 57,
        "service_name" : "AMBARI_METRICS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/58",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 28,
        "definition_name" : "journalnode_process",
        "host_name" : "zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 58,
        "service_name" : "HDFS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alerts/59",
      "Alert" : {
        "cluster_name" : "sparkpoc",
        "definition_id" : 2,
        "definition_name" : "zookeeper_server_process",
        "host_name" : "zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
        "id" : 59,
        "service_name" : "ZOOKEEPER"
      }
    }
  ],
  "stack_versions" : [
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/stack_versions/1",
      "ClusterStackVersions" : {
        "cluster_name" : "sparkpoc",
        "id" : 1,
        "repository_version" : 1,
        "stack" : "HDP",
        "version" : "2.6"
      }
    }
  ],
  "service_config_versions" : [
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024m",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:1:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:1:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:1:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:1:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }\n    ",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards\n    ",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}\n    ",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:1:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"\n",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553536185,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024m",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553553967,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=3",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024m",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547129087042",
          "version" : 3,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink,org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.WdatpTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547129087195,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 3,
      "service_config_version_note" : "Adding WDATP timeline",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=4",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024m",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547393896244",
          "version" : 4,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547393896390,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 4,
      "service_config_version_note" : "Removing WDATP",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=5",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024m",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547564509034",
          "version" : 5,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink,org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.WdatpTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547564509193,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 5,
      "service_config_version_note" : "adding wdatp sink",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=6",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547564509034",
          "version" : 5,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink,org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.WdatpTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "version1547566054222",
          "version" : 3,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "/home/sshuser/ams",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547566054385,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 6,
      "service_config_version_note" : "adding classpath",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=7",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "version1547566054222",
          "version" : 3,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "/home/sshuser/ams",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547566424767",
          "version" : 6,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.WdatpTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547566424933,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 7,
      "service_config_version_note" : "remove HDI",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=8",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547564509034",
          "version" : 5,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink,org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.WdatpTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "version1547566054222",
          "version" : 3,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "/home/sshuser/ams",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547566548314,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 8,
      "service_config_version_note" : "Created from service config version V6",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=9",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547564509034",
          "version" : 5,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink,org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.WdatpTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "version1547566893452",
          "version" : 4,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "/home/sshuser/ams",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar:/home/sshuser/ams",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547566893614,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 9,
      "service_config_version_note" : "editing ams-env",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=10",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "version1547567058886",
          "version" : 5,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "/home/sshuser/ams",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547567058887",
          "version" : 7,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547567059102,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 10,
      "service_config_version_note" : "returning to normal",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=11",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "version1547567058886",
          "version" : 5,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "/home/sshuser/ams",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547642491417",
          "version" : 8,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.WdatpTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547642491609,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 11,
      "service_config_version_note" : "removig hdi adding wdatp",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=12",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024m",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256m",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024m",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "version1547567058886",
          "version" : 5,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "/home/sshuser/ams",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547567058887",
          "version" : 7,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547643380276,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 12,
      "service_config_version_note" : "Created from service config version V10",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=13",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547567058887",
          "version" : 7,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "version1547647211880",
          "version" : 6,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "/home/sshuser/ams",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar:/home/sshuser/ams/wdatpmdsdclient-1.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "version1547647211881",
          "version" : 3,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar:/home/sshuser/ams/wdatpmdsdclient-1.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547647212117,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 13,
      "service_config_version_note" : "Adding jar to all possible class paths",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=14",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "version1547647211880",
          "version" : 6,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "/home/sshuser/ams",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar:/home/sshuser/ams/wdatpmdsdclient-1.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "version1547647211881",
          "version" : 3,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar:/home/sshuser/ams/wdatpmdsdclient-1.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547647623968",
          "version" : 9,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.WdatpTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547647624141,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 14,
      "service_config_version_note" : "Updating to WDATP",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=15",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "version1547647211880",
          "version" : 6,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "/home/sshuser/ams",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar:/home/sshuser/ams/wdatpmdsdclient-1.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "version1547647211881",
          "version" : 3,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar:/home/sshuser/ams/wdatpmdsdclient-1.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547648616845",
          "version" : 10,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.WdatpTimelineMetricsAggregatorSink,org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.HdInsightTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547648617198,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 15,
      "service_config_version_note" : "adding hdinsight",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=AMBARI_METRICS&service_config_version=16",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ams-ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ams-ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ams-ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.read.shortcircuit" : "true",
            "hbase.client.scanner.caching" : "10000",
            "hbase.client.scanner.timeout.period" : "300000",
            "hbase.cluster.distributed" : "false",
            "hbase.hregion.majorcompaction" : "0",
            "hbase.hregion.max.filesize" : "4294967296",
            "hbase.hregion.memstore.block.multiplier" : "4",
            "hbase.hregion.memstore.flush.size" : "134217728",
            "hbase.hstore.blockingStoreFiles" : "200",
            "hbase.hstore.flusher.count" : "2",
            "hbase.local.dir" : "${hbase.tmp.dir}/local",
            "hbase.master.info.bindAddress" : "0.0.0.0",
            "hbase.master.info.port" : "61310",
            "hbase.master.normalizer.class" : "org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
            "hbase.master.port" : "61300",
            "hbase.master.wait.on.regionservers.mintostart" : "1",
            "hbase.normalizer.enabled" : "false",
            "hbase.normalizer.period" : "600000",
            "hbase.regionserver.global.memstore.lowerLimit" : "0.4",
            "hbase.regionserver.global.memstore.upperLimit" : "0.5",
            "hbase.regionserver.info.port" : "61330",
            "hbase.regionserver.port" : "61320",
            "hbase.regionserver.thread.compaction.large" : "2",
            "hbase.regionserver.thread.compaction.small" : "3",
            "hbase.replication" : "false",
            "hbase.rootdir" : "file:///mnt/data/ambari-metrics-collector/hbase",
            "hbase.rpc.timeout" : "300000",
            "hbase.snapshot.enabled" : "false",
            "hbase.tmp.dir" : "/var/lib/ambari-metrics-collector/hbase-tmp",
            "hbase.zookeeper.leaderport" : "61388",
            "hbase.zookeeper.peerport" : "61288",
            "hbase.zookeeper.property.clientPort" : "{{zookeeper_clientPort}}",
            "hbase.zookeeper.property.dataDir" : "${hbase.tmp.dir}/zookeeper",
            "hbase.zookeeper.property.tickTime" : "6000",
            "hbase.zookeeper.quorum" : "{{zookeeper_quorum_hosts}}",
            "hfile.block.cache.size" : "0.3",
            "phoenix.coprocessor.maxMetaDataCacheSize" : "20480000",
            "phoenix.coprocessor.maxServerCacheTimeToLiveMs" : "60000",
            "phoenix.groupby.maxCacheSize" : "307200000",
            "phoenix.mutate.batchSize" : "10000",
            "phoenix.query.keepAliveMs" : "300000",
            "phoenix.query.maxGlobalMemoryPercentage" : "15",
            "phoenix.query.rowKeyOrderSaltedTable" : "true",
            "phoenix.query.spoolThresholdBytes" : "20971520",
            "phoenix.query.timeoutMs" : "300000",
            "phoenix.sequence.saltBuckets" : "2",
            "phoenix.spool.directory" : "${hbase.tmp.dir}/phoenix-spool",
            "zookeeper.session.timeout" : "120000",
            "zookeeper.session.timeout.localHBaseCluster" : "120000",
            "zookeeper.znode.parent" : "/ams-hbase-unsecure"
          },
          "properties_attributes" : {
            "final" : {
              "hbase.zookeeper.quorum" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_hbase_log_maxbackupindex" : "20",
            "ams_hbase_log_maxfilesize" : "256",
            "ams_hbase_security_log_maxbackupindex" : "20",
            "ams_hbase_security_log_maxfilesize" : "256",
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \\\"hbase.root.logger\\\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize=1024MB\nhbase.log.maxbackupindex=10\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize=1024MB\nhbase.security.log.maxbackupindex=10\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ams-hbase\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ams-hbase\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.truststore.location" : "/etc/security/clientKeys/all.jks",
            "ssl.client.truststore.password" : "SECRET:ams-ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "METRICS_COLLECTOR:ams_collector,ams_hbase_master,ams_hbase_regionserver;METRICS_MONITOR:ams_monitor;METRICS_GRAFANA:ams_grafana",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"ams_hbase_master\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-master-*.log\"\n    },\n    {\n      \"type\":\"ams_hbase_regionserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/hbase-ams-regionserver-*.log\"\n    },\n    {\n      \"type\":\"ams_collector\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_collector_log_dir', '/var/log/ambari-metrics-collector')}}/ambari-metrics-collector.log\"\n    },\n    {\n      \"type\":\"ams_monitor\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-env/metrics_monitor_log_dir', '/var/log/ambari-metrics-monitor')}}/ambari-metrics-monitor.out\"\n    },\n    {\n      \"type\":\"ams_grafana\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/ams-grafana-env/metrics_grafana_log_dir', '/var/log/ambari-metrics-grafana')}}/grafana.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_collector\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %p %c: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_hbase_master\",\n            \"ams_hbase_regionserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_grafana\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t] %c{2}: %m%n\",\n      \"multiline_pattern\":\"^(%{DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{DATESTAMP:logtime}%{SPACE}\\\\[%{WORD:level}\\\\]%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy/MM/dd HH:mm:ss\"\n          }\n         },\n        \"level\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"I\",\n              \"post_value\":\"INFO\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"W\",\n              \"post_value\":\"WARN\"\n            }\n          },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"D\",\n              \"post_value\":\"DEBUG\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"E\",\n               \"post_value\":\"ERROR\"\n             }\n           },\n           {\n             \"map_fieldvalue\":{\n               \"pre_value\":\"F\",\n               \"post_value\":\"FATAL\"\n             }\n           }\n         ]\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"ams_monitor\"\n          ]\n        }\n      },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\\\[%{LOGLEVEL:level}\\\\]%{SPACE}%{JAVAFILE:file}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       },\n      \"level\":[\n        {\n          \"map_fieldvalue\":{\n            \"pre_value\":\"WARNING\",\n            \"post_value\":\"WARN\"\n          }\n        }\n      ]\n     }\n   ]\n }",
            "service_name" : "AMS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.masterregion.protocol.acl" : "*"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-security-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams.zookeeper.keytab" : "",
            "ams.zookeeper.principal" : "",
            "hadoop.security.authentication" : "",
            "hbase.coprocessor.master.classes" : "",
            "hbase.coprocessor.region.classes" : "",
            "hbase.master.kerberos.principal" : "",
            "hbase.master.keytab.file" : "",
            "hbase.myclient.keytab" : "",
            "hbase.myclient.principal" : "",
            "hbase.regionserver.kerberos.principal" : "",
            "hbase.regionserver.keytab.file" : "",
            "hbase.security.authentication" : "",
            "hbase.security.authorization" : "",
            "hbase.zookeeper.property.authProvider.1" : "",
            "hbase.zookeeper.property.jaasLoginRenew" : "",
            "hbase.zookeeper.property.kerberos.removeHostFromPrincipal" : "",
            "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal" : ""
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ca_cert" : "",
            "cert_file" : "/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
            "cert_key" : "/etc/ambari-metrics-grafana/conf/ams-grafana.key",
            "content" : "\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\n;protocol = http\nprotocol = {{ams_grafana_protocol}}\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\n;http_port = 3000\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\n;static_root_path = public\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\n;cert_file =\n;cert_key =\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n;password =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Buffer length of channel, keep it as it is if you don't know what it is.\n;buffer_len = 10000\n\n# Either \"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\", default is \"Trace\"\n;level = Info\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
            "port" : "3000",
            "protocol" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-grafana-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
            "metrics_grafana_data_dir" : "/var/lib/ambari-metrics-grafana",
            "metrics_grafana_log_dir" : "/var/log/ambari-metrics-grafana",
            "metrics_grafana_password" : "SECRET:ams-grafana-env:2:metrics_grafana_password",
            "metrics_grafana_pid_dir" : "/var/run/ambari-metrics-grafana",
            "metrics_grafana_username" : "admin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ams_log_max_backup_size" : "80",
            "ams_log_number_of_backup_files" : "60",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file,ETW,FilterLog\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize=80MB\nlog4j.appender.file.MaxBackupIndex=10\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c{1}:%L - %m%n\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=ambari-metrics-collector\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=ambari-metrics-collector\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-env",
          "tag" : "version1547647211880",
          "version" : 6,
          "properties" : {
            "ambari_metrics_user" : "ams",
            "ams_classpath_additional" : "/home/sshuser/ams",
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n#Adding Wasb2 dependencies into classpath\n\nexport COLLECTOR_ADDITIONAL_CLASSPATH=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar:/home/sshuser/ams/wdatpmdsdclient-1.0.jar",
            "failover_strategy_blacklisted_interval" : "600",
            "metrics_collector_heapsize" : "1024",
            "metrics_collector_log_dir" : "/var/log/ambari-metrics-collector",
            "metrics_collector_pid_dir" : "/var/run/ambari-metrics-collector",
            "metrics_monitor_log_dir" : "/var/log/ambari-metrics-monitor",
            "metrics_monitor_pid_dir" : "/var/run/ambari-metrics-monitor",
            "timeline.metrics.skip.disk.metrics.patterns" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-hbase-env",
          "tag" : "version1547647211881",
          "version" : 3,
          "properties" : {
            "content" : "\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp=/usr/hdp/current/hadoop-client/hadoop-azure.jar:/usr/hdp/current/hadoop-client/lib/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/hadoop-mapreduce-client/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/hadoop-mapreduce-client/threadly-4.9.0.jar:/home/sshuser/ams/wdatpmdsdclient-1.0.jar\n\nif [  -n \"${HBASE_CLASSPATH}\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=$additional_cp\nfi\n\n# The maximum amount of heap to use for hbase shell.\n export HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}}\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}}\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\nexport HBASE_ROOT_LOGGER=\"${HBASE_ROOT_LOGGER:-\"INFO,RFA\"},ETW,FilterLog\"",
            "hbase_classpath_additional" : "",
            "hbase_log_dir" : "/var/log/ambari-metrics-collector",
            "hbase_master_heapsize" : "1024",
            "hbase_master_maxperm_size" : "128",
            "hbase_master_xmn_size" : "256",
            "hbase_pid_dir" : "/var/run/ambari-metrics-collector/",
            "hbase_regionserver_heapsize" : "1024",
            "hbase_regionserver_shutdown_timeout" : "30",
            "hbase_regionserver_xmn_max" : "256m",
            "hbase_regionserver_xmn_ratio" : "0.2",
            "max_open_files_limit" : "32768",
            "regionserver_xmn_size" : "128"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ams-site",
          "tag" : "version1547648902632",
          "version" : 11,
          "properties" : {
            "cluster.zookeeper.property.clientPort" : "{{cluster_zookeeper_clientPort}}",
            "cluster.zookeeper.quorum" : "{{cluster_zookeeper_quorum_hosts}}",
            "failover.strategy" : "round-robin",
            "phoenix.query.maxGlobalMemoryPercentage" : "25",
            "phoenix.spool.directory" : "/tmp",
            "timeline.metrics.aggregator.checkpoint.dir" : "/var/lib/ambari-metrics-collector/checkpoint",
            "timeline.metrics.aggregators.skip.blockcache.enabled" : "false",
            "timeline.metrics.cache.commit.interval" : "3",
            "timeline.metrics.cache.enabled" : "true",
            "timeline.metrics.cache.size" : "150",
            "timeline.metrics.cluster.aggregate.splitpoints" : " ",
            "timeline.metrics.cluster.aggregation.sql.filters" : "sdisk\\_%,boottime",
            "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.daily.disabled" : "false",
            "timeline.metrics.cluster.aggregator.daily.interval" : "86400",
            "timeline.metrics.cluster.aggregator.daily.ttl" : "63072000",
            "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.hourly.disabled" : "false",
            "timeline.metrics.cluster.aggregator.hourly.interval" : "3600",
            "timeline.metrics.cluster.aggregator.hourly.ttl" : "31536000",
            "timeline.metrics.cluster.aggregator.interpolation.enabled" : "true",
            "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.minute.disabled" : "false",
            "timeline.metrics.cluster.aggregator.minute.interval" : "300",
            "timeline.metrics.cluster.aggregator.minute.ttl" : "2592000",
            "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.cluster.aggregator.second.disabled" : "false",
            "timeline.metrics.cluster.aggregator.second.interval" : "120",
            "timeline.metrics.cluster.aggregator.second.timeslice.interval" : "30",
            "timeline.metrics.cluster.aggregator.second.ttl" : "259200",
            "timeline.metrics.daily.aggregator.minute.interval" : "86400",
            "timeline.metrics.downsampler.event.metric.patterns" : "topology\\.%",
            "timeline.metrics.downsampler.topn.function" : "max",
            "timeline.metrics.downsampler.topn.metric.patterns" : "dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
            "timeline.metrics.downsampler.topn.value" : "10",
            "timeline.metrics.hbase.compression.scheme" : "SNAPPY",
            "timeline.metrics.hbase.data.block.encoding" : "FAST_DIFF",
            "timeline.metrics.hbase.fifo.compaction.enabled" : "false",
            "timeline.metrics.hbase.init.check.enabled" : "false",
            "timeline.metrics.host.aggregate.splitpoints" : " ",
            "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.daily.disabled" : "false",
            "timeline.metrics.host.aggregator.daily.ttl" : "31536000",
            "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.hourly.disabled" : "false",
            "timeline.metrics.host.aggregator.hourly.interval" : "3600",
            "timeline.metrics.host.aggregator.hourly.ttl" : "2592000",
            "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier" : "2",
            "timeline.metrics.host.aggregator.minute.disabled" : "false",
            "timeline.metrics.host.aggregator.minute.interval" : "300",
            "timeline.metrics.host.aggregator.minute.ttl" : "604800",
            "timeline.metrics.host.aggregator.ttl" : "86400",
            "timeline.metrics.service.aggregator.sink.class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.metrics.timeline.WdatpTimelineMetricsAggregatorSink",
            "timeline.metrics.service.checkpointDelay" : "60",
            "timeline.metrics.service.cluster.aggregator.appIds" : "datanode,nodemanager,hbase",
            "timeline.metrics.service.default.result.limit" : "15840",
            "timeline.metrics.service.http.policy" : "HTTP_ONLY",
            "timeline.metrics.service.metadata.filters" : "ContainerResource",
            "timeline.metrics.service.operation.mode" : "embedded",
            "timeline.metrics.service.resultset.fetchSize" : "2000",
            "timeline.metrics.service.rpc.address" : "0.0.0.0:30200",
            "timeline.metrics.service.use.groupBy.aggregators" : "false",
            "timeline.metrics.service.watcher.delay" : "30",
            "timeline.metrics.service.watcher.disabled" : "true",
            "timeline.metrics.service.watcher.initial.delay" : "600",
            "timeline.metrics.service.watcher.timeout" : "30",
            "timeline.metrics.service.webapp.address" : "0.0.0.0:6188",
            "timeline.metrics.sink.collection.period" : "10",
            "timeline.metrics.sink.report.interval" : "60",
            "timeline.metrics.whitelist.file" : "/etc/ambari-metrics-collector/conf/whitelistedmetrics.txt"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1547648902817,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 16,
      "service_config_version_note" : "Removing HDI",
      "service_name" : "AMBARI_METRICS",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=HDFS&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hdfs-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "dfs.block.access.token.enable" : "true",
            "dfs.blockreport.initialDelay" : "0",
            "dfs.blocksize" : "134217728",
            "dfs.client.failover.proxy.provider.mycluster" : "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",
            "dfs.client.read.shortcircuit" : "true",
            "dfs.client.read.shortcircuit.streams.cache.size" : "4096",
            "dfs.client.retry.policy.enabled" : "false",
            "dfs.cluster.administrators" : " hdfs",
            "dfs.content-summary.limit" : "5000",
            "dfs.datanode.address" : "0.0.0.0:30010",
            "dfs.datanode.balance.bandwidthPerSec" : "6250000",
            "dfs.datanode.data.dir" : "/mnt/resource/hadoop/hdfs/data",
            "dfs.datanode.data.dir.perm" : "750",
            "dfs.datanode.du.reserved" : "1073741824",
            "dfs.datanode.failed.volumes.tolerated" : "0",
            "dfs.datanode.http.address" : "0.0.0.0:30075",
            "dfs.datanode.https.address" : "0.0.0.0:30475",
            "dfs.datanode.ipc.address" : "0.0.0.0:30020",
            "dfs.datanode.max.transfer.threads" : "1024",
            "dfs.domain.socket.path" : "/var/lib/hadoop-hdfs/dn_socket",
            "dfs.encrypt.data.transfer.cipher.suites" : "AES/CTR/NoPadding",
            "dfs.encryption.key.provider.uri" : "",
            "dfs.ha.automatic-failover.enabled" : "true",
            "dfs.ha.fencing.methods" : "shell(/bin/true)",
            "dfs.ha.namenodes.mycluster" : "nn1,nn2",
            "dfs.heartbeat.interval" : "3",
            "dfs.hosts.exclude" : "/etc/hadoop/conf/dfs.exclude",
            "dfs.http.policy" : "HTTP_ONLY",
            "dfs.https.port" : "50470",
            "dfs.journalnode.edits.dir" : "/hadoop/hdfs/journal",
            "dfs.journalnode.http-address" : "0.0.0.0:8480",
            "dfs.journalnode.https-address" : "0.0.0.0:8481",
            "dfs.namenode.accesstime.precision" : "0",
            "dfs.namenode.audit.log.async" : "true",
            "dfs.namenode.avoid.read.stale.datanode" : "true",
            "dfs.namenode.avoid.write.stale.datanode" : "true",
            "dfs.namenode.checkpoint.dir" : "/hadoop/hdfs/namesecondary",
            "dfs.namenode.checkpoint.edits.dir" : "${dfs.namenode.checkpoint.dir}",
            "dfs.namenode.checkpoint.period" : "21600",
            "dfs.namenode.checkpoint.txns" : "1000000",
            "dfs.namenode.datanode.registration.ip-hostname-check" : "false",
            "dfs.namenode.fslock.fair" : "false",
            "dfs.namenode.handler.count" : "100",
            "dfs.namenode.http-address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30070",
            "dfs.namenode.http-address.mycluster.nn1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30070",
            "dfs.namenode.http-address.mycluster.nn2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30070",
            "dfs.namenode.https-address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30470",
            "dfs.namenode.https-address.mycluster.nn1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30470",
            "dfs.namenode.https-address.mycluster.nn2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30470",
            "dfs.namenode.name.dir" : "/hadoop/hdfs/namenode",
            "dfs.namenode.name.dir.restore" : "true",
            "dfs.namenode.rpc-address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8020",
            "dfs.namenode.rpc-address.mycluster.nn1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8020",
            "dfs.namenode.rpc-address.mycluster.nn2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8020",
            "dfs.namenode.safemode.extension" : "0",
            "dfs.namenode.safemode.threshold-pct" : "0.99f",
            "dfs.namenode.secondary.http-address" : "localhost:50090",
            "dfs.namenode.shared.edits.dir" : "qjournal://zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8485;zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8485;zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8485/mycluster",
            "dfs.namenode.stale.datanode.interval" : "30000",
            "dfs.namenode.startup.delay.block.deletion.sec" : "3600",
            "dfs.namenode.write.stale.datanode.ratio" : "1.0f",
            "dfs.nameservices" : "mycluster",
            "dfs.permissions.enabled" : "false",
            "dfs.permissions.superusergroup" : "hdfs",
            "dfs.qjm.operations.timeout" : "5m",
            "dfs.replication" : "3",
            "dfs.replication.max" : "50",
            "dfs.support.append" : "true",
            "dfs.webhdfs.enabled" : "false",
            "fs.permissions.umask-mode" : "022",
            "hadoop.caller.context.enabled" : "true",
            "manage.include.files" : "false",
            "nfs.exports.allowed.hosts" : "* rw",
            "nfs.file.dump.dir" : "/tmp/.hdfs-nfs"
          },
          "properties_attributes" : {
            "final" : {
              "dfs.webhdfs.enabled" : "true",
              "dfs.namenode.http-address" : "true",
              "dfs.support.append" : "true",
              "dfs.namenode.name.dir" : "true",
              "dfs.datanode.failed.volumes.tolerated" : "true",
              "dfs.datanode.data.dir" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hdfs-logsearch-conf",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "component_mappings" : "NAMENODE:hdfs_namenode;DATANODE:hdfs_datanode;SECONDARY_NAMENODE:hdfs_secondarynamenode;JOURNALNODE:hdfs_journalnode;ZKFC:hdfs_zkfc;NFS_GATEWAY:hdfs_nfs3",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"hdfs_datanode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-datanode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_namenode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-namenode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_journalnode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-journalnode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_secondarynamenode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-secondarynamenode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_zkfc\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-zkfc-*.log\"\n    },\n    {\n      \"type\":\"hdfs_nfs3\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-nfs3-*.log\"\n    },\n    {\n      \"type\":\"hdfs_audit\",\n      \"rowtype\":\"audit\",\n      \"is_enabled\":\"true\",\n      \"add_fields\":{\n        \"logType\":\"HDFSAudit\",\n        \"enforcer\":\"hadoop-acl\",\n        \"repoType\":\"1\",\n        \"repo\":\"hdfs\"\n      },\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hdfs-audit.log\"\n    }\n   ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_datanode\",\n            \"hdfs_journalnode\",\n            \"hdfs_secondarynamenode\",\n            \"hdfs_namenode\",\n            \"hdfs_zkfc\",\n            \"hdfs_nfs3\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\\\(%{JAVAFILE:file}:%{JAVAMETHOD:method}\\\\(%{INT:line_number}\\\\)\\\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n        }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_audit\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:evtTime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:evtTime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"evtTime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"keyvalue\",\n      \"sort_order\":1,\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_audit\"\n          ]\n         }\n       },\n      \"source_field\":\"log_message\",\n      \"value_split\":\"=\",\n      \"field_split\":\"\\t\",\n      \"post_map_values\":{\n        \"src\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"resource\"\n          }\n         },\n        \"ip\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"cliIP\"\n          }\n         },\n        \"allowed\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"true\",\n              \"post_value\":\"1\"\n            }\n           },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"false\",\n              \"post_value\":\"0\"\n            }\n           },\n          {\n            \"map_fieldname\":{\n              \"new_fieldname\":\"result\"\n            }\n           }\n         ],\n        \"cmd\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"action\"\n          }\n         },\n        \"proto\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"cliType\"\n          }\n         },\n        \"callerContext\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"req_caller_id\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"sort_order\":2,\n      \"source_field\":\"ugi\",\n      \"remove_source_field\":\"false\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_audit\"\n          ]\n         }\n       },\n      \"message_pattern\":\"%{USERNAME:p_user}.+auth:%{USERNAME:p_authType}.+via %{USERNAME:k_user}.+auth:%{USERNAME:k_authType}|%{USERNAME:user}.+auth:%{USERNAME:authType}|%{USERNAME:x_user}\",\n      \"post_map_values\":{\n        \"user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"reqUser\"\n          }\n         },\n        \"x_user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"reqUser\"\n          }\n         },\n        \"p_user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"reqUser\"\n          }\n         },\n        \"k_user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"proxyUsers\"\n          }\n         },\n        \"p_authType\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"authType\"\n          }\n         },\n        \"k_authType\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"proxyAuthType\"\n          }\n         }\n       }\n     }\n   ]\n }\n    ",
            "service_name" : "HDFS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hadoop-metrics2.properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements. See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# syntax: [prefix].[source|sink|jmx].[instance].[options]\n# See package.html for org.apache.hadoop.metrics2 for details\n\n{% if has_ganglia_server %}\n*.period=60\n\n*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31\n*.sink.ganglia.period=10\n\n# default for supportsparse is false\n*.sink.ganglia.supportsparse=true\n\n.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both\n.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40\n\n# Hook up to the server\nnamenode.sink.ganglia.servers={{ganglia_server_host}}:8661\ndatanode.sink.ganglia.servers={{ganglia_server_host}}:8659\njobtracker.sink.ganglia.servers={{ganglia_server_host}}:8662\ntasktracker.sink.ganglia.servers={{ganglia_server_host}}:8658\nmaptask.sink.ganglia.servers={{ganglia_server_host}}:8660\nreducetask.sink.ganglia.servers={{ganglia_server_host}}:8660\nresourcemanager.sink.ganglia.servers={{ganglia_server_host}}:8664\nnodemanager.sink.ganglia.servers={{ganglia_server_host}}:8657\nhistoryserver.sink.ganglia.servers={{ganglia_server_host}}:8666\njournalnode.sink.ganglia.servers={{ganglia_server_host}}:8654\nnimbus.sink.ganglia.servers={{ganglia_server_host}}:8649\nsupervisor.sink.ganglia.servers={{ganglia_server_host}}:8650\n\nresourcemanager.sink.ganglia.tagsForPrefix.yarn=Queue\n\n{% endif %}\n\n{% if has_metric_collector %}\n\n*.period={{metrics_collection_period}}\n*.sink.timeline.plugin.urls=file:///usr/lib/ambari-metrics-hadoop-sink/ambari-metrics-hadoop-sink.jar\n*.sink.timeline.class=org.apache.hadoop.metrics2.sink.timeline.HadoopTimelineMetricsSink\n*.sink.timeline.period={{metrics_collection_period}}\n*.sink.timeline.sendInterval={{metrics_report_interval}}000\n*.sink.timeline.slave.host.name={{hostname}}\n*.sink.timeline.zookeeper.quorum={{zookeeper_quorum}}\n*.sink.timeline.protocol={{metric_collector_protocol}}\n*.sink.timeline.port={{metric_collector_port}}\n\n# HTTPS properties\n*.sink.timeline.truststore.path = {{metric_truststore_path}}\n*.sink.timeline.truststore.type = {{metric_truststore_type}}\n*.sink.timeline.truststore.password = {{metric_truststore_password}}\n\ndatanode.sink.timeline.collector.hosts={{ams_collector_hosts}}\nnamenode.sink.timeline.collector.hosts={{ams_collector_hosts}}\nresourcemanager.sink.timeline.collector.hosts={{ams_collector_hosts}}\nnodemanager.sink.timeline.collector.hosts={{ams_collector_hosts}}\njobhistoryserver.sink.timeline.collector.hosts={{ams_collector_hosts}}\njournalnode.sink.timeline.collector.hosts={{ams_collector_hosts}}\nmaptask.sink.timeline.collector.hosts={{ams_collector_hosts}}\nreducetask.sink.timeline.collector.hosts={{ams_collector_hosts}}\napplicationhistoryserver.sink.timeline.collector.hosts={{ams_collector_hosts}}\n\nresourcemanager.sink.timeline.tagsForPrefix.yarn=Queue\n\n{% if is_nn_client_port_configured %}\n# Namenode rpc ports customization\nnamenode.sink.timeline.metric.rpc.client.port={{nn_rpc_client_port}}\n{% endif %}\n{% if is_nn_dn_port_configured %}\nnamenode.sink.timeline.metric.rpc.datanode.port={{nn_rpc_dn_port}}\n{% endif %}\n{% if is_nn_healthcheck_port_configured %}\nnamenode.sink.timeline.metric.rpc.healthcheck.port={{nn_rpc_healthcheck_port}}\n{% endif %}\n\n{% endif %}\n    "
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-policymgr-ssl",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "xasecure.policymgr.clientssl.keystore" : "/usr/hdp/current/hadoop-client/conf/ranger-plugin-keystore.jks",
            "xasecure.policymgr.clientssl.keystore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.keystore.password" : "SECRET:ranger-hdfs-policymgr-ssl:1:xasecure.policymgr.clientssl.keystore.password",
            "xasecure.policymgr.clientssl.truststore" : "/usr/hdp/current/hadoop-client/conf/ranger-plugin-truststore.jks",
            "xasecure.policymgr.clientssl.truststore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.truststore.password" : "SECRET:ranger-hdfs-policymgr-ssl:1:xasecure.policymgr.clientssl.truststore.password"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ssl-client",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ssl.client.keystore.location" : "/etc/security/clientKeys/keystore.jks",
            "ssl.client.keystore.password" : "SECRET:ssl-client:1:ssl.client.keystore.password",
            "ssl.client.keystore.type" : "jks",
            "ssl.client.truststore.password" : "SECRET:ssl-client:1:ssl.client.truststore.password",
            "ssl.client.truststore.reload.interval" : "10000",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-security",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ranger.plugin.hdfs.policy.cache.dir" : "/etc/ranger/{{repo_name}}/policycache",
            "ranger.plugin.hdfs.policy.pollIntervalMs" : "30000",
            "ranger.plugin.hdfs.policy.rest.ssl.config.file" : "/etc/hadoop/conf/ranger-policymgr-ssl.xml",
            "ranger.plugin.hdfs.policy.rest.url" : "{{policymgr_mgr_url}}",
            "ranger.plugin.hdfs.policy.source.impl" : "org.apache.ranger.admin.client.RangerAdminRESTClient",
            "ranger.plugin.hdfs.service.name" : "{{repo_name}}",
            "xasecure.add-hadoop-authorization" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ssl-server",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ssl-server:1:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ssl-server:1:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ssl-server:1:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hadoop-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Set Hadoop-specific environment variables here.\n\n# The only required environment variable is JAVA_HOME.  All others are\n# optional.  When running a distributed configuration it is best to\n# set JAVA_HOME in this file, so that it is correctly defined on\n# remote nodes.\n\n# The java implementation to use.  Required.\nexport JAVA_HOME={{java_home}}\nexport HADOOP_HOME_WARN_SUPPRESS=1\n\n# Hadoop home directory\nexport HADOOP_HOME=${HADOOP_HOME:-/usr/lib/hadoop}\n\n# Hadoop Configuration Directory\n#TODO: if env var set that can cause problems\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\n\n{# this is different for HDP1 #}\n# Path to jsvc required by secure HDP 2.0 datanode\nexport JSVC_HOME={{jsvc_path}}\n\n\n# The maximum amount of heap to use, in MB. Default is 1000.\nexport HADOOP_HEAPSIZE=\"{{hadoop_heapsize}}\"\n\nexport HADOOP_NAMENODE_INIT_HEAPSIZE=\"-Xms{{namenode_heapsize}}\"\n\n# Extra Java runtime options.  Empty by default.\nexport HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true ${HADOOP_OPTS}\"\n\n# Command specific options appended to HADOOP_OPTS when specified\nexport HADOOP_NAMENODE_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=namenode ${HADOOP_NAMENODE_OPTS}\"\nHADOOP_JOBTRACKER_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{jtnode_opt_newsize}} -XX:MaxNewSize={{jtnode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xmx{{jtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dhadoop.mapreduce.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}\"\n\nHADOOP_TASKTRACKER_OPTS=\"-server -Xmx{{ttnode_heapsize}} -Dhadoop.security.logger=ERROR,console -Dmapred.audit.logger=ERROR,console ${HADOOP_TASKTRACKER_OPTS}\"\nHADOOP_DATANODE_OPTS=\"-Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=ERROR,DRFAS -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=datanode ${HADOOP_DATANODE_OPTS}\"\nHADOOP_BALANCER_OPTS=\"-server -Xmx{{hadoop_heapsize}}m ${HADOOP_BALANCER_OPTS}\"\n\nexport HADOOP_SECONDARYNAMENODE_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps ${HADOOP_NAMENODE_INIT_HEAPSIZE} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=secondarynamenode ${HADOOP_SECONDARYNAMENODE_OPTS}\"\n\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\nexport HADOOP_CLIENT_OPTS=\"-Xmx${HADOOP_HEAPSIZE}m $HADOOP_CLIENT_OPTS\"\n# On secure datanodes, user to run the datanode as after dropping privileges\nexport HADOOP_SECURE_DN_USER={{hdfs_user}}\n\n# Extra ssh options.  Empty by default.\nexport HADOOP_SSH_OPTS=\"-o ConnectTimeout=5 -o SendEnv=HADOOP_CONF_DIR\"\n\n# Where log files are stored.  $HADOOP_HOME/logs by default.\nexport HADOOP_LOG_DIR={{hdfs_log_dir_prefix}}/$USER\n\n# History server logs\nexport HADOOP_MAPRED_LOG_DIR={{mapred_log_dir_prefix}}/$USER\n\n# Where log files are stored in the secure data environment.\nexport HADOOP_SECURE_DN_LOG_DIR={{hdfs_log_dir_prefix}}/$HADOOP_SECURE_DN_USER\n\n# File naming remote slave hosts.  $HADOOP_HOME/conf/slaves by default.\n# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves\n\n# host:path where hadoop code should be rsync'd from.  Unset by default.\n# export HADOOP_MASTER=master:/home/$USER/src/hadoop\n\n# Seconds to sleep between slave commands.  Unset by default.  This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HADOOP_SLAVE_SLEEP=0.1\n\n# The directory where pid files are stored. /tmp by default.\nexport HADOOP_PID_DIR={{hadoop_pid_dir_prefix}}/$USER\nexport HADOOP_SECURE_DN_PID_DIR={{hadoop_pid_dir_prefix}}/$HADOOP_SECURE_DN_USER\n\n# History server pid\nexport HADOOP_MAPRED_PID_DIR={{mapred_pid_dir_prefix}}/$USER\n\nYARN_RESOURCEMANAGER_OPTS=\"-Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=resourcemanager\"\n\n# A string representing this instance of hadoop. $USER by default.\nexport HADOOP_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes.  See 'man nice'.\n\n# export HADOOP_NICENESS=10\n\n# Use libraries from standard classpath\nJAVA_JDBC_LIBS=\"\"\n#Add libraries required by mysql connector\nfor jarFile in `ls /usr/share/java/*mysql* 2>/dev/null`\ndo\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\ndone\n#Add libraries required by oracle connector\nfor jarFile in `ls /usr/share/java/*ojdbc* 2>/dev/null`\ndo\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\ndone\n#Add libraries required by nodemanager\nMAPREDUCE_LIBS={{mapreduce_libs_path}}\nexport HADOOP_CLASSPATH=${HADOOP_CLASSPATH}${JAVA_JDBC_LIBS}:${MAPREDUCE_LIBS}\n\nif [ -d \"/usr/lib/tez\" ]; then\n  export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/tez/*:/usr/lib/tez/lib/*:/etc/tez/conf\nfi\n\n# Setting path to hdfs command line\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\n\n#Mostly required for hadoop 2.0\nexport JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:/usr/lib/hadoop/lib/native/Linux-amd64-64\n\n# Enable ACLs on zookeper znodes if required\n{% if hadoop_zkfc_opts is defined %}\nexport HADOOP_ZKFC_OPTS=\"{{hadoop_zkfc_opts}} $HADOOP_ZKFC_OPTS\"\n{% endif %}",
            "dtnode_heapsize" : "1024",
            "hadoop_heapsize" : "1024",
            "hadoop_pid_dir_prefix" : "/var/run/hadoop",
            "hadoop_root_logger" : "INFO,RFA",
            "hdfs_log_dir_prefix" : "/var/log/hadoop",
            "hdfs_tmp_dir" : "/tmp",
            "hdfs_user" : "hdfs",
            "hdfs_user_nofile_limit" : "128000",
            "hdfs_user_nproc_limit" : "65536",
            "keyserver_host" : " ",
            "keyserver_port" : "",
            "namenode_backup_dir" : "/tmp/upgrades",
            "namenode_heapsize" : "1024",
            "namenode_opt_maxnewsize" : "200",
            "namenode_opt_maxpermsize" : "256",
            "namenode_opt_newsize" : "200",
            "namenode_opt_permsize" : "128",
            "nfsgateway_heapsize" : "1024",
            "proxyuser_group" : "users"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hdfs-log4j",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nhadoop.root.logger=INFO,console\nhadoop.log.dir=.\nhadoop.log.file=hadoop.log\n\n\n# Define the root logger to the system property \\\"hadoop.root.logger\\\".\n\nlog4j.rootLogger=${hadoop.root.logger}, EventCounter, ETW, FilterLog\nlog4j.threshhold=ALL\n\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\n\n#\n# TaskLog Appender\n#\n\n#Default values\nhadoop.tasklog.taskid=null\nhadoop.tasklog.iscleanup=false\nhadoop.tasklog.noKeepSplits=4\nhadoop.tasklog.totalLogFileSize=100\nhadoop.tasklog.purgeLogSplits=true\nhadoop.tasklog.logsRetainHours=12\nlog4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender\nlog4j.appender.TLA.taskId=${hadoop.tasklog.taskid}\nlog4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}\nlog4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}\n\nlog4j.appender.TLA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n#\n#Security audit appender\n#\nhadoop.security.logger=INFO,console\nhadoop.security.log.maxfilesize=1024MB\nhadoop.security.log.maxbackupindex=10\nlog4j.category.SecurityLogger=${hadoop.security.logger}\nhadoop.security.log.file=SecurityAuth.audit\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.appender.RFA.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\nlog4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n#\n# hdfs audit logging\n#\nhdfs.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}\nlog4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false\nlog4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log\nlog4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.appender.RFAAUDIT.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFAAUDIT.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n\n#\n# mapred audit logging\n#\nmapred.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}\nlog4j.additivity.org.apache.hadoop.mapred.AuditLogger=false\nlog4j.appender.MRAUDIT=org.apache.log4j.RollingFileAppender\nlog4j.appender.MRAUDIT.File=${hadoop.log.dir}/mapred-audit.log\nlog4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.MRAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.MRAUDIT.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.MRAUDIT.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n\n#\n# Rolling File Appender\n#\n\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} - %m%n\n\n\n# Custom Logging levels\n\nhadoop.metrics.log.level=WARN\n#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG\n#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.metrics2=${hadoop.metrics.log.level}\n\n# Jets3t library\nlog4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR\n\n#\n# Null Appender\n# Trap security logger on the hadoop client side\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n# Removes \\\"deprecated\\\" messages\nlog4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=${component}\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux\n\n#\n# rm amlauncher logging\n#\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher=${hadoop.root.logger}, ETW, AMFilterLog\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher=false\nlog4j.appender.AMFilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.AMFilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.AMFilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.AMFilterLog.component=${component}\nlog4j.appender.AMFilterLog.whitelistFileName=${whitelist.filename}\nlog4j.appender.AMFilterLog.OSType=Linux",
            "hadoop_log_max_backup_size" : "256",
            "hadoop_log_number_of_backup_files" : "10",
            "hadoop_security_log_max_backup_size" : "256",
            "hadoop_security_log_number_of_backup_files" : "20"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-audit",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ranger.plugin.hdfs.ambari.cluster.name" : "{{cluster_name}}",
            "xasecure.audit.destination.hdfs" : "true",
            "xasecure.audit.destination.hdfs.batch.filespool.dir" : "/var/log/hadoop/hdfs/audit/hdfs/spool",
            "xasecure.audit.destination.hdfs.dir" : "hdfs://NAMENODE_HOSTNAME:8020/ranger/audit",
            "xasecure.audit.destination.solr" : "false",
            "xasecure.audit.destination.solr.batch.filespool.dir" : "/var/log/hadoop/hdfs/audit/solr/spool",
            "xasecure.audit.destination.solr.urls" : "",
            "xasecure.audit.destination.solr.zookeepers" : "NONE",
            "xasecure.audit.is.enabled" : "true",
            "xasecure.audit.provider.summary.enabled" : "false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-plugin-properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "REPOSITORY_CONFIG_PASSWORD" : "SECRET:ranger-hdfs-plugin-properties:1:REPOSITORY_CONFIG_PASSWORD",
            "REPOSITORY_CONFIG_USERNAME" : "hadoop",
            "common.name.for.certificate" : "",
            "external_admin_password" : "",
            "external_admin_username" : "",
            "external_ranger_admin_password" : "",
            "external_ranger_admin_username" : "",
            "hadoop.rpc.protection" : "authentication",
            "policy_user" : "ambari-qa",
            "ranger-hdfs-plugin-enabled" : "No"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hadoop-policy",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "security.admin.operations.protocol.acl" : "hadoop",
            "security.client.datanode.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.datanode.protocol.acl" : "*",
            "security.inter.datanode.protocol.acl" : "*",
            "security.inter.tracker.protocol.acl" : "*",
            "security.job.client.protocol.acl" : "*",
            "security.job.task.protocol.acl" : "*",
            "security.namenode.protocol.acl" : "*",
            "security.refresh.policy.protocol.acl" : "hadoop",
            "security.refresh.usertogroups.mappings.protocol.acl" : "hadoop"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "core-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "fs.AbstractFileSystem.wasb.impl" : "org.apache.hadoop.fs.azure.Wasb",
            "fs.AbstractFileSystem.wasbs.impl" : "org.apache.hadoop.fs.azure.Wasbs",
            "fs.azure.account.key.tomerdatabrickspoc.blob.core.windows.net" : "MIIB/QYJKoZIhvcNAQcDoIIB7jCCAeoCAQAxggFdMIIBWQIBADBBMC0xKzApBgNVBAMTImRiZW5jcnlwdGlvbi5oZGluc2lnaHRzZXJ2aWNlcy5uZXQCEFlCzg6fLGa3TjeH4AM+F9owDQYJKoZIhvcNAQEBBQAEggEAJD2Q3SKnf+IcngBaLY8r3PJlb0jOFhFmjqujLFWXoW2KyB80rzj5OhtJEbTxUlaoBi0x5YJfD74KeSA6bzw5L4HovV0GqtehYjUkiR7YtjGGmLMwp222c+KfWgzx6lkEpSI2kxQUKBYqDdB+u+2TJP68vE1nCN6uEBXxxw16dEJYlckPD9J3SqINPIvbzP9HlL5E2Q+ExvrFV67iSFu6R0JHg7s18eLOYoBRovbCSxOap0CgQ0VnjC7LyiyzE0QxMeHpQknWp5DxBNnl6K+oe8fpy2I0r+6dr/QwbguaPCp/yIkt0D8oir82BZACthbmTKJ4VtNof+fpEl7TNhlYRTCBgwYJKoZIhvcNAQcBMBQGCCqGSIb3DQMHBAgee9vmGwh09YBgB6//Ylc6/DWW0Edm4nMsk3ED1KYVH+rni02XLIcIfSAnsB2LLf/K/RfxaWCmQQJC/tGlxuCvbvJ04yL4ieghp/LMgSCdTHGo3+jNe1g1HcRlI5r3hV4P9HCOuiPbAyNl",
            "fs.azure.account.keyprovider.tomerdatabrickspoc.blob.core.windows.net" : "org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider",
            "fs.azure.io.copyblob.retry.max.retries" : "60",
            "fs.azure.io.read.tolerate.concurrent.append" : "true",
            "fs.azure.page.blob.dir" : "/mapreducestaging,/atshistory,/tezstaging,/ams/hbase/WALs,/ams/hbase/oldWALs,/ams/hbase/MasterProcWALs",
            "fs.azure.shellkeyprovider.script" : "/usr/lib/hdinsight-common/scripts/decrypt.sh",
            "fs.azure.user.agent.prefix" : "User-Agent: APN/1.0 Hortonworks/1.0 HDP/{{version}}",
            "fs.defaultFS" : "wasb://sparkpoc-2018-08-29t14-31-11-593z@tomerdatabrickspoc.blob.core.windows.net",
            "fs.s3a.fast.upload" : "true",
            "fs.s3a.fast.upload.buffer" : "disk",
            "fs.s3a.multipart.size" : "67108864",
            "fs.s3a.user.agent.prefix" : "User-Agent: APN/1.0 Hortonworks/1.0 HDP/{{version}}",
            "fs.trash.interval" : "360",
            "ha.failover-controller.active-standby-elector.zk.op.retries" : "120",
            "ha.zookeeper.quorum" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "hadoop.cache.data.dirprefix.list" : "/mnt/iocache/",
            "hadoop.cache.data.fullness.percentage" : "50",
            "hadoop.custom-extensions.root" : "/hdp/ext/{{major_stack_version}}/hadoop",
            "hadoop.http.authentication.simple.anonymous.allowed" : "true",
            "hadoop.proxyuser.hcat.groups" : "*",
            "hadoop.proxyuser.hive.groups" : "*",
            "hadoop.proxyuser.livy.groups" : "*",
            "hadoop.proxyuser.livy.hosts" : "*",
            "hadoop.proxyuser.oozie.groups" : "*",
            "hadoop.security.auth_to_local" : "DEFAULT",
            "hadoop.security.authentication" : "simple",
            "hadoop.security.authorization" : "false",
            "hadoop.security.key.provider.path" : "",
            "io.compression.codec.lzo.class" : "com.hadoop.compression.lzo.LzoCodec",
            "io.compression.codecs" : "org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec",
            "io.file.buffer.size" : "131072",
            "io.serializations" : "org.apache.hadoop.io.serializer.WritableSerialization",
            "iocachedisabled.fs.AbstractFileSystem.wasb.impl" : "org.apache.hadoop.fs.azure.Wasb",
            "iocachedisabled.fs.AbstractFileSystem.wasbs.impl" : "org.apache.hadoop.fs.azure.Wasbs",
            "iocachedisabled.fs.wasb.impl" : "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
            "iocachedisabled.fs.wasbs.impl" : "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
            "iocacheenabled.fs.AbstractFileSystem.wasb.impl" : "com.qubole.rubix.hadoop2.CachingAbstractNativeAzureFileSystem",
            "iocacheenabled.fs.AbstractFileSystem.wasbs.impl" : "com.qubole.rubix.hadoop2.CachingAbstractNativeAzureSecureFileSystem",
            "iocacheenabled.fs.wasb.impl" : "com.qubole.rubix.hadoop2.CachingNativeAzureFileSystem",
            "iocacheenabled.fs.wasbs.impl" : "com.qubole.rubix.hadoop2.CachingNativeAzureSecureFileSystem",
            "ipc.client.connect.max.retries" : "50",
            "ipc.client.connection.maxidletime" : "30000",
            "ipc.client.idlethreshold" : "8000",
            "ipc.server.tcpnodelay" : "true",
            "mapreduce.jobtracker.webinterface.trusted" : "false",
            "net.topology.script.file.name" : "/etc/hadoop/conf/topology_script.py"
          },
          "properties_attributes" : {
            "final" : {
              "fs.defaultFS" : "true"
            }
          }
        }
      ],
      "createtime" : 1535553518771,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "HDFS",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=HDFS&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hdfs-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.block.access.token.enable" : "true",
            "dfs.blockreport.initialDelay" : "0",
            "dfs.blocksize" : "134217728",
            "dfs.client.failover.proxy.provider.mycluster" : "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",
            "dfs.client.read.shortcircuit" : "true",
            "dfs.client.read.shortcircuit.streams.cache.size" : "4096",
            "dfs.client.retry.policy.enabled" : "false",
            "dfs.cluster.administrators" : " hdfs",
            "dfs.content-summary.limit" : "5000",
            "dfs.datanode.address" : "0.0.0.0:30010",
            "dfs.datanode.balance.bandwidthPerSec" : "6250000",
            "dfs.datanode.data.dir" : "/mnt/resource/hadoop/hdfs/data",
            "dfs.datanode.data.dir.perm" : "750",
            "dfs.datanode.du.reserved" : "1073741824",
            "dfs.datanode.failed.volumes.tolerated" : "0",
            "dfs.datanode.http.address" : "0.0.0.0:30075",
            "dfs.datanode.https.address" : "0.0.0.0:30475",
            "dfs.datanode.ipc.address" : "0.0.0.0:30020",
            "dfs.datanode.max.transfer.threads" : "1024",
            "dfs.domain.socket.path" : "/var/lib/hadoop-hdfs/dn_socket",
            "dfs.encrypt.data.transfer.cipher.suites" : "AES/CTR/NoPadding",
            "dfs.encryption.key.provider.uri" : "",
            "dfs.ha.automatic-failover.enabled" : "true",
            "dfs.ha.fencing.methods" : "shell(/bin/true)",
            "dfs.ha.namenodes.mycluster" : "nn1,nn2",
            "dfs.heartbeat.interval" : "3",
            "dfs.hosts.exclude" : "/etc/hadoop/conf/dfs.exclude",
            "dfs.http.policy" : "HTTP_ONLY",
            "dfs.https.port" : "50470",
            "dfs.internal.nameservices" : "mycluster",
            "dfs.journalnode.edits.dir" : "/hadoop/hdfs/journal",
            "dfs.journalnode.http-address" : "0.0.0.0:8480",
            "dfs.journalnode.https-address" : "0.0.0.0:8481",
            "dfs.namenode.accesstime.precision" : "0",
            "dfs.namenode.audit.log.async" : "true",
            "dfs.namenode.avoid.read.stale.datanode" : "true",
            "dfs.namenode.avoid.write.stale.datanode" : "true",
            "dfs.namenode.checkpoint.dir" : "/hadoop/hdfs/namesecondary",
            "dfs.namenode.checkpoint.edits.dir" : "${dfs.namenode.checkpoint.dir}",
            "dfs.namenode.checkpoint.period" : "21600",
            "dfs.namenode.checkpoint.txns" : "1000000",
            "dfs.namenode.datanode.registration.ip-hostname-check" : "false",
            "dfs.namenode.fslock.fair" : "false",
            "dfs.namenode.handler.count" : "100",
            "dfs.namenode.http-address.mycluster.nn1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30070",
            "dfs.namenode.http-address.mycluster.nn2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30070",
            "dfs.namenode.https-address.mycluster.nn1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30470",
            "dfs.namenode.https-address.mycluster.nn2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30470",
            "dfs.namenode.name.dir" : "/hadoop/hdfs/namenode",
            "dfs.namenode.name.dir.restore" : "true",
            "dfs.namenode.rpc-address.mycluster.nn1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8020",
            "dfs.namenode.rpc-address.mycluster.nn2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8020",
            "dfs.namenode.safemode.extension" : "0",
            "dfs.namenode.safemode.threshold-pct" : "0.99f",
            "dfs.namenode.secondary.http-address" : "localhost:50090",
            "dfs.namenode.shared.edits.dir" : "qjournal://zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8485;zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8485;zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8485/mycluster",
            "dfs.namenode.stale.datanode.interval" : "30000",
            "dfs.namenode.startup.delay.block.deletion.sec" : "3600",
            "dfs.namenode.write.stale.datanode.ratio" : "1.0f",
            "dfs.nameservices" : "mycluster",
            "dfs.permissions.enabled" : "false",
            "dfs.permissions.superusergroup" : "hdfs",
            "dfs.qjm.operations.timeout" : "5m",
            "dfs.replication" : "3",
            "dfs.replication.max" : "50",
            "dfs.support.append" : "true",
            "dfs.webhdfs.enabled" : "false",
            "fs.permissions.umask-mode" : "022",
            "hadoop.caller.context.enabled" : "true",
            "manage.include.files" : "false",
            "nfs.exports.allowed.hosts" : "* rw",
            "nfs.file.dump.dir" : "/tmp/.hdfs-nfs"
          },
          "properties_attributes" : {
            "final" : {
              "dfs.webhdfs.enabled" : "true",
              "dfs.namenode.http-address" : "true",
              "dfs.support.append" : "true",
              "dfs.namenode.name.dir" : "true",
              "dfs.datanode.failed.volumes.tolerated" : "true",
              "dfs.datanode.data.dir" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hdfs-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "NAMENODE:hdfs_namenode;DATANODE:hdfs_datanode;SECONDARY_NAMENODE:hdfs_secondarynamenode;JOURNALNODE:hdfs_journalnode;ZKFC:hdfs_zkfc;NFS_GATEWAY:hdfs_nfs3",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"hdfs_datanode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-datanode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_namenode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-namenode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_journalnode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-journalnode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_secondarynamenode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-secondarynamenode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_zkfc\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-zkfc-*.log\"\n    },\n    {\n      \"type\":\"hdfs_nfs3\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-nfs3-*.log\"\n    },\n    {\n      \"type\":\"hdfs_audit\",\n      \"rowtype\":\"audit\",\n      \"is_enabled\":\"true\",\n      \"add_fields\":{\n        \"logType\":\"HDFSAudit\",\n        \"enforcer\":\"hadoop-acl\",\n        \"repoType\":\"1\",\n        \"repo\":\"hdfs\"\n      },\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hdfs-audit.log\"\n    }\n   ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_datanode\",\n            \"hdfs_journalnode\",\n            \"hdfs_secondarynamenode\",\n            \"hdfs_namenode\",\n            \"hdfs_zkfc\",\n            \"hdfs_nfs3\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\\\(%{JAVAFILE:file}:%{JAVAMETHOD:method}\\\\(%{INT:line_number}\\\\)\\\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n        }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_audit\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:evtTime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:evtTime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"evtTime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"keyvalue\",\n      \"sort_order\":1,\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_audit\"\n          ]\n         }\n       },\n      \"source_field\":\"log_message\",\n      \"value_split\":\"=\",\n      \"field_split\":\"\\t\",\n      \"post_map_values\":{\n        \"src\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"resource\"\n          }\n         },\n        \"ip\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"cliIP\"\n          }\n         },\n        \"allowed\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"true\",\n              \"post_value\":\"1\"\n            }\n           },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"false\",\n              \"post_value\":\"0\"\n            }\n           },\n          {\n            \"map_fieldname\":{\n              \"new_fieldname\":\"result\"\n            }\n           }\n         ],\n        \"cmd\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"action\"\n          }\n         },\n        \"proto\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"cliType\"\n          }\n         },\n        \"callerContext\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"req_caller_id\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"sort_order\":2,\n      \"source_field\":\"ugi\",\n      \"remove_source_field\":\"false\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_audit\"\n          ]\n         }\n       },\n      \"message_pattern\":\"%{USERNAME:p_user}.+auth:%{USERNAME:p_authType}.+via %{USERNAME:k_user}.+auth:%{USERNAME:k_authType}|%{USERNAME:user}.+auth:%{USERNAME:authType}|%{USERNAME:x_user}\",\n      \"post_map_values\":{\n        \"user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"reqUser\"\n          }\n         },\n        \"x_user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"reqUser\"\n          }\n         },\n        \"p_user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"reqUser\"\n          }\n         },\n        \"k_user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"proxyUsers\"\n          }\n         },\n        \"p_authType\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"authType\"\n          }\n         },\n        \"k_authType\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"proxyAuthType\"\n          }\n         }\n       }\n     }\n   ]\n }",
            "service_name" : "HDFS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hadoop-metrics2.properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements. See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# syntax: [prefix].[source|sink|jmx].[instance].[options]\n# See package.html for org.apache.hadoop.metrics2 for details\n\n{% if has_ganglia_server %}\n*.period=60\n\n*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31\n*.sink.ganglia.period=10\n\n# default for supportsparse is false\n*.sink.ganglia.supportsparse=true\n\n.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both\n.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40\n\n# Hook up to the server\nnamenode.sink.ganglia.servers={{ganglia_server_host}}:8661\ndatanode.sink.ganglia.servers={{ganglia_server_host}}:8659\njobtracker.sink.ganglia.servers={{ganglia_server_host}}:8662\ntasktracker.sink.ganglia.servers={{ganglia_server_host}}:8658\nmaptask.sink.ganglia.servers={{ganglia_server_host}}:8660\nreducetask.sink.ganglia.servers={{ganglia_server_host}}:8660\nresourcemanager.sink.ganglia.servers={{ganglia_server_host}}:8664\nnodemanager.sink.ganglia.servers={{ganglia_server_host}}:8657\nhistoryserver.sink.ganglia.servers={{ganglia_server_host}}:8666\njournalnode.sink.ganglia.servers={{ganglia_server_host}}:8654\nnimbus.sink.ganglia.servers={{ganglia_server_host}}:8649\nsupervisor.sink.ganglia.servers={{ganglia_server_host}}:8650\n\nresourcemanager.sink.ganglia.tagsForPrefix.yarn=Queue\n\n{% endif %}\n\n{% if has_metric_collector %}\n\n*.period={{metrics_collection_period}}\n*.sink.timeline.plugin.urls=file:///usr/lib/ambari-metrics-hadoop-sink/ambari-metrics-hadoop-sink.jar\n*.sink.timeline.class=org.apache.hadoop.metrics2.sink.timeline.HadoopTimelineMetricsSink\n*.sink.timeline.period={{metrics_collection_period}}\n*.sink.timeline.sendInterval={{metrics_report_interval}}000\n*.sink.timeline.slave.host.name={{hostname}}\n*.sink.timeline.zookeeper.quorum={{zookeeper_quorum}}\n*.sink.timeline.protocol={{metric_collector_protocol}}\n*.sink.timeline.port={{metric_collector_port}}\n\n# HTTPS properties\n*.sink.timeline.truststore.path = {{metric_truststore_path}}\n*.sink.timeline.truststore.type = {{metric_truststore_type}}\n*.sink.timeline.truststore.password = {{metric_truststore_password}}\n\ndatanode.sink.timeline.collector.hosts={{ams_collector_hosts}}\nnamenode.sink.timeline.collector.hosts={{ams_collector_hosts}}\nresourcemanager.sink.timeline.collector.hosts={{ams_collector_hosts}}\nnodemanager.sink.timeline.collector.hosts={{ams_collector_hosts}}\njobhistoryserver.sink.timeline.collector.hosts={{ams_collector_hosts}}\njournalnode.sink.timeline.collector.hosts={{ams_collector_hosts}}\nmaptask.sink.timeline.collector.hosts={{ams_collector_hosts}}\nreducetask.sink.timeline.collector.hosts={{ams_collector_hosts}}\napplicationhistoryserver.sink.timeline.collector.hosts={{ams_collector_hosts}}\n\nresourcemanager.sink.timeline.tagsForPrefix.yarn=Queue\n\n{% if is_nn_client_port_configured %}\n# Namenode rpc ports customization\nnamenode.sink.timeline.metric.rpc.client.port={{nn_rpc_client_port}}\n{% endif %}\n{% if is_nn_dn_port_configured %}\nnamenode.sink.timeline.metric.rpc.datanode.port={{nn_rpc_dn_port}}\n{% endif %}\n{% if is_nn_healthcheck_port_configured %}\nnamenode.sink.timeline.metric.rpc.healthcheck.port={{nn_rpc_healthcheck_port}}\n{% endif %}\n\n{% endif %}"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-policymgr-ssl",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "xasecure.policymgr.clientssl.keystore" : "/usr/hdp/current/hadoop-client/conf/ranger-plugin-keystore.jks",
            "xasecure.policymgr.clientssl.keystore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.keystore.password" : "SECRET:ranger-hdfs-policymgr-ssl:2:xasecure.policymgr.clientssl.keystore.password",
            "xasecure.policymgr.clientssl.truststore" : "/usr/hdp/current/hadoop-client/conf/ranger-plugin-truststore.jks",
            "xasecure.policymgr.clientssl.truststore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.truststore.password" : "SECRET:ranger-hdfs-policymgr-ssl:2:xasecure.policymgr.clientssl.truststore.password"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.keystore.location" : "/etc/security/clientKeys/keystore.jks",
            "ssl.client.keystore.password" : "SECRET:ssl-client:2:ssl.client.keystore.password",
            "ssl.client.keystore.type" : "jks",
            "ssl.client.truststore.password" : "SECRET:ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.reload.interval" : "10000",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-security",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ranger.plugin.hdfs.policy.cache.dir" : "/etc/ranger/{{repo_name}}/policycache",
            "ranger.plugin.hdfs.policy.pollIntervalMs" : "30000",
            "ranger.plugin.hdfs.policy.rest.ssl.config.file" : "/etc/hadoop/conf/ranger-policymgr-ssl.xml",
            "ranger.plugin.hdfs.policy.rest.url" : "{{policymgr_mgr_url}}",
            "ranger.plugin.hdfs.policy.source.impl" : "org.apache.ranger.admin.client.RangerAdminRESTClient",
            "ranger.plugin.hdfs.service.name" : "{{repo_name}}",
            "xasecure.add-hadoop-authorization" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hadoop-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set Hadoop-specific environment variables here.\n\n# The only required environment variable is JAVA_HOME.  All others are\n# optional.  When running a distributed configuration it is best to\n# set JAVA_HOME in this file, so that it is correctly defined on\n# remote nodes.\n\n# The java implementation to use.  Required.\nexport JAVA_HOME={{java_home}}\nexport HADOOP_HOME_WARN_SUPPRESS=1\n\n# Hadoop home directory\nexport HADOOP_HOME=${HADOOP_HOME:-/usr/lib/hadoop}\n\n# Hadoop Configuration Directory\n#TODO: if env var set that can cause problems\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\n\n{# this is different for HDP1 #}\n# Path to jsvc required by secure HDP 2.0 datanode\nexport JSVC_HOME={{jsvc_path}}\n\n\n# The maximum amount of heap to use, in MB. Default is 1000.\nexport HADOOP_HEAPSIZE=\"{{hadoop_heapsize}}\"\n\nexport HADOOP_NAMENODE_INIT_HEAPSIZE=\"-Xms{{namenode_heapsize}}\"\n\n# Extra Java runtime options.  Empty by default.\nexport HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true ${HADOOP_OPTS}\"\n\n# Command specific options appended to HADOOP_OPTS when specified\nexport HADOOP_NAMENODE_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=namenode ${HADOOP_NAMENODE_OPTS}\"\nHADOOP_JOBTRACKER_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{jtnode_opt_newsize}} -XX:MaxNewSize={{jtnode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xmx{{jtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dhadoop.mapreduce.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}\"\n\nHADOOP_TASKTRACKER_OPTS=\"-server -Xmx{{ttnode_heapsize}} -Dhadoop.security.logger=ERROR,console -Dmapred.audit.logger=ERROR,console ${HADOOP_TASKTRACKER_OPTS}\"\nHADOOP_DATANODE_OPTS=\"-Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=ERROR,DRFAS -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=datanode ${HADOOP_DATANODE_OPTS}\"\nHADOOP_BALANCER_OPTS=\"-server -Xmx{{hadoop_heapsize}}m ${HADOOP_BALANCER_OPTS}\"\n\nexport HADOOP_SECONDARYNAMENODE_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps ${HADOOP_NAMENODE_INIT_HEAPSIZE} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=secondarynamenode ${HADOOP_SECONDARYNAMENODE_OPTS}\"\n\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\nexport HADOOP_CLIENT_OPTS=\"-Xmx${HADOOP_HEAPSIZE}m $HADOOP_CLIENT_OPTS\"\n# On secure datanodes, user to run the datanode as after dropping privileges\nexport HADOOP_SECURE_DN_USER={{hdfs_user}}\n\n# Extra ssh options.  Empty by default.\nexport HADOOP_SSH_OPTS=\"-o ConnectTimeout=5 -o SendEnv=HADOOP_CONF_DIR\"\n\n# Where log files are stored.  $HADOOP_HOME/logs by default.\nexport HADOOP_LOG_DIR={{hdfs_log_dir_prefix}}/$USER\n\n# History server logs\nexport HADOOP_MAPRED_LOG_DIR={{mapred_log_dir_prefix}}/$USER\n\n# Where log files are stored in the secure data environment.\nexport HADOOP_SECURE_DN_LOG_DIR={{hdfs_log_dir_prefix}}/$HADOOP_SECURE_DN_USER\n\n# File naming remote slave hosts.  $HADOOP_HOME/conf/slaves by default.\n# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves\n\n# host:path where hadoop code should be rsync'd from.  Unset by default.\n# export HADOOP_MASTER=master:/home/$USER/src/hadoop\n\n# Seconds to sleep between slave commands.  Unset by default.  This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HADOOP_SLAVE_SLEEP=0.1\n\n# The directory where pid files are stored. /tmp by default.\nexport HADOOP_PID_DIR={{hadoop_pid_dir_prefix}}/$USER\nexport HADOOP_SECURE_DN_PID_DIR={{hadoop_pid_dir_prefix}}/$HADOOP_SECURE_DN_USER\n\n# History server pid\nexport HADOOP_MAPRED_PID_DIR={{mapred_pid_dir_prefix}}/$USER\n\nYARN_RESOURCEMANAGER_OPTS=\"-Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=resourcemanager\"\n\n# A string representing this instance of hadoop. $USER by default.\nexport HADOOP_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes.  See 'man nice'.\n\n# export HADOOP_NICENESS=10\n\n# Use libraries from standard classpath\nJAVA_JDBC_LIBS=\"\"\n#Add libraries required by mysql connector\nfor jarFile in `ls /usr/share/java/*mysql* 2>/dev/null`\ndo\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\ndone\n#Add libraries required by oracle connector\nfor jarFile in `ls /usr/share/java/*ojdbc* 2>/dev/null`\ndo\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\ndone\n#Add libraries required by nodemanager\nMAPREDUCE_LIBS={{mapreduce_libs_path}}\nexport HADOOP_CLASSPATH=${HADOOP_CLASSPATH}${JAVA_JDBC_LIBS}:${MAPREDUCE_LIBS}\n\nif [ -d \"/usr/lib/tez\" ]; then\n  export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/tez/*:/usr/lib/tez/lib/*:/etc/tez/conf\nfi\n\n# Setting path to hdfs command line\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\n\n#Mostly required for hadoop 2.0\nexport JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:/usr/lib/hadoop/lib/native/Linux-amd64-64\n\n# Enable ACLs on zookeper znodes if required\n{% if hadoop_zkfc_opts is defined %}\nexport HADOOP_ZKFC_OPTS=\"{{hadoop_zkfc_opts}} $HADOOP_ZKFC_OPTS\"\n{% endif %}",
            "dfs_ha_initial_namenode_active" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
            "dfs_ha_initial_namenode_standby" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
            "dtnode_heapsize" : "1024m",
            "hadoop_heapsize" : "1024",
            "hadoop_pid_dir_prefix" : "/var/run/hadoop",
            "hadoop_root_logger" : "INFO,RFA",
            "hdfs_log_dir_prefix" : "/var/log/hadoop",
            "hdfs_tmp_dir" : "/tmp",
            "hdfs_user" : "hdfs",
            "hdfs_user_nofile_limit" : "128000",
            "hdfs_user_nproc_limit" : "65536",
            "keyserver_host" : " ",
            "keyserver_port" : "",
            "namenode_backup_dir" : "/tmp/upgrades",
            "namenode_heapsize" : "1024m",
            "namenode_opt_maxnewsize" : "200m",
            "namenode_opt_maxpermsize" : "256m",
            "namenode_opt_newsize" : "200m",
            "namenode_opt_permsize" : "128m",
            "nfsgateway_heapsize" : "1024",
            "proxyuser_group" : "users"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hdfs-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nhadoop.root.logger=INFO,console\nhadoop.log.dir=.\nhadoop.log.file=hadoop.log\n\n\n# Define the root logger to the system property \\\"hadoop.root.logger\\\".\n\nlog4j.rootLogger=${hadoop.root.logger}, EventCounter, ETW, FilterLog\nlog4j.threshhold=ALL\n\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\n\n#\n# TaskLog Appender\n#\n\n#Default values\nhadoop.tasklog.taskid=null\nhadoop.tasklog.iscleanup=false\nhadoop.tasklog.noKeepSplits=4\nhadoop.tasklog.totalLogFileSize=100\nhadoop.tasklog.purgeLogSplits=true\nhadoop.tasklog.logsRetainHours=12\nlog4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender\nlog4j.appender.TLA.taskId=${hadoop.tasklog.taskid}\nlog4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}\nlog4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}\n\nlog4j.appender.TLA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n#\n#Security audit appender\n#\nhadoop.security.logger=INFO,console\nhadoop.security.log.maxfilesize=1024MB\nhadoop.security.log.maxbackupindex=10\nlog4j.category.SecurityLogger=${hadoop.security.logger}\nhadoop.security.log.file=SecurityAuth.audit\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.appender.RFA.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\nlog4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n#\n# hdfs audit logging\n#\nhdfs.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}\nlog4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false\nlog4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log\nlog4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.appender.RFAAUDIT.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFAAUDIT.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n\n#\n# mapred audit logging\n#\nmapred.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}\nlog4j.additivity.org.apache.hadoop.mapred.AuditLogger=false\nlog4j.appender.MRAUDIT=org.apache.log4j.RollingFileAppender\nlog4j.appender.MRAUDIT.File=${hadoop.log.dir}/mapred-audit.log\nlog4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.MRAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.MRAUDIT.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.MRAUDIT.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n\n#\n# Rolling File Appender\n#\n\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} - %m%n\n\n\n# Custom Logging levels\n\nhadoop.metrics.log.level=WARN\n#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG\n#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.metrics2=${hadoop.metrics.log.level}\n\n# Jets3t library\nlog4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR\n\n#\n# Null Appender\n# Trap security logger on the hadoop client side\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n# Removes \\\"deprecated\\\" messages\nlog4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=${component}\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux\n\n#\n# rm amlauncher logging\n#\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher=${hadoop.root.logger}, ETW, AMFilterLog\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher=false\nlog4j.appender.AMFilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.AMFilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.AMFilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.AMFilterLog.component=${component}\nlog4j.appender.AMFilterLog.whitelistFileName=${whitelist.filename}\nlog4j.appender.AMFilterLog.OSType=Linux",
            "hadoop_log_max_backup_size" : "256",
            "hadoop_log_number_of_backup_files" : "10",
            "hadoop_security_log_max_backup_size" : "256",
            "hadoop_security_log_number_of_backup_files" : "20"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-audit",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ranger.plugin.hdfs.ambari.cluster.name" : "{{cluster_name}}",
            "xasecure.audit.destination.hdfs" : "true",
            "xasecure.audit.destination.hdfs.batch.filespool.dir" : "/var/log/hadoop/hdfs/audit/hdfs/spool",
            "xasecure.audit.destination.hdfs.dir" : "hdfs://NAMENODE_HOSTNAME:8020/ranger/audit",
            "xasecure.audit.destination.solr" : "false",
            "xasecure.audit.destination.solr.batch.filespool.dir" : "/var/log/hadoop/hdfs/audit/solr/spool",
            "xasecure.audit.destination.solr.urls" : "",
            "xasecure.audit.destination.solr.zookeepers" : "NONE",
            "xasecure.audit.is.enabled" : "true",
            "xasecure.audit.provider.summary.enabled" : "false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-plugin-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "REPOSITORY_CONFIG_PASSWORD" : "SECRET:ranger-hdfs-plugin-properties:2:REPOSITORY_CONFIG_PASSWORD",
            "REPOSITORY_CONFIG_USERNAME" : "hadoop",
            "common.name.for.certificate" : "",
            "external_admin_password" : "",
            "external_admin_username" : "",
            "external_ranger_admin_password" : "",
            "external_ranger_admin_username" : "",
            "hadoop.rpc.protection" : "authentication",
            "policy_user" : "ambari-qa",
            "ranger-hdfs-plugin-enabled" : "No"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hadoop-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.operations.protocol.acl" : "hadoop",
            "security.client.datanode.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.datanode.protocol.acl" : "*",
            "security.inter.datanode.protocol.acl" : "*",
            "security.inter.tracker.protocol.acl" : "*",
            "security.job.client.protocol.acl" : "*",
            "security.job.task.protocol.acl" : "*",
            "security.namenode.protocol.acl" : "*",
            "security.refresh.policy.protocol.acl" : "hadoop",
            "security.refresh.usertogroups.mappings.protocol.acl" : "hadoop"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "core-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fs.AbstractFileSystem.wasb.impl" : "org.apache.hadoop.fs.azure.Wasb",
            "fs.AbstractFileSystem.wasbs.impl" : "org.apache.hadoop.fs.azure.Wasbs",
            "fs.azure.account.key.tomerdatabrickspoc.blob.core.windows.net" : "MIIB/QYJKoZIhvcNAQcDoIIB7jCCAeoCAQAxggFdMIIBWQIBADBBMC0xKzApBgNVBAMTImRiZW5jcnlwdGlvbi5oZGluc2lnaHRzZXJ2aWNlcy5uZXQCEFlCzg6fLGa3TjeH4AM+F9owDQYJKoZIhvcNAQEBBQAEggEAJD2Q3SKnf+IcngBaLY8r3PJlb0jOFhFmjqujLFWXoW2KyB80rzj5OhtJEbTxUlaoBi0x5YJfD74KeSA6bzw5L4HovV0GqtehYjUkiR7YtjGGmLMwp222c+KfWgzx6lkEpSI2kxQUKBYqDdB+u+2TJP68vE1nCN6uEBXxxw16dEJYlckPD9J3SqINPIvbzP9HlL5E2Q+ExvrFV67iSFu6R0JHg7s18eLOYoBRovbCSxOap0CgQ0VnjC7LyiyzE0QxMeHpQknWp5DxBNnl6K+oe8fpy2I0r+6dr/QwbguaPCp/yIkt0D8oir82BZACthbmTKJ4VtNof+fpEl7TNhlYRTCBgwYJKoZIhvcNAQcBMBQGCCqGSIb3DQMHBAgee9vmGwh09YBgB6//Ylc6/DWW0Edm4nMsk3ED1KYVH+rni02XLIcIfSAnsB2LLf/K/RfxaWCmQQJC/tGlxuCvbvJ04yL4ieghp/LMgSCdTHGo3+jNe1g1HcRlI5r3hV4P9HCOuiPbAyNl",
            "fs.azure.account.keyprovider.tomerdatabrickspoc.blob.core.windows.net" : "org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider",
            "fs.azure.io.copyblob.retry.max.retries" : "60",
            "fs.azure.io.read.tolerate.concurrent.append" : "true",
            "fs.azure.page.blob.dir" : "/mapreducestaging,/atshistory,/tezstaging,/ams/hbase/WALs,/ams/hbase/oldWALs,/ams/hbase/MasterProcWALs",
            "fs.azure.shellkeyprovider.script" : "/usr/lib/hdinsight-common/scripts/decrypt.sh",
            "fs.azure.user.agent.prefix" : "User-Agent: APN/1.0 Hortonworks/1.0 HDP/{{version}}",
            "fs.defaultFS" : "wasb://sparkpoc-2018-08-29t14-31-11-593z@tomerdatabrickspoc.blob.core.windows.net",
            "fs.s3a.fast.upload" : "true",
            "fs.s3a.fast.upload.buffer" : "disk",
            "fs.s3a.multipart.size" : "67108864",
            "fs.s3a.user.agent.prefix" : "User-Agent: APN/1.0 Hortonworks/1.0 HDP/{{version}}",
            "fs.trash.interval" : "360",
            "ha.failover-controller.active-standby-elector.zk.op.retries" : "120",
            "ha.zookeeper.quorum" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "hadoop.cache.data.dirprefix.list" : "/mnt/iocache/",
            "hadoop.cache.data.fullness.percentage" : "50",
            "hadoop.custom-extensions.root" : "/hdp/ext/{{major_stack_version}}/hadoop",
            "hadoop.http.authentication.simple.anonymous.allowed" : "true",
            "hadoop.proxyuser.hcat.groups" : "*",
            "hadoop.proxyuser.hcat.hosts" : "*",
            "hadoop.proxyuser.hive.groups" : "*",
            "hadoop.proxyuser.hive.hosts" : "*",
            "hadoop.proxyuser.livy.groups" : "*",
            "hadoop.proxyuser.livy.hosts" : "*",
            "hadoop.proxyuser.oozie.groups" : "*",
            "hadoop.proxyuser.oozie.hosts" : "*",
            "hadoop.security.auth_to_local" : "DEFAULT",
            "hadoop.security.authentication" : "simple",
            "hadoop.security.authorization" : "false",
            "hadoop.security.key.provider.path" : "",
            "io.compression.codec.lzo.class" : "com.hadoop.compression.lzo.LzoCodec",
            "io.compression.codecs" : "org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec",
            "io.file.buffer.size" : "131072",
            "io.serializations" : "org.apache.hadoop.io.serializer.WritableSerialization",
            "iocachedisabled.fs.AbstractFileSystem.wasb.impl" : "org.apache.hadoop.fs.azure.Wasb",
            "iocachedisabled.fs.AbstractFileSystem.wasbs.impl" : "org.apache.hadoop.fs.azure.Wasbs",
            "iocachedisabled.fs.wasb.impl" : "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
            "iocachedisabled.fs.wasbs.impl" : "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
            "iocacheenabled.fs.AbstractFileSystem.wasb.impl" : "com.qubole.rubix.hadoop2.CachingAbstractNativeAzureFileSystem",
            "iocacheenabled.fs.AbstractFileSystem.wasbs.impl" : "com.qubole.rubix.hadoop2.CachingAbstractNativeAzureSecureFileSystem",
            "iocacheenabled.fs.wasb.impl" : "com.qubole.rubix.hadoop2.CachingNativeAzureFileSystem",
            "iocacheenabled.fs.wasbs.impl" : "com.qubole.rubix.hadoop2.CachingNativeAzureSecureFileSystem",
            "ipc.client.connect.max.retries" : "50",
            "ipc.client.connection.maxidletime" : "30000",
            "ipc.client.idlethreshold" : "8000",
            "ipc.server.tcpnodelay" : "true",
            "mapreduce.jobtracker.webinterface.trusted" : "false",
            "net.topology.script.file.name" : "/etc/hadoop/conf/topology_script.py"
          },
          "properties_attributes" : {
            "final" : {
              "fs.defaultFS" : "true"
            }
          }
        }
      ],
      "createtime" : 1535553549351,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "HDFS",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=HDFS&service_config_version=3",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hdfs-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.block.access.token.enable" : "true",
            "dfs.blockreport.initialDelay" : "0",
            "dfs.blocksize" : "134217728",
            "dfs.client.failover.proxy.provider.mycluster" : "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",
            "dfs.client.read.shortcircuit" : "true",
            "dfs.client.read.shortcircuit.streams.cache.size" : "4096",
            "dfs.client.retry.policy.enabled" : "false",
            "dfs.cluster.administrators" : " hdfs",
            "dfs.content-summary.limit" : "5000",
            "dfs.datanode.address" : "0.0.0.0:30010",
            "dfs.datanode.balance.bandwidthPerSec" : "6250000",
            "dfs.datanode.data.dir" : "/mnt/resource/hadoop/hdfs/data",
            "dfs.datanode.data.dir.perm" : "750",
            "dfs.datanode.du.reserved" : "1073741824",
            "dfs.datanode.failed.volumes.tolerated" : "0",
            "dfs.datanode.http.address" : "0.0.0.0:30075",
            "dfs.datanode.https.address" : "0.0.0.0:30475",
            "dfs.datanode.ipc.address" : "0.0.0.0:30020",
            "dfs.datanode.max.transfer.threads" : "1024",
            "dfs.domain.socket.path" : "/var/lib/hadoop-hdfs/dn_socket",
            "dfs.encrypt.data.transfer.cipher.suites" : "AES/CTR/NoPadding",
            "dfs.encryption.key.provider.uri" : "",
            "dfs.ha.automatic-failover.enabled" : "true",
            "dfs.ha.fencing.methods" : "shell(/bin/true)",
            "dfs.ha.namenodes.mycluster" : "nn1,nn2",
            "dfs.heartbeat.interval" : "3",
            "dfs.hosts.exclude" : "/etc/hadoop/conf/dfs.exclude",
            "dfs.http.policy" : "HTTP_ONLY",
            "dfs.https.port" : "50470",
            "dfs.internal.nameservices" : "mycluster",
            "dfs.journalnode.edits.dir" : "/hadoop/hdfs/journal",
            "dfs.journalnode.http-address" : "0.0.0.0:8480",
            "dfs.journalnode.https-address" : "0.0.0.0:8481",
            "dfs.namenode.accesstime.precision" : "0",
            "dfs.namenode.audit.log.async" : "true",
            "dfs.namenode.avoid.read.stale.datanode" : "true",
            "dfs.namenode.avoid.write.stale.datanode" : "true",
            "dfs.namenode.checkpoint.dir" : "/hadoop/hdfs/namesecondary",
            "dfs.namenode.checkpoint.edits.dir" : "${dfs.namenode.checkpoint.dir}",
            "dfs.namenode.checkpoint.period" : "21600",
            "dfs.namenode.checkpoint.txns" : "1000000",
            "dfs.namenode.datanode.registration.ip-hostname-check" : "false",
            "dfs.namenode.fslock.fair" : "false",
            "dfs.namenode.handler.count" : "100",
            "dfs.namenode.http-address.mycluster.nn1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30070",
            "dfs.namenode.http-address.mycluster.nn2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30070",
            "dfs.namenode.https-address.mycluster.nn1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30470",
            "dfs.namenode.https-address.mycluster.nn2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:30470",
            "dfs.namenode.name.dir" : "/hadoop/hdfs/namenode",
            "dfs.namenode.name.dir.restore" : "true",
            "dfs.namenode.rpc-address.mycluster.nn1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8020",
            "dfs.namenode.rpc-address.mycluster.nn2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8020",
            "dfs.namenode.safemode.extension" : "0",
            "dfs.namenode.safemode.threshold-pct" : "0.99f",
            "dfs.namenode.secondary.http-address" : "localhost:50090",
            "dfs.namenode.shared.edits.dir" : "qjournal://zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8485;zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8485;zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8485/mycluster",
            "dfs.namenode.stale.datanode.interval" : "30000",
            "dfs.namenode.startup.delay.block.deletion.sec" : "3600",
            "dfs.namenode.write.stale.datanode.ratio" : "1.0f",
            "dfs.nameservices" : "mycluster",
            "dfs.permissions.enabled" : "false",
            "dfs.permissions.superusergroup" : "hdfs",
            "dfs.qjm.operations.timeout" : "5m",
            "dfs.replication" : "3",
            "dfs.replication.max" : "50",
            "dfs.support.append" : "true",
            "dfs.webhdfs.enabled" : "false",
            "fs.permissions.umask-mode" : "022",
            "hadoop.caller.context.enabled" : "true",
            "manage.include.files" : "false",
            "nfs.exports.allowed.hosts" : "* rw",
            "nfs.file.dump.dir" : "/tmp/.hdfs-nfs"
          },
          "properties_attributes" : {
            "final" : {
              "dfs.webhdfs.enabled" : "true",
              "dfs.namenode.http-address" : "true",
              "dfs.support.append" : "true",
              "dfs.namenode.name.dir" : "true",
              "dfs.datanode.failed.volumes.tolerated" : "true",
              "dfs.datanode.data.dir" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hdfs-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "NAMENODE:hdfs_namenode;DATANODE:hdfs_datanode;SECONDARY_NAMENODE:hdfs_secondarynamenode;JOURNALNODE:hdfs_journalnode;ZKFC:hdfs_zkfc;NFS_GATEWAY:hdfs_nfs3",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"hdfs_datanode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-datanode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_namenode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-namenode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_journalnode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-journalnode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_secondarynamenode\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-secondarynamenode-*.log\"\n    },\n    {\n      \"type\":\"hdfs_zkfc\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-zkfc-*.log\"\n    },\n    {\n      \"type\":\"hdfs_nfs3\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hadoop-{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}-nfs3-*.log\"\n    },\n    {\n      \"type\":\"hdfs_audit\",\n      \"rowtype\":\"audit\",\n      \"is_enabled\":\"true\",\n      \"add_fields\":{\n        \"logType\":\"HDFSAudit\",\n        \"enforcer\":\"hadoop-acl\",\n        \"repoType\":\"1\",\n        \"repo\":\"hdfs\"\n      },\n      \"path\":\"{{default('/configurations/hadoop-env/hdfs_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/hadoop-env/hdfs_user', 'hdfs')}}/hdfs-audit.log\"\n    }\n   ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_datanode\",\n            \"hdfs_journalnode\",\n            \"hdfs_secondarynamenode\",\n            \"hdfs_namenode\",\n            \"hdfs_zkfc\",\n            \"hdfs_nfs3\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\\\(%{JAVAFILE:file}:%{JAVAMETHOD:method}\\\\(%{INT:line_number}\\\\)\\\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n        }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_audit\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:evtTime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:evtTime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}:%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"evtTime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"keyvalue\",\n      \"sort_order\":1,\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_audit\"\n          ]\n         }\n       },\n      \"source_field\":\"log_message\",\n      \"value_split\":\"=\",\n      \"field_split\":\"\\t\",\n      \"post_map_values\":{\n        \"src\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"resource\"\n          }\n         },\n        \"ip\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"cliIP\"\n          }\n         },\n        \"allowed\":[\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"true\",\n              \"post_value\":\"1\"\n            }\n           },\n          {\n            \"map_fieldvalue\":{\n              \"pre_value\":\"false\",\n              \"post_value\":\"0\"\n            }\n           },\n          {\n            \"map_fieldname\":{\n              \"new_fieldname\":\"result\"\n            }\n           }\n         ],\n        \"cmd\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"action\"\n          }\n         },\n        \"proto\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"cliType\"\n          }\n         },\n        \"callerContext\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"req_caller_id\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"sort_order\":2,\n      \"source_field\":\"ugi\",\n      \"remove_source_field\":\"false\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hdfs_audit\"\n          ]\n         }\n       },\n      \"message_pattern\":\"%{USERNAME:p_user}.+auth:%{USERNAME:p_authType}.+via %{USERNAME:k_user}.+auth:%{USERNAME:k_authType}|%{USERNAME:user}.+auth:%{USERNAME:authType}|%{USERNAME:x_user}\",\n      \"post_map_values\":{\n        \"user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"reqUser\"\n          }\n         },\n        \"x_user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"reqUser\"\n          }\n         },\n        \"p_user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"reqUser\"\n          }\n         },\n        \"k_user\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"proxyUsers\"\n          }\n         },\n        \"p_authType\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"authType\"\n          }\n         },\n        \"k_authType\":{\n          \"map_fieldname\":{\n            \"new_fieldname\":\"proxyAuthType\"\n          }\n         }\n       }\n     }\n   ]\n }",
            "service_name" : "HDFS"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hadoop-metrics2.properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements. See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# syntax: [prefix].[source|sink|jmx].[instance].[options]\n# See package.html for org.apache.hadoop.metrics2 for details\n\n{% if has_ganglia_server %}\n*.period=60\n\n*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31\n*.sink.ganglia.period=10\n\n# default for supportsparse is false\n*.sink.ganglia.supportsparse=true\n\n.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both\n.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40\n\n# Hook up to the server\nnamenode.sink.ganglia.servers={{ganglia_server_host}}:8661\ndatanode.sink.ganglia.servers={{ganglia_server_host}}:8659\njobtracker.sink.ganglia.servers={{ganglia_server_host}}:8662\ntasktracker.sink.ganglia.servers={{ganglia_server_host}}:8658\nmaptask.sink.ganglia.servers={{ganglia_server_host}}:8660\nreducetask.sink.ganglia.servers={{ganglia_server_host}}:8660\nresourcemanager.sink.ganglia.servers={{ganglia_server_host}}:8664\nnodemanager.sink.ganglia.servers={{ganglia_server_host}}:8657\nhistoryserver.sink.ganglia.servers={{ganglia_server_host}}:8666\njournalnode.sink.ganglia.servers={{ganglia_server_host}}:8654\nnimbus.sink.ganglia.servers={{ganglia_server_host}}:8649\nsupervisor.sink.ganglia.servers={{ganglia_server_host}}:8650\n\nresourcemanager.sink.ganglia.tagsForPrefix.yarn=Queue\n\n{% endif %}\n\n{% if has_metric_collector %}\n\n*.period={{metrics_collection_period}}\n*.sink.timeline.plugin.urls=file:///usr/lib/ambari-metrics-hadoop-sink/ambari-metrics-hadoop-sink.jar\n*.sink.timeline.class=org.apache.hadoop.metrics2.sink.timeline.HadoopTimelineMetricsSink\n*.sink.timeline.period={{metrics_collection_period}}\n*.sink.timeline.sendInterval={{metrics_report_interval}}000\n*.sink.timeline.slave.host.name={{hostname}}\n*.sink.timeline.zookeeper.quorum={{zookeeper_quorum}}\n*.sink.timeline.protocol={{metric_collector_protocol}}\n*.sink.timeline.port={{metric_collector_port}}\n\n# HTTPS properties\n*.sink.timeline.truststore.path = {{metric_truststore_path}}\n*.sink.timeline.truststore.type = {{metric_truststore_type}}\n*.sink.timeline.truststore.password = {{metric_truststore_password}}\n\ndatanode.sink.timeline.collector.hosts={{ams_collector_hosts}}\nnamenode.sink.timeline.collector.hosts={{ams_collector_hosts}}\nresourcemanager.sink.timeline.collector.hosts={{ams_collector_hosts}}\nnodemanager.sink.timeline.collector.hosts={{ams_collector_hosts}}\njobhistoryserver.sink.timeline.collector.hosts={{ams_collector_hosts}}\njournalnode.sink.timeline.collector.hosts={{ams_collector_hosts}}\nmaptask.sink.timeline.collector.hosts={{ams_collector_hosts}}\nreducetask.sink.timeline.collector.hosts={{ams_collector_hosts}}\napplicationhistoryserver.sink.timeline.collector.hosts={{ams_collector_hosts}}\n\nresourcemanager.sink.timeline.tagsForPrefix.yarn=Queue\n\n{% if is_nn_client_port_configured %}\n# Namenode rpc ports customization\nnamenode.sink.timeline.metric.rpc.client.port={{nn_rpc_client_port}}\n{% endif %}\n{% if is_nn_dn_port_configured %}\nnamenode.sink.timeline.metric.rpc.datanode.port={{nn_rpc_dn_port}}\n{% endif %}\n{% if is_nn_healthcheck_port_configured %}\nnamenode.sink.timeline.metric.rpc.healthcheck.port={{nn_rpc_healthcheck_port}}\n{% endif %}\n\n{% endif %}"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-policymgr-ssl",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "xasecure.policymgr.clientssl.keystore" : "/usr/hdp/current/hadoop-client/conf/ranger-plugin-keystore.jks",
            "xasecure.policymgr.clientssl.keystore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.keystore.password" : "SECRET:ranger-hdfs-policymgr-ssl:2:xasecure.policymgr.clientssl.keystore.password",
            "xasecure.policymgr.clientssl.truststore" : "/usr/hdp/current/hadoop-client/conf/ranger-plugin-truststore.jks",
            "xasecure.policymgr.clientssl.truststore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.truststore.password" : "SECRET:ranger-hdfs-policymgr-ssl:2:xasecure.policymgr.clientssl.truststore.password"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ssl-client",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.client.keystore.location" : "/etc/security/clientKeys/keystore.jks",
            "ssl.client.keystore.password" : "SECRET:ssl-client:2:ssl.client.keystore.password",
            "ssl.client.keystore.type" : "jks",
            "ssl.client.truststore.password" : "SECRET:ssl-client:2:ssl.client.truststore.password",
            "ssl.client.truststore.reload.interval" : "10000",
            "ssl.client.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-security",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ranger.plugin.hdfs.policy.cache.dir" : "/etc/ranger/{{repo_name}}/policycache",
            "ranger.plugin.hdfs.policy.pollIntervalMs" : "30000",
            "ranger.plugin.hdfs.policy.rest.ssl.config.file" : "/etc/hadoop/conf/ranger-policymgr-ssl.xml",
            "ranger.plugin.hdfs.policy.rest.url" : "{{policymgr_mgr_url}}",
            "ranger.plugin.hdfs.policy.source.impl" : "org.apache.ranger.admin.client.RangerAdminRESTClient",
            "ranger.plugin.hdfs.service.name" : "{{repo_name}}",
            "xasecure.add-hadoop-authorization" : "true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ssl-server",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ssl.server.keystore.keypassword" : "SECRET:ssl-server:2:ssl.server.keystore.keypassword",
            "ssl.server.keystore.location" : "/etc/security/serverKeys/keystore.jks",
            "ssl.server.keystore.password" : "SECRET:ssl-server:2:ssl.server.keystore.password",
            "ssl.server.keystore.type" : "jks",
            "ssl.server.truststore.location" : "/etc/security/serverKeys/all.jks",
            "ssl.server.truststore.password" : "SECRET:ssl-server:2:ssl.server.truststore.password",
            "ssl.server.truststore.reload.interval" : "10000",
            "ssl.server.truststore.type" : "jks"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hadoop-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set Hadoop-specific environment variables here.\n\n# The only required environment variable is JAVA_HOME.  All others are\n# optional.  When running a distributed configuration it is best to\n# set JAVA_HOME in this file, so that it is correctly defined on\n# remote nodes.\n\n# The java implementation to use.  Required.\nexport JAVA_HOME={{java_home}}\nexport HADOOP_HOME_WARN_SUPPRESS=1\n\n# Hadoop home directory\nexport HADOOP_HOME=${HADOOP_HOME:-/usr/lib/hadoop}\n\n# Hadoop Configuration Directory\n#TODO: if env var set that can cause problems\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\n\n{# this is different for HDP1 #}\n# Path to jsvc required by secure HDP 2.0 datanode\nexport JSVC_HOME={{jsvc_path}}\n\n\n# The maximum amount of heap to use, in MB. Default is 1000.\nexport HADOOP_HEAPSIZE=\"{{hadoop_heapsize}}\"\n\nexport HADOOP_NAMENODE_INIT_HEAPSIZE=\"-Xms{{namenode_heapsize}}\"\n\n# Extra Java runtime options.  Empty by default.\nexport HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true ${HADOOP_OPTS}\"\n\n# Command specific options appended to HADOOP_OPTS when specified\nexport HADOOP_NAMENODE_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=namenode ${HADOOP_NAMENODE_OPTS}\"\nHADOOP_JOBTRACKER_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{jtnode_opt_newsize}} -XX:MaxNewSize={{jtnode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xmx{{jtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dhadoop.mapreduce.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}\"\n\nHADOOP_TASKTRACKER_OPTS=\"-server -Xmx{{ttnode_heapsize}} -Dhadoop.security.logger=ERROR,console -Dmapred.audit.logger=ERROR,console ${HADOOP_TASKTRACKER_OPTS}\"\nHADOOP_DATANODE_OPTS=\"-Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=ERROR,DRFAS -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=datanode ${HADOOP_DATANODE_OPTS}\"\nHADOOP_BALANCER_OPTS=\"-server -Xmx{{hadoop_heapsize}}m ${HADOOP_BALANCER_OPTS}\"\n\nexport HADOOP_SECONDARYNAMENODE_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps ${HADOOP_NAMENODE_INIT_HEAPSIZE} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=secondarynamenode ${HADOOP_SECONDARYNAMENODE_OPTS}\"\n\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\nexport HADOOP_CLIENT_OPTS=\"-Xmx${HADOOP_HEAPSIZE}m $HADOOP_CLIENT_OPTS\"\n# On secure datanodes, user to run the datanode as after dropping privileges\nexport HADOOP_SECURE_DN_USER={{hdfs_user}}\n\n# Extra ssh options.  Empty by default.\nexport HADOOP_SSH_OPTS=\"-o ConnectTimeout=5 -o SendEnv=HADOOP_CONF_DIR\"\n\n# Where log files are stored.  $HADOOP_HOME/logs by default.\nexport HADOOP_LOG_DIR={{hdfs_log_dir_prefix}}/$USER\n\n# History server logs\nexport HADOOP_MAPRED_LOG_DIR={{mapred_log_dir_prefix}}/$USER\n\n# Where log files are stored in the secure data environment.\nexport HADOOP_SECURE_DN_LOG_DIR={{hdfs_log_dir_prefix}}/$HADOOP_SECURE_DN_USER\n\n# File naming remote slave hosts.  $HADOOP_HOME/conf/slaves by default.\n# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves\n\n# host:path where hadoop code should be rsync'd from.  Unset by default.\n# export HADOOP_MASTER=master:/home/$USER/src/hadoop\n\n# Seconds to sleep between slave commands.  Unset by default.  This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HADOOP_SLAVE_SLEEP=0.1\n\n# The directory where pid files are stored. /tmp by default.\nexport HADOOP_PID_DIR={{hadoop_pid_dir_prefix}}/$USER\nexport HADOOP_SECURE_DN_PID_DIR={{hadoop_pid_dir_prefix}}/$HADOOP_SECURE_DN_USER\n\n# History server pid\nexport HADOOP_MAPRED_PID_DIR={{mapred_pid_dir_prefix}}/$USER\n\nYARN_RESOURCEMANAGER_OPTS=\"-Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY -Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=resourcemanager\"\n\n# A string representing this instance of hadoop. $USER by default.\nexport HADOOP_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes.  See 'man nice'.\n\n# export HADOOP_NICENESS=10\n\n# Use libraries from standard classpath\nJAVA_JDBC_LIBS=\"\"\n#Add libraries required by mysql connector\nfor jarFile in `ls /usr/share/java/*mysql* 2>/dev/null`\ndo\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\ndone\n#Add libraries required by oracle connector\nfor jarFile in `ls /usr/share/java/*ojdbc* 2>/dev/null`\ndo\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\ndone\n#Add libraries required by nodemanager\nMAPREDUCE_LIBS={{mapreduce_libs_path}}\nexport HADOOP_CLASSPATH=${HADOOP_CLASSPATH}${JAVA_JDBC_LIBS}:${MAPREDUCE_LIBS}\n\nif [ -d \"/usr/lib/tez\" ]; then\n  export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/tez/*:/usr/lib/tez/lib/*:/etc/tez/conf\nfi\n\n# Setting path to hdfs command line\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\n\n#Mostly required for hadoop 2.0\nexport JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:/usr/lib/hadoop/lib/native/Linux-amd64-64\n\n# Enable ACLs on zookeper znodes if required\n{% if hadoop_zkfc_opts is defined %}\nexport HADOOP_ZKFC_OPTS=\"{{hadoop_zkfc_opts}} $HADOOP_ZKFC_OPTS\"\n{% endif %}",
            "dfs_ha_initial_namenode_active" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
            "dfs_ha_initial_namenode_standby" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
            "dtnode_heapsize" : "1024m",
            "hadoop_heapsize" : "1024",
            "hadoop_pid_dir_prefix" : "/var/run/hadoop",
            "hadoop_root_logger" : "INFO,RFA",
            "hdfs_log_dir_prefix" : "/var/log/hadoop",
            "hdfs_tmp_dir" : "/tmp",
            "hdfs_user" : "hdfs",
            "hdfs_user_nofile_limit" : "128000",
            "hdfs_user_nproc_limit" : "65536",
            "keyserver_host" : " ",
            "keyserver_port" : "",
            "namenode_backup_dir" : "/tmp/upgrades",
            "namenode_heapsize" : "1024m",
            "namenode_opt_maxnewsize" : "200m",
            "namenode_opt_maxpermsize" : "256m",
            "namenode_opt_newsize" : "200m",
            "namenode_opt_permsize" : "128m",
            "nfsgateway_heapsize" : "1024",
            "proxyuser_group" : "users"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hdfs-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nhadoop.root.logger=INFO,console\nhadoop.log.dir=.\nhadoop.log.file=hadoop.log\n\n\n# Define the root logger to the system property \\\"hadoop.root.logger\\\".\n\nlog4j.rootLogger=${hadoop.root.logger}, EventCounter, ETW, FilterLog\nlog4j.threshhold=ALL\n\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\n\n#\n# TaskLog Appender\n#\n\n#Default values\nhadoop.tasklog.taskid=null\nhadoop.tasklog.iscleanup=false\nhadoop.tasklog.noKeepSplits=4\nhadoop.tasklog.totalLogFileSize=100\nhadoop.tasklog.purgeLogSplits=true\nhadoop.tasklog.logsRetainHours=12\nlog4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender\nlog4j.appender.TLA.taskId=${hadoop.tasklog.taskid}\nlog4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}\nlog4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}\n\nlog4j.appender.TLA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n#\n#Security audit appender\n#\nhadoop.security.logger=INFO,console\nhadoop.security.log.maxfilesize=1024MB\nhadoop.security.log.maxbackupindex=10\nlog4j.category.SecurityLogger=${hadoop.security.logger}\nhadoop.security.log.file=SecurityAuth.audit\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.appender.RFA.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\nlog4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n#\n# hdfs audit logging\n#\nhdfs.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}\nlog4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false\nlog4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log\nlog4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\nlog4j.appender.RFAAUDIT.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFAAUDIT.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n\n#\n# mapred audit logging\n#\nmapred.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}\nlog4j.additivity.org.apache.hadoop.mapred.AuditLogger=false\nlog4j.appender.MRAUDIT=org.apache.log4j.RollingFileAppender\nlog4j.appender.MRAUDIT.File=${hadoop.log.dir}/mapred-audit.log\nlog4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.MRAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.MRAUDIT.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.MRAUDIT.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n\n#\n# Rolling File Appender\n#\n\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\nlog4j.appender.RFA.MaxFileSize=1024MB\nlog4j.appender.RFA.MaxBackupIndex=10\n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} - %m%n\n\n\n# Custom Logging levels\n\nhadoop.metrics.log.level=WARN\n#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG\n#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.metrics2=${hadoop.metrics.log.level}\n\n# Jets3t library\nlog4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR\n\n#\n# Null Appender\n# Trap security logger on the hadoop client side\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n# Removes \\\"deprecated\\\" messages\nlog4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN\n\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=${component}\nlog4j.appender.FilterLog.whitelistFileName=NA\nlog4j.appender.FilterLog.OSType=Linux\n\n#\n# rm amlauncher logging\n#\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher=${hadoop.root.logger}, ETW, AMFilterLog\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher=false\nlog4j.appender.AMFilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.AMFilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.AMFilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.AMFilterLog.component=${component}\nlog4j.appender.AMFilterLog.whitelistFileName=${whitelist.filename}\nlog4j.appender.AMFilterLog.OSType=Linux",
            "hadoop_log_max_backup_size" : "256",
            "hadoop_log_number_of_backup_files" : "10",
            "hadoop_security_log_max_backup_size" : "256",
            "hadoop_security_log_number_of_backup_files" : "20"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-audit",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ranger.plugin.hdfs.ambari.cluster.name" : "{{cluster_name}}",
            "xasecure.audit.destination.hdfs" : "true",
            "xasecure.audit.destination.hdfs.batch.filespool.dir" : "/var/log/hadoop/hdfs/audit/hdfs/spool",
            "xasecure.audit.destination.hdfs.dir" : "hdfs://NAMENODE_HOSTNAME:8020/ranger/audit",
            "xasecure.audit.destination.solr" : "false",
            "xasecure.audit.destination.solr.batch.filespool.dir" : "/var/log/hadoop/hdfs/audit/solr/spool",
            "xasecure.audit.destination.solr.urls" : "",
            "xasecure.audit.destination.solr.zookeepers" : "NONE",
            "xasecure.audit.is.enabled" : "true",
            "xasecure.audit.provider.summary.enabled" : "false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hdfs-plugin-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "REPOSITORY_CONFIG_PASSWORD" : "SECRET:ranger-hdfs-plugin-properties:2:REPOSITORY_CONFIG_PASSWORD",
            "REPOSITORY_CONFIG_USERNAME" : "hadoop",
            "common.name.for.certificate" : "",
            "external_admin_password" : "",
            "external_admin_username" : "",
            "external_ranger_admin_password" : "",
            "external_ranger_admin_username" : "",
            "hadoop.rpc.protection" : "authentication",
            "policy_user" : "ambari-qa",
            "ranger-hdfs-plugin-enabled" : "No"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hadoop-policy",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "security.admin.operations.protocol.acl" : "hadoop",
            "security.client.datanode.protocol.acl" : "*",
            "security.client.protocol.acl" : "*",
            "security.datanode.protocol.acl" : "*",
            "security.inter.datanode.protocol.acl" : "*",
            "security.inter.tracker.protocol.acl" : "*",
            "security.job.client.protocol.acl" : "*",
            "security.job.task.protocol.acl" : "*",
            "security.namenode.protocol.acl" : "*",
            "security.refresh.policy.protocol.acl" : "hadoop",
            "security.refresh.usertogroups.mappings.protocol.acl" : "hadoop"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "core-site",
          "tag" : "iocachedeactivateversion20180829144422",
          "version" : 3,
          "properties" : {
            "fs.AbstractFileSystem.wasb.impl" : "org.apache.hadoop.fs.azure.Wasb",
            "fs.AbstractFileSystem.wasbs.impl" : "org.apache.hadoop.fs.azure.Wasbs",
            "fs.azure.account.key.tomerdatabrickspoc.blob.core.windows.net" : "MIIB/QYJKoZIhvcNAQcDoIIB7jCCAeoCAQAxggFdMIIBWQIBADBBMC0xKzApBgNVBAMTImRiZW5jcnlwdGlvbi5oZGluc2lnaHRzZXJ2aWNlcy5uZXQCEFlCzg6fLGa3TjeH4AM+F9owDQYJKoZIhvcNAQEBBQAEggEAJD2Q3SKnf+IcngBaLY8r3PJlb0jOFhFmjqujLFWXoW2KyB80rzj5OhtJEbTxUlaoBi0x5YJfD74KeSA6bzw5L4HovV0GqtehYjUkiR7YtjGGmLMwp222c+KfWgzx6lkEpSI2kxQUKBYqDdB+u+2TJP68vE1nCN6uEBXxxw16dEJYlckPD9J3SqINPIvbzP9HlL5E2Q+ExvrFV67iSFu6R0JHg7s18eLOYoBRovbCSxOap0CgQ0VnjC7LyiyzE0QxMeHpQknWp5DxBNnl6K+oe8fpy2I0r+6dr/QwbguaPCp/yIkt0D8oir82BZACthbmTKJ4VtNof+fpEl7TNhlYRTCBgwYJKoZIhvcNAQcBMBQGCCqGSIb3DQMHBAgee9vmGwh09YBgB6//Ylc6/DWW0Edm4nMsk3ED1KYVH+rni02XLIcIfSAnsB2LLf/K/RfxaWCmQQJC/tGlxuCvbvJ04yL4ieghp/LMgSCdTHGo3+jNe1g1HcRlI5r3hV4P9HCOuiPbAyNl",
            "fs.azure.account.keyprovider.tomerdatabrickspoc.blob.core.windows.net" : "org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider",
            "fs.azure.io.copyblob.retry.max.retries" : "60",
            "fs.azure.io.read.tolerate.concurrent.append" : "true",
            "fs.azure.page.blob.dir" : "/mapreducestaging,/atshistory,/tezstaging,/ams/hbase/WALs,/ams/hbase/oldWALs,/ams/hbase/MasterProcWALs",
            "fs.azure.shellkeyprovider.script" : "/usr/lib/hdinsight-common/scripts/decrypt.sh",
            "fs.azure.user.agent.prefix" : "User-Agent: APN/1.0 Hortonworks/1.0 HDP/{{version}}",
            "fs.defaultFS" : "wasb://sparkpoc-2018-08-29t14-31-11-593z@tomerdatabrickspoc.blob.core.windows.net",
            "fs.s3a.fast.upload" : "true",
            "fs.s3a.fast.upload.buffer" : "disk",
            "fs.s3a.multipart.size" : "67108864",
            "fs.s3a.user.agent.prefix" : "User-Agent: APN/1.0 Hortonworks/1.0 HDP/{{version}}",
            "fs.trash.interval" : "360",
            "ha.failover-controller.active-standby-elector.zk.op.retries" : "120",
            "ha.zookeeper.quorum" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "hadoop.cache.data.dirprefix.list" : "/mnt/iocache/",
            "hadoop.cache.data.fullness.percentage" : "50",
            "hadoop.custom-extensions.root" : "/hdp/ext/{{major_stack_version}}/hadoop",
            "hadoop.http.authentication.simple.anonymous.allowed" : "true",
            "hadoop.proxyuser.hcat.groups" : "*",
            "hadoop.proxyuser.hcat.hosts" : "*",
            "hadoop.proxyuser.hive.groups" : "*",
            "hadoop.proxyuser.hive.hosts" : "*",
            "hadoop.proxyuser.livy.groups" : "*",
            "hadoop.proxyuser.livy.hosts" : "*",
            "hadoop.proxyuser.oozie.groups" : "*",
            "hadoop.proxyuser.oozie.hosts" : "*",
            "hadoop.security.auth_to_local" : "DEFAULT",
            "hadoop.security.authentication" : "simple",
            "hadoop.security.authorization" : "false",
            "hadoop.security.key.provider.path" : "",
            "io.compression.codec.lzo.class" : "com.hadoop.compression.lzo.LzoCodec",
            "io.compression.codecs" : "org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec",
            "io.file.buffer.size" : "131072",
            "io.serializations" : "org.apache.hadoop.io.serializer.WritableSerialization",
            "iocachedisabled.fs.AbstractFileSystem.wasb.impl" : "org.apache.hadoop.fs.azure.Wasb",
            "iocachedisabled.fs.AbstractFileSystem.wasbs.impl" : "org.apache.hadoop.fs.azure.Wasbs",
            "iocachedisabled.fs.wasb.impl" : "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
            "iocachedisabled.fs.wasbs.impl" : "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
            "iocacheenabled.fs.AbstractFileSystem.wasb.impl" : "com.qubole.rubix.hadoop2.CachingAbstractNativeAzureFileSystem",
            "iocacheenabled.fs.AbstractFileSystem.wasbs.impl" : "com.qubole.rubix.hadoop2.CachingAbstractNativeAzureSecureFileSystem",
            "iocacheenabled.fs.wasb.impl" : "com.qubole.rubix.hadoop2.CachingNativeAzureFileSystem",
            "iocacheenabled.fs.wasbs.impl" : "com.qubole.rubix.hadoop2.CachingNativeAzureSecureFileSystem",
            "ipc.client.connect.max.retries" : "50",
            "ipc.client.connection.maxidletime" : "30000",
            "ipc.client.idlethreshold" : "8000",
            "ipc.server.tcpnodelay" : "true",
            "mapreduce.jobtracker.webinterface.trusted" : "false",
            "net.topology.script.file.name" : "/etc/hadoop/conf/topology_script.py"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553862661,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 3,
      "service_config_version_note" : null,
      "service_name" : "HDFS",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=HIVE&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "llap-cli-log4j2",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nstatus = WARN\nname = LlapCliLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.hive.log.level = INFO\nproperty.hive.root.logger = console\nproperty.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}\nproperty.hive.log.file = llap-cli.log\nproperty.hive.llapstatus.consolelogger.level = INFO\n\n# list of all appenders\nappenders = console, RFA, llapstatusconsole,FilterLog\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nappender.FilterLog.type = FilterLog\nappender.FilterLog.name = FilterLog\nappender.FilterLog.layout.type = PatternLayout\n#appender.FilterLog.layout.pattern =   %d{ISO8601} %5p [%t] %c{2}: %m%n\nappender.FilterLog.layout.pattern = %d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\nappender.FilterLog.source=CentralFilteredHadoopServiceLogs\nappender.FilterLog.component=llapcli\nappender.FilterLog.whitelistFile=NA\nappender.FilterLog.osType=Linux\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %p %c{2}: %m%n\n\n# llapstatusconsole appender\nappender.llapstatusconsole.type = Console\nappender.llapstatusconsole.name = llapstatusconsole\nappender.llapstatusconsole.target = SYSTEM_OUT\nappender.llapstatusconsole.layout.type = PatternLayout\nappender.llapstatusconsole.layout.pattern = %m%n\n\nappender.RFA.type = RollingRandomAccessFile\nappender.RFA.name = RFA\nappender.RFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}\n\n# Use %pid in the filePattern to append process-id@host-name to the filename if you want separate log files for different CLI session\nappender.RFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}_%i\nappender.RFA.layout.type = PatternLayout\nappender.RFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n\nappender.RFA.policies.type = Policies\nappender.RFA.policies.size.type = SizeBasedTriggeringPolicy\nappender.RFA.policies.size.size = 1024MB\nappender.RFA.strategy.type = DefaultRolloverStrategy\nappender.RFA.strategy.max = 10\n\n# list of all loggers\nloggers = ZooKeeper, DataNucleus, Datastore, JPOX, HadoopConf, LlapStatusServiceDriverConsole\n\nlogger.ZooKeeper.name = org.apache.zookeeper\nlogger.ZooKeeper.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\nlogger.HadoopConf.name = org.apache.hadoop.conf.Configuration\nlogger.HadoopConf.level = ERROR\n\nlogger.LlapStatusServiceDriverConsole.name = LlapStatusServiceDriverConsole\nlogger.LlapStatusServiceDriverConsole.additivity = false\nlogger.LlapStatusServiceDriverConsole.level = ${sys:hive.llapstatus.consolelogger.level}\n\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root, RFA, FL\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}\nrootLogger.appenderRef.RFA.ref = RFA\nrootLogger.appenderRef.FL.ref = FilterLog\nlogger.LlapStatusServiceDriverConsole.appenderRefs = llapstatusconsole, RFA\nlogger.LlapStatusServiceDriverConsole.appenderRef.llapstatusconsole.ref = llapstatusconsole\nlogger.LlapStatusServiceDriverConsole.appenderRef.RFA.ref = RFA",
            "llap_cli_log_maxbackupindex" : "30",
            "llap_cli_log_maxfilesize" : "256"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "llap-daemon-log4j",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# This is the log4j2 properties file used by llap-daemons. There's several loggers defined, which\n# can be selected while configuring LLAP.\n# Based on the one selected - UI links etc need to be manipulated in the system.\n# Note: Some names and logic is common to this file and llap LogHelpers. Make sure to change that\n# as well, if changing this file.\n\nstatus = INFO\nname = LlapDaemonLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.llap.daemon.log.level = {{llap_log_level}}\nproperty.llap.daemon.root.logger = console\nproperty.llap.daemon.log.dir = .\nproperty.llap.daemon.log.file = llapdaemon.log\nproperty.llap.daemon.historylog.file = llapdaemon_history.log\nproperty.llap.daemon.log.maxfilesize = {{hive_llap_log_maxfilesize}}MB\nproperty.llap.daemon.log.maxbackupindex = {{hive_llap_log_maxbackupindex}}\n\n# list of all appenders\nappenders = console, RFA, HISTORYAPPENDER, query-routing\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{ISO8601} %5p [%t (%X{fragmentId})] %c{2}: %m%n\n\n# rolling file appender\nappender.RFA.type = RollingRandomAccessFile\nappender.RFA.name = RFA\nappender.RFA.fileName = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.log.file}\nappender.RFA.filePattern = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.log.file}_%d{yyyy-MM-dd-HH}_%i.done\nappender.RFA.layout.type = PatternLayout\nappender.RFA.layout.pattern = %d{ISO8601} %-5p [%t (%X{fragmentId})] %c: %m%n\nappender.RFA.policies.type = Policies\nappender.RFA.policies.time.type = TimeBasedTriggeringPolicy\nappender.RFA.policies.time.interval = 1\nappender.RFA.policies.time.modulate = true\nappender.RFA.policies.size.type = SizeBasedTriggeringPolicy\nappender.RFA.policies.size.size = ${sys:llap.daemon.log.maxfilesize}\nappender.RFA.strategy.type = DefaultRolloverStrategy\nappender.RFA.strategy.max = ${sys:llap.daemon.log.maxbackupindex}\n\n# history file appender\nappender.HISTORYAPPENDER.type = RollingRandomAccessFile\nappender.HISTORYAPPENDER.name = HISTORYAPPENDER\nappender.HISTORYAPPENDER.fileName = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.historylog.file}\nappender.HISTORYAPPENDER.filePattern = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.historylog.file}_%d{yyyy-MM-dd-HH}_%i.done\nappender.HISTORYAPPENDER.layout.type = PatternLayout\nappender.HISTORYAPPENDER.layout.pattern = %m%n\nappender.HISTORYAPPENDER.policies.type = Policies\nappender.HISTORYAPPENDER.policies.size.type = SizeBasedTriggeringPolicy\nappender.HISTORYAPPENDER.policies.size.size = ${sys:llap.daemon.log.maxfilesize}\nappender.HISTORYAPPENDER.policies.time.type = TimeBasedTriggeringPolicy\nappender.HISTORYAPPENDER.policies.time.interval = 1\nappender.HISTORYAPPENDER.policies.time.modulate = true\nappender.HISTORYAPPENDER.strategy.type = DefaultRolloverStrategy\nappender.HISTORYAPPENDER.strategy.max = ${sys:llap.daemon.log.maxbackupindex}\n\n# queryId based routing file appender\nappender.query-routing.type = Routing\nappender.query-routing.name = query-routing\nappender.query-routing.routes.type = Routes\nappender.query-routing.routes.pattern = $${ctx:queryId}\n#Purge polciy for query-based Routing Appender\nappender.query-routing.purgePolicy.type = LlapRoutingAppenderPurgePolicy\n# Note: Do not change this name without changing the corresponding entry in LlapConstants\nappender.query-routing.purgePolicy.name = llapLogPurgerQueryRouting\n# default route\nappender.query-routing.routes.route-default.type = Route\nappender.query-routing.routes.route-default.key = $${ctx:queryId}\nappender.query-routing.routes.route-default.ref = RFA\n# queryId based route\nappender.query-routing.routes.route-mdc.type = Route\nappender.query-routing.routes.route-mdc.file-mdc.type = LlapWrappedAppender\nappender.query-routing.routes.route-mdc.file-mdc.name = IrrelevantName-query-routing\nappender.query-routing.routes.route-mdc.file-mdc.app.type = RandomAccessFile\nappender.query-routing.routes.route-mdc.file-mdc.app.name = file-mdc\nappender.query-routing.routes.route-mdc.file-mdc.app.fileName = ${sys:llap.daemon.log.dir}/${ctx:queryId}-${ctx:dagId}.log\nappender.query-routing.routes.route-mdc.file-mdc.app.layout.type = PatternLayout\nappender.query-routing.routes.route-mdc.file-mdc.app.layout.pattern = %d{ISO8601} %5p [%t (%X{fragmentId})] %c{2}: %m%n\n\n# list of all loggers\nloggers = PerfLogger, EncodedReader, NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, HistoryLogger, LlapIoImpl, LlapIoOrc, LlapIoCache, LlapIoLocking, TezSM, TezSS, TezHC, LlapDaemon\n\nlogger.LlapDaemon.name = org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon\nlogger.LlapDaemon.level = INFO\n\n# shut up the Tez logs that log debug-level stuff on INFO\n\nlogger.TezSM.name = org.apache.tez.runtime.library.common.shuffle.impl.ShuffleManager.fetch\nlogger.TezSM.level = WARN\nlogger.TezSS.name = org.apache.tez.runtime.library.common.shuffle.orderedgrouped.ShuffleScheduler.fetch\nlogger.TezSS.level = WARN\nlogger.TezHC.name = org.apache.tez.http.HttpConnection.url\nlogger.TezHC.level = WARN\n\nlogger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger\nlogger.PerfLogger.level = DEBUG\n\nlogger.EncodedReader.name = org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl\nlogger.EncodedReader.level = INFO\n\nlogger.LlapIoImpl.name = LlapIoImpl\nlogger.LlapIoImpl.level = INFO\n\nlogger.LlapIoOrc.name = LlapIoOrc\nlogger.LlapIoOrc.level = WARN\n\nlogger.LlapIoCache.name = LlapIoCache\nlogger.LlapIoCache.level = WARN\n\nlogger.LlapIoLocking.name = LlapIoLocking\nlogger.LlapIoLocking.level = WARN\n\nlogger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn\nlogger.NIOServerCnxn.level = WARN\n\nlogger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO\nlogger.ClientCnxnSocketNIO.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\nlogger.HistoryLogger.name = org.apache.hadoop.hive.llap.daemon.HistoryLogger\nlogger.HistoryLogger.level = INFO\nlogger.HistoryLogger.additivity = false\nlogger.HistoryLogger.appenderRefs = HistoryAppender\nlogger.HistoryLogger.appenderRef.HistoryAppender.ref = HISTORYAPPENDER\n\n# root logger\nrootLogger.level = ${sys:llap.daemon.log.level}\nrootLogger.appenderRefs = root\nrootLogger.appenderRef.root.ref = ${sys:llap.daemon.root.logger}\n  ",
            "hive_llap_log_maxbackupindex" : "240",
            "hive_llap_log_maxfilesize" : "256"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "webhcat-log4j",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Define some default values that can be overridden by system properties\nwebhcat.root.logger = DEBUG, standard\nwebhcat.log.dir = .\nwebhcat.log.file = webhcat.log\n\nlog4j.rootLogger = ${webhcat.root.logger}, ETW, FullPIILogs\n\n# Logging Threshold\nlog4j.threshhold = DEBUG\n\nlog4j.appender.standard  =  org.apache.log4j.RollingFileAppender\nlog4j.appender.standard.File = ${webhcat.log.dir}/${webhcat.log.file}\nlog4j.appender.standard.MaxFileSize=1024MB\nlog4j.appender.standard.MaxBackupIndex=10\nlog4j.appender.standard.layout = org.apache.log4j.PatternLayout\nlog4j.appender.standard.layout.conversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Class logging settings\nlog4j.logger.com.sun.jersey = DEBUG, FullPIILogs\nlog4j.logger.com.sun.jersey.spi.container.servlet.WebComponent = ERROR, FullPIILogs\nlog4j.logger.org.apache.hadoop = INFO, FullPIILogs\nlog4j.logger.org.apache.hadoop.conf = WARN, FullPIILogs\nlog4j.logger.org.apache.zookeeper = WARN\nlog4j.logger.org.eclipse.jetty = INFO, FullPIILogs\n#EtwLog Appender\n\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=templeton\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=hive\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=INFO\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true",
            "webhcat_log_maxbackupindex" : "20",
            "webhcat_log_maxfilesize" : "256"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-logsearch-conf",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "component_mappings" : "HIVE_METASTORE:hive_metastore;HIVE_SERVER:hive_hiveserver2;WEBHCAT_SERVER:webhcat_server",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"hive_hiveserver2\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hive-env/hive_log_dir', '/var/log/hive')}}/hiveserver2.log\"\n    },\n    {\n      \"type\":\"hive_metastore\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hive-env/hive_log_dir', '/var/log/hive')}}/hivemetastore.log\"\n    },\n    {\n      \"type\": \"webhcat_server\",\n      \"rowntype\":\"service\",\n      \"path\":\"{{default('configurations/hive-env/hcat_log_dir', '/var/log/webhcat')}}/webhcat.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hive_hiveserver2\",\n            \"hive_metastore\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]:%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\\\(%{JAVAFILE:file}:%{JAVAMETHOD:method}\\\\(%{INT:line_number}\\\\)\\\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"webhcat_server\"\n          ]\n         }\n       },\n      \"log4j_format\":\" %-5p | %d{DATE} | %c | %m%n\",\n      \"multiline_pattern\":\"^(%{SPACE}%{LOGLEVEL:level}%{CUSTOM_SEPARATOR}%{CUSTOM_DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{SPACE}%{LOGLEVEL:level}%{CUSTOM_SEPARATOR}%{CUSTOM_DATESTAMP:logtime}%{CUSTOM_SEPARATOR}%{JAVACLASS:file}%{CUSTOM_SEPARATOR}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"dd MMM yyyy HH:mm:ss,SSS\"\n          }\n         },\n        \"level\":{\n           \"map_fieldvalue\":{\n             \"pre_value\":\"WARNING\",\n             \"post_value\":\"WARN\"\n            }\n        }\n       }\n     }\n   ]\n }\n    ",
            "service_name" : "Hive"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-interactive-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\nif [ \"$SERVICE\" = \"cli\" ]; then\n  if [ -z \"$DEBUG\" ]; then\n    export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:+UseParNewGC -XX:-UseGCOverheadLimit\"\n  else\n    export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:-UseGCOverheadLimit\"\n  fi\nfi\n\n# The heap size of the jvm stared by hive shell script can be controlled via:\n\nif [ \"$SERVICE\" = \"metastore\" ]; then\n  export HADOOP_HEAPSIZE={{hive_metastore_heapsize}} # Setting for HiveMetastore\nelse\n  export HADOOP_HEAPSIZE={{hive_interactive_heapsize}} # Setting for HiveServer2 and Client\nfi\n\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS  -Xmx${HADOOP_HEAPSIZE}m\"\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS{{heap_dump_opts}}\"\n\n# Larger heap size may be required when running queries over large number of files or partitions.\n# By default hive shell scripts use a heap size of 256 (MB).  Larger heap size would also be\n# appropriate for hive server (hwi etc).\n\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nHADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\n# Hive Configuration Directory can be controlled by:\nexport HIVE_CONF_DIR={{hive_server_interactive_conf_dir}}\n\n# Add additional hcatalog jars\nif [ \"${HIVE_AUX_JARS_PATH}\" != \"\" ]; then\n  export HIVE_AUX_JARS_PATH=${HIVE_AUX_JARS_PATH}\nelse\n  export HIVE_AUX_JARS_PATH=/usr/hdp/current/hive-server2-hive2/lib/hive-hcatalog-core.jar\nfi\n\nexport METASTORE_PORT={{hive_metastore_port}}\n\n# Spark assembly contains a conflicting copy of HiveConf from hive-1.2\nexport HIVE_SKIP_SPARK_ASSEMBLY=true\n\n    ",
            "enable_hive_interactive" : "false",
            "hive_aux_jars" : "",
            "hive_heapsize" : "512",
            "llap_app_name" : "llap0",
            "llap_extra_slider_opts" : "",
            "llap_headroom_space" : "12288",
            "llap_heap_size" : "0",
            "llap_java_opts" : "-XX:+AlwaysPreTouch {% if java_version > 7 %}-Xss512k -XX:+UseG1GC -XX:TLABSize=8m -XX:+ResizeTLAB -XX:+UseNUMA -XX:+AggressiveOpts -XX:InitiatingHeapOccupancyPercent=40 -XX:G1ReservePercent=20 -XX:MaxGCPauseMillis=200{% else %}-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC{% endif %}{{heap_dump_opts}}",
            "llap_log_level" : "INFO",
            "num_llap_nodes" : "1",
            "num_llap_nodes_for_llap_daemons" : "1",
            "num_retries_for_checking_llap_status" : "20",
            "slider_am_container_mb" : "341"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hiveserver2-interactive-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "hive.async.log.enabled" : "false",
            "hive.metastore.metrics.enabled" : "true",
            "hive.service.metrics.hadoop2.component" : "hiveserver2",
            "hive.service.metrics.reporter" : "HADOOP2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-log4j2",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nstatus = TRACE\nname = HiveLog4j2\n#packages = org.apache.hadoop.hive.ql.log\n\n\n# list of properties\nproperty.hive.log.level = {{hive_log_level}}\nproperty.hive.root.logger = RFA\nproperty.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}\nproperty.hive.log.file = hive.log\n\n# list of all appenders\nappenders = console, RFA, FilterLog\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nappender.FilterLog.type = FilterLog\nappender.FilterLog.name = FilterLog\nappender.FilterLog.layout.type = PatternLayout\n#appender.FilterLog.layout.pattern =   %d{ISO8601} %5p [%t] %c{2}: %m%n\nappender.FilterLog.layout.pattern = %d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\nappender.FilterLog.source=CentralFilteredHadoopServiceLogs\nappender.FilterLog.component=hiveserver2interactive\nappender.FilterLog.whitelistFile=NA\nappender.FilterLog.osType=Linux\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\n\n# daily rolling file appender\nappender.RFA.type = RollingRandomAccessFile\nappender.RFA.name = RFA\nappender.RFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}\n# Use %pid in the filePattern to append process-id@host-name to the filename if you want separate log files for different CLI session\nappender.RFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}_%i\nappender.RFA.layout.type = PatternLayout\nappender.RFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n\nappender.RFA.policies.type = Policies\nappender.RFA.policies.time.type = SizeBasedTriggeringPolicy\n\n\nappender.RFA.strategy.type = DefaultRolloverStrategy\nappender.RFA.strategy.max = 10\nappender.RFA.policies.size.type = SizeBasedTriggeringPolicy\nappender.RFA.policies.size.size = 1024MB\n\n# list of all loggers\nloggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX\n\nlogger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn\nlogger.NIOServerCnxn.level = WARN\n\nlogger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO\nlogger.ClientCnxnSocketNIO.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root, FL\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}\nrootLogger.appenderRef.FL.ref = FilterLog",
            "hive2_log_maxbackupindex" : "30",
            "hive2_log_maxfilesize" : "256"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hive-policymgr-ssl",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "xasecure.policymgr.clientssl.keystore" : "/usr/hdp/current/hive-server2/conf/ranger-plugin-keystore.jks",
            "xasecure.policymgr.clientssl.keystore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.keystore.password" : "SECRET:ranger-hive-policymgr-ssl:1:xasecure.policymgr.clientssl.keystore.password",
            "xasecure.policymgr.clientssl.truststore" : "/usr/hdp/current/hive-server2/conf/ranger-plugin-truststore.jks",
            "xasecure.policymgr.clientssl.truststore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.truststore.password" : "SECRET:ranger-hive-policymgr-ssl:1:xasecure.policymgr.clientssl.truststore.password"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "parquet-logging",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Properties file which configures the operation of the JDK\n# logging facility.\n\n# The system will look for this config file, first using\n# a System property specified at startup:\n#\n# >java -Djava.util.logging.config.file=myLoggingConfigFilePath\n#\n# If this property is not specified, then the config file is\n# retrieved from its default location at:\n#\n# JDK_HOME/jre/lib/logging.properties\n\n# Global logging properties.\n# ------------------------------------------\n# The set of handlers to be loaded upon startup.\n# Comma-separated list of class names.\n# (? LogManager docs say no comma here, but JDK example has comma.)\n# handlers=java.util.logging.ConsoleHandler\norg.apache.parquet.handlers= java.util.logging.FileHandler\n\n# Default global logging level.\n# Loggers and Handlers may override this level\n.level=INFO\n\n# Handlers\n# -----------------------------------------\n\n# --- ConsoleHandler ---\n# Override of global logging level\njava.util.logging.ConsoleHandler.level=INFO\njava.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter\njava.util.logging.SimpleFormatter.format=[%1$tc] %4$s: %2$s - %5$s %6$s%n\n\n# --- FileHandler ---\n# Override of global logging level\njava.util.logging.FileHandler.level=ALL\n\n# Naming style for the output file:\n# (The output file is placed in the system temporary directory.\n# %u is used to provide unique identifier for the file.\n# For more information refer\n# https://docs.oracle.com/javase/7/docs/api/java/util/logging/FileHandler.html)\njava.util.logging.FileHandler.pattern=%t/parquet-%u.log\n\n# Limiting size of output file in bytes:\njava.util.logging.FileHandler.limit=50000000\n\n# Number of output files to cycle through, by appending an\n# integer to the base file name:\njava.util.logging.FileHandler.count=1\n\n# Style of output (Simple or XML):\njava.util.logging.FileHandler.formatter=java.util.logging.SimpleFormatter\n\n    "
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hcat-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n      # Licensed to the Apache Software Foundation (ASF) under one\n      # or more contributor license agreements. See the NOTICE file\n      # distributed with this work for additional information\n      # regarding copyright ownership. The ASF licenses this file\n      # to you under the Apache License, Version 2.0 (the\n      # \"License\"); you may not use this file except in compliance\n      # with the License. You may obtain a copy of the License at\n      #\n      # http://www.apache.org/licenses/LICENSE-2.0\n      #\n      # Unless required by applicable law or agreed to in writing, software\n      # distributed under the License is distributed on an \"AS IS\" BASIS,\n      # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n      # See the License for the specific language governing permissions and\n      # limitations under the License.\n\n      JAVA_HOME={{java64_home}}\n      HCAT_PID_DIR={{hcat_pid_dir}}/\n      HCAT_LOG_DIR={{hcat_log_dir}}/\n      HCAT_CONF_DIR={{hcat_conf_dir}}\n      HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n      #DBROOT is the path where the connector jars are downloaded\n      DBROOT={{hcat_dbroot}}\n      USER={{hcat_user}}\n      METASTORE_PORT={{hive_metastore_port}}\n    "
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ambari.hive.db.schema.name" : "hive",
            "atlas.hook.hive.maxThreads" : "1",
            "atlas.hook.hive.minThreads" : "1",
            "datanucleus.autoCreateSchema" : "false",
            "datanucleus.cache.level2.type" : "none",
            "datanucleus.connectionPool.maxIdle" : "2",
            "datanucleus.connectionPoolingType" : "HikariCP",
            "datanucleus.fixedDatastore" : "true",
            "fs.azure.delete.threads" : "128",
            "fs.azure.rename.threads" : "128",
            "hive.auto.convert.join" : "true",
            "hive.auto.convert.join.noconditionaltask" : "true",
            "hive.auto.convert.join.noconditionaltask.size" : "319039733",
            "hive.auto.convert.sortmerge.join" : "false",
            "hive.auto.convert.sortmerge.join.to.mapjoin" : "false",
            "hive.cbo.enable" : "true",
            "hive.cli.print.header" : "false",
            "hive.cluster.delegation.token.store.class" : "org.apache.hadoop.hive.thrift.ZooKeeperTokenStore",
            "hive.cluster.delegation.token.store.zookeeper.connectString" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "hive.cluster.delegation.token.store.zookeeper.znode" : "/hive/cluster/delegation",
            "hive.compactor.abortedtxn.threshold" : "1000",
            "hive.compactor.check.interval" : "300L",
            "hive.compactor.delta.num.threshold" : "10",
            "hive.compactor.delta.pct.threshold" : "0.1f",
            "hive.compactor.initiator.on" : "false",
            "hive.compactor.worker.threads" : "0",
            "hive.compactor.worker.timeout" : "86400L",
            "hive.compute.query.using.stats" : "true",
            "hive.conf.restricted.list" : "hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role",
            "hive.convert.join.bucket.mapjoin.tez" : "false",
            "hive.default.fileformat" : "TextFile",
            "hive.default.fileformat.managed" : "TextFile",
            "hive.enforce.bucketing" : "true",
            "hive.enforce.sorting" : "true",
            "hive.enforce.sortmergebucketmapjoin" : "true",
            "hive.exec.compress.intermediate" : "false",
            "hive.exec.compress.output" : "false",
            "hive.exec.dynamic.partition" : "true",
            "hive.exec.dynamic.partition.mode" : "nonstrict",
            "hive.exec.failure.hooks" : "org.apache.hadoop.hive.ql.hooks.ATSHook",
            "hive.exec.max.created.files" : "100000",
            "hive.exec.max.dynamic.partitions" : "5000",
            "hive.exec.max.dynamic.partitions.pernode" : "2000",
            "hive.exec.orc.compression.strategy" : "SPEED",
            "hive.exec.orc.default.compress" : "ZLIB",
            "hive.exec.orc.default.stripe.size" : "67108864",
            "hive.exec.orc.encoding.strategy" : "SPEED",
            "hive.exec.parallel" : "false",
            "hive.exec.parallel.thread.number" : "8",
            "hive.exec.post.hooks" : "org.apache.hadoop.hive.ql.hooks.ATSHook",
            "hive.exec.pre.hooks" : "org.apache.hadoop.hive.ql.hooks.ATSHook",
            "hive.exec.reducers.bytes.per.reducer" : "67108864",
            "hive.exec.reducers.max" : "1009",
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.exec.submit.local.task.via.child" : "true",
            "hive.exec.submitviachild" : "false",
            "hive.execution.engine" : "tez",
            "hive.exim.uri.scheme.whitelist" : "wasb,hdfs,pfile,wasbs,abfs,abfss,adls,adlss",
            "hive.fetch.task.aggr" : "false",
            "hive.fetch.task.conversion" : "more",
            "hive.fetch.task.conversion.threshold" : "1073741824",
            "hive.hmshandler.retry.attempts" : "5",
            "hive.hmshandler.retry.interval" : "1000",
            "hive.limit.optimize.enable" : "true",
            "hive.limit.pushdown.memory.usage" : "0.04",
            "hive.map.aggr" : "true",
            "hive.map.aggr.hash.force.flush.memory.threshold" : "0.9",
            "hive.map.aggr.hash.min.reduction" : "0.5",
            "hive.map.aggr.hash.percentmemory" : "0.5",
            "hive.mapjoin.bucket.cache.size" : "10000",
            "hive.mapjoin.optimized.hashtable" : "true",
            "hive.mapred.reduce.tasks.speculative.execution" : "false",
            "hive.merge.mapfiles" : "true",
            "hive.merge.mapredfiles" : "false",
            "hive.merge.orcfile.stripe.level" : "true",
            "hive.merge.rcfile.block.level" : "true",
            "hive.merge.size.per.task" : "256000000",
            "hive.merge.smallfiles.avgsize" : "16000000",
            "hive.merge.tezfiles" : "false",
            "hive.metastore.authorization.storage.checks" : "false",
            "hive.metastore.cache.pinobjtypes" : "Table,Database,Type,FieldSchema,Order",
            "hive.metastore.client.connect.retry.delay" : "5s",
            "hive.metastore.client.socket.timeout" : "1800s",
            "hive.metastore.connect.retries" : "5",
            "hive.metastore.execute.setugi" : "false",
            "hive.metastore.failure.retries" : "24",
            "hive.metastore.kerberos.keytab.file" : "/etc/security/keytabs/hive.service.keytab",
            "hive.metastore.kerberos.principal" : "hive/_HOST@EXAMPLE.COM",
            "hive.metastore.pre.event.listeners" : "org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener",
            "hive.metastore.sasl.enabled" : "false",
            "hive.metastore.schema.verification" : "true",
            "hive.metastore.server.max.threads" : "100000",
            "hive.metastore.uris" : "thrift://hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:9083,thrift://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:9083",
            "hive.metastore.warehouse.dir" : "/hive/warehouse",
            "hive.mv.files.thread" : "128",
            "hive.optimize.bucketmapjoin" : "true",
            "hive.optimize.bucketmapjoin.sortedmerge" : "false",
            "hive.optimize.constant.propagation" : "true",
            "hive.optimize.index.filter" : "true",
            "hive.optimize.metadataonly" : "true",
            "hive.optimize.null.scan" : "true",
            "hive.optimize.reducededuplication" : "true",
            "hive.optimize.reducededuplication.min.reducer" : "4",
            "hive.optimize.sort.dynamic.partition" : "true",
            "hive.orc.compute.splits.num.threads" : "10",
            "hive.orc.splits.include.file.footer" : "false",
            "hive.prewarm.enabled" : "false",
            "hive.prewarm.numcontainers" : "3",
            "hive.scratchdir.lock" : "true",
            "hive.security.authenticator.manager" : "org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator",
            "hive.security.authorization.enabled" : "false",
            "hive.security.authorization.manager" : "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory",
            "hive.security.metastore.authenticator.manager" : "org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator",
            "hive.security.metastore.authorization.auth.reads" : "true",
            "hive.security.metastore.authorization.manager" : "org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly",
            "hive.server2.allow.user.substitution" : "true",
            "hive.server2.authentication" : "NONE",
            "hive.server2.authentication.kerberos.keytab" : "/etc/security/keytabs/hive.service.keytab",
            "hive.server2.authentication.kerberos.principal" : "hive/_HOST@EXAMPLE.COM",
            "hive.server2.authentication.ldap.url" : " ",
            "hive.server2.authentication.spnego.keytab" : "HTTP/_HOST@EXAMPLE.COM",
            "hive.server2.authentication.spnego.principal" : "/etc/security/keytabs/spnego.service.keytab",
            "hive.server2.clear.dangling.scratchdir" : "true",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.logging.operation.enabled" : "true",
            "hive.server2.logging.operation.log.location" : "/tmp/hive/operation_logs",
            "hive.server2.max.start.attempts" : "5",
            "hive.server2.support.dynamic.service.discovery" : "true",
            "hive.server2.table.type.mapping" : "CLASSIC",
            "hive.server2.tez.default.queues" : "default",
            "hive.server2.tez.initialize.default.sessions" : "false",
            "hive.server2.tez.sessions.per.default.queue" : "1",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10001",
            "hive.server2.thrift.max.worker.threads" : "500",
            "hive.server2.thrift.port" : "10000",
            "hive.server2.thrift.sasl.qop" : "auth",
            "hive.server2.transport.mode" : "http",
            "hive.server2.use.SSL" : "false",
            "hive.server2.zookeeper.namespace" : "hiveserver2",
            "hive.smbjoin.cache.rows" : "10000",
            "hive.start.cleanup.scratchdir" : "false",
            "hive.stats.autogather" : "true",
            "hive.stats.dbclass" : "fs",
            "hive.stats.fetch.column.stats" : "true",
            "hive.stats.fetch.partition.stats" : "true",
            "hive.support.concurrency" : "false",
            "hive.tez.auto.reducer.parallelism" : "true",
            "hive.tez.container.size" : "1536",
            "hive.tez.cpu.vcores" : "-1",
            "hive.tez.dynamic.partition.pruning" : "true",
            "hive.tez.dynamic.partition.pruning.max.data.size" : "104857600",
            "hive.tez.dynamic.partition.pruning.max.event.size" : "1048576",
            "hive.tez.exec.print.summary" : "true",
            "hive.tez.input.format" : "org.apache.hadoop.hive.ql.io.HiveInputFormat",
            "hive.tez.java.opts" : "-Xmx1024M -Xms1024M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",
            "hive.tez.log.level" : "INFO",
            "hive.tez.max.partition.factor" : "3f",
            "hive.tez.min.partition.factor" : "1f",
            "hive.tez.smb.number.waves" : "0.5",
            "hive.txn.manager" : "org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager",
            "hive.txn.max.open.batch" : "1000",
            "hive.txn.timeout" : "300",
            "hive.user.install.directory" : "/user",
            "hive.vectorized.execution.enabled" : "true",
            "hive.vectorized.execution.reduce.enabled" : "true",
            "hive.vectorized.groupby.checkinterval" : "4096",
            "hive.vectorized.groupby.flush.percent" : "0.1",
            "hive.vectorized.groupby.maxentries" : "100000",
            "hive.warehouse.subdir.inherit.perms" : "true",
            "hive.zookeeper.client.port" : "2181",
            "hive.zookeeper.namespace" : "hive_zookeeper_namespace",
            "hive.zookeeper.quorum" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "javax.jdo.option.ConnectionDriverName" : "com.microsoft.sqlserver.jdbc.SQLServerDriver",
            "javax.jdo.option.ConnectionPassword" : "SECRET:hive-site:1:javax.jdo.option.ConnectionPassword",
            "javax.jdo.option.ConnectionURL" : "jdbc:sqlserver://rfkowvb6yq.database.windows.net;databaseName=v368131f056aa2d44e882781a655de2485ehivemetastore;trustServerCertificate=false;encrypt=true;hostNameInCertificate=*.database.windows.net;",
            "javax.jdo.option.ConnectionUserName" : "v368131f056aa2d44e882781a655de2485ehivemetastoreLogin@rfkowvb6yq.database.windows.net",
            "tez.runtime.shuffle.connect.timeout" : "30000",
            "tez.shuffle-vertex-manager.max-src-fraction" : "0.95",
            "tez.shuffle-vertex-manager.min-src-fraction" : "0.9"
          },
          "properties_attributes" : {
            "hidden" : {
              "javax.jdo.option.ConnectionPassword" : "HIVE_CLIENT,WEBHCAT_SERVER,HCAT,CONFIG_DOWNLOAD"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hive-plugin-properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "REPOSITORY_CONFIG_PASSWORD" : "SECRET:ranger-hive-plugin-properties:1:REPOSITORY_CONFIG_PASSWORD",
            "REPOSITORY_CONFIG_USERNAME" : "hive",
            "common.name.for.certificate" : "",
            "external_admin_password" : "",
            "external_admin_username" : "",
            "external_ranger_admin_password" : "",
            "external_ranger_admin_username" : "",
            "jdbc.driverClassName" : "org.apache.hive.jdbc.HiveDriver",
            "policy_user" : "ambari-qa"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "webhcat-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "fs.azure.skip.metrics" : "true",
            "templeton.enable.job.reconnect.default" : "true",
            "templeton.exec.timeout" : "60000",
            "templeton.hadoop" : "/usr/hdp/${hdp.version}/hadoop/bin/hadoop",
            "templeton.hadoop.conf.dir" : "/etc/hadoop/conf",
            "templeton.hadoop.queue.name" : "default",
            "templeton.hcat" : "/usr/hdp/${hdp.version}/hive/bin/hcat",
            "templeton.hcat.home" : "hive.tar.gz/hive/hcatalog",
            "templeton.hive.archive" : "wasb:///hdp/apps/${hdp.version}/hive/hive.tar.gz",
            "templeton.hive.extra.files" : "/usr/hdp/${hdp.version}/tez/conf/tez-site.xml",
            "templeton.hive.home" : "hive.tar.gz/hive",
            "templeton.hive.path" : "hive.tar.gz/hive/bin/hive",
            "templeton.hive.properties" : "hive.metastore.local=false,hive.metastore.uris=thrift://hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:9083\\,thrift://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:9083,hive.user.install.directory=/user,hive.root.logger=WARN\\,console,hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.ATSHook,hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.ATSHook,hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.ATSHook",
            "templeton.jar" : "/usr/hdp/${hdp.version}/hive/share/webhcat/svr/lib/hive-webhcat-*.jar",
            "templeton.job.list.timeout" : "90",
            "templeton.job.status.timeout" : "90",
            "templeton.job.submit.timeout" : "90",
            "templeton.job.timeout.task.retry.count" : "10",
            "templeton.job.timeout.task.retry.interval" : "10",
            "templeton.jobs.listorder" : "lexicographicaldesc",
            "templeton.libjars" : "/usr/hdp/${hdp.version}/zookeeper/zookeeper.jar,/usr/hdp/${hdp.version}/hive/lib/hive-common.jar",
            "templeton.mapper.memory.mb" : "1024",
            "templeton.override.enabled" : "false",
            "templeton.parallellism.job.list" : "5",
            "templeton.parallellism.job.status" : "50",
            "templeton.parallellism.job.submit" : "15",
            "templeton.pig.archive" : "wasb:///hdp/apps/${hdp.version}/pig/pig.tar.gz",
            "templeton.pig.path" : "pig.tar.gz/pig/bin/pig",
            "templeton.port" : "30111",
            "templeton.python" : "${env.PYTHON_CMD}",
            "templeton.sqoop.archive" : "wasb:///hdp/apps/${hdp.version}/sqoop/sqoop.tar.gz",
            "templeton.sqoop.home" : "/usr/hdp/${hdp.version}/sqoop",
            "templeton.sqoop.path" : "/usr/hdp/${hdp.version}/sqoop/bin/sqoop",
            "templeton.storage.class" : "org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage",
            "templeton.streaming.jar" : "wasb:///hdp/apps/${hdp.version}/mapreduce/hadoop-streaming.jar",
            "templeton.zookeeper.hosts" : "localhost:2181",
            "yarn.app.mapreduce.client.job.retry-interval" : "1000",
            "yarn.app.mapreduce.client.max-retries" : "2",
            "yarn.resourcemanager.connect.max-wait.ms" : "40000",
            "yarn.resourcemanager.connect.retry-interval.ms" : "10000"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "tez-interactive-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "tez.am.am-rm.heartbeat.interval-ms.max" : "10000",
            "tez.am.client.heartbeat.poll.interval.millis" : "6000",
            "tez.am.client.heartbeat.timeout.secs" : "90",
            "tez.am.node-blacklisting.enabled" : "false",
            "tez.am.resource.memory.mb" : "1536",
            "tez.am.task.listener.thread-count" : "1",
            "tez.am.task.reschedule.higher.priority" : "false",
            "tez.container.max.java.heap.fraction" : "-1",
            "tez.dag.recovery.enabled" : "false",
            "tez.grouping.node.local.only" : "true",
            "tez.history.logging.log.level" : "TASK_ATTEMPT",
            "tez.history.logging.taskattempt-filters" : "SERVICE_BUSY,EXTERNAL_PREEMPTION",
            "tez.history.logging.timeline.num-dags-per-group" : "5",
            "tez.lib.uris" : "/hdp/apps/${hdp.version}/tez_hive2/tez.tar.gz",
            "tez.runtime.enable.final-merge.in.output" : "false",
            "tez.runtime.io.sort.mb" : "512",
            "tez.runtime.pipelined-shuffle.enabled" : "false",
            "tez.runtime.pipelined.sorter.lazy-allocate.memory" : "true",
            "tez.runtime.report.partition.stats" : "true",
            "tez.runtime.shuffle.connect.timeout" : "30000",
            "tez.runtime.shuffle.fetch.buffer.percent" : "0.6",
            "tez.runtime.shuffle.fetch.verify-disk-checksum" : "false",
            "tez.runtime.shuffle.keep-alive.enabled" : "true",
            "tez.runtime.shuffle.memory.limit.percent" : "0.25",
            "tez.runtime.shuffle.parallel.copies" : "8",
            "tez.runtime.shuffle.read.timeout" : "30000",
            "tez.runtime.shuffle.ssl.enable" : "false",
            "tez.runtime.unordered.output.buffer.size-mb" : "100",
            "tez.session.am.dag.submit.timeout.secs" : "1209600",
            "tez.task.heartbeat.timeout.check-ms" : "15000",
            "tez.task.timeout-ms" : "90000"
          },
          "properties_attributes" : {
            "final" : {
              "tez.runtime.shuffle.ssl.enable" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-exec-log4j",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Define some default values that can be overridden by system properties\n\nhive.log.threshold=ALL\nhive.root.logger=INFO,FA\nhive.log.dir=${java.io.tmpdir}/${user.name}\nhive.query.id=hadoop\nhive.log.file=${hive.query.id}.log\n\n# Define the root logger to the system property \\\"hadoop.root.logger\\\".\nlog4j.rootLogger=${hive.root.logger}, EventCounter,FullPIILogs\n# Logging Threshold\nlog4j.threshhold=${hive.log.threshold}\n\n#\n# File Appender\n#\n\nlog4j.appender.FA=org.apache.log4j.FileAppender\nlog4j.appender.FA.File=${hive.log.dir}/${hive.log.file}\nlog4j.appender.FA.layout=org.apache.log4j.PatternLayout\n\n# Pattern format: Date LogLevel LoggerName LogMessage\n# Pattern format: Date LogLevel LoggerName LogMessage\n# Debugging Pattern format\nlog4j.appender.FA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\n\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\n\n#custom logging levels\n#log4j.logger.xxx=DEBUG\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter\n\n\nlog4j.category.DataNucleus=WARN,FA,FullPIILogs\nlog4j.category.Datastore=WARN,FA,FullPIILogs\nlog4j.category.Datastore.Schema=WARN,FA,FullPIILogs\nlog4j.category.JPOX.Datastore=WARN,FA,FullPIILogs\nlog4j.category.JPOX.Plugin=WARN,FA,FullPIILogs\nlog4j.category.JPOX.MetaData=WARN,FA,FullPIILogs\nlog4j.category.JPOX.Query=WARN,FA,FullPIILogs\nlog4j.category.JPOX.General=WARN,FA,FullPIILogs\nlog4j.category.JPOX.Enhancer=WARN,FA,FullPIILogs\n\n\n# Silence useless ZK logs\nlog4j.logger.org.apache.zookeeper.server.NIOServerCnxn=ERROR,FA,FullPIILogs\nlog4j.logger.org.apache.zookeeper.ClientCnxnSocketNIO=ERROR,FA,FullPIILogs\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=hive\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=INFO\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hivemetastore-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "hive.metastore.metrics.enabled" : "true",
            "hive.service.metrics.hadoop2.component" : "hivemetastore",
            "hive.service.metrics.reporter" : "HADOOP2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "webhcat-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# The file containing the running pid\nPID_FILE={{webhcat_pid_file}}\n\nTEMPLETON_LOG_DIR={{templeton_log_dir}}/\n\n\nWEBHCAT_LOG_DIR={{templeton_log_dir}}/\n\n# The console error log\nERROR_LOG={{templeton_log_dir}}/webhcat-console-error.log\n\n# The console log\nCONSOLE_LOG={{templeton_log_dir}}/webhcat-console.log\n\n#TEMPLETON_JAR=templeton_jar_name\n\n#HADOOP_PREFIX=hadoop_prefix\n\n#HCAT_PREFIX=hive_prefix\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n    "
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-exec-log4j2",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.status = INFO\n\nname = HiveExecLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.hive.log.level = {{hive_log_level}}\nproperty.hive.root.logger = FA\nproperty.hive.query.id = hadoop\nproperty.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}\nproperty.hive.log.file = ${sys:hive.query.id}.log\n\n# list of all appenders\nappenders = console, FA, FilterLog\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nappender.FilterLog.type = FilterLog\nappender.FilterLog.name = FilterLog\nappender.FilterLog.layout.type = PatternLayout\n#appender.FilterLog.layout.pattern =   %d{ISO8601} %5p [%t] %c{2}: %m%n\nappender.FilterLog.layout.pattern = %d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\nappender.FilterLog.source=CentralFilteredHadoopServiceLogs\nappender.FilterLog.component=hiveexec\nappender.FilterLog.whitelistFile=NA\nappender.FilterLog.osType=Linux\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\n\n# simple file appender\nappender.FA.type = File\nappender.FA.name = FA\nappender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}\nappender.FA.layout.type = PatternLayout\nappender.FA.layout.pattern = %d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\n\n# list of all loggers\nloggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX\n\nlogger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn\nlogger.NIOServerCnxn.level = WARN\n\nlogger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO\nlogger.ClientCnxnSocketNIO.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root, FL\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}\nrootLogger.appenderRef.FL.ref = FilterLog"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hiveserver2-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "hive.conf.restricted.list" : "hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role",
            "hive.metastore.metrics.enabled" : "true",
            "hive.security.authenticator.manager" : "org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator",
            "hive.security.authorization.enabled" : "false",
            "hive.security.authorization.manager" : "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory",
            "hive.service.metrics.hadoop2.component" : "hiveserver2",
            "hive.service.metrics.reporter" : "HADOOP2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-log4j",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhive.log.threshold=ALL\nhive.root.logger=DEBUG,RFA\nhive.log.dir=${java.io.tmpdir}/${user.name}\nhive.log.file=hive.log\n\nlog4jhive.log.maxfilesize=1024MB\nlog4jhive.log.maxbackupindex=10\n\n\nlog4j.rootLogger=${hive.root.logger}, EventCounter, ETW, FullPIILogs\nlog4j.threshold=${hive.log.threshold}\n\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hive.log.dir}/${hive.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jhive.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jhive.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\nlog4j.appender.console.encoding=UTF-8\n\n#custom logging levels\n#log4j.logger.xxx=DEBUG\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter\n\n\nlog4j.category.DataNucleus=WARN,RFA,ETW,FullPIILogs\nlog4j.category.Datastore=WARN,RFA,ETW,FullPIILogs\nlog4j.category.Datastore.Schema=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.Datastore=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.Plugin=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.MetaData=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.Query=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.General=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.Enhancer=WARN,RFA,ETW,FullPIILogs\n\n# Silence useless ZK logs\nlog4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN,RFA\nlog4j.logger.org.apache.zookeeper.ClientCnxnSocketNIO=WARN,RFA\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=hive\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=hive\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=INFO\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true",
            "hive_log_maxbackupindex" : "30",
            "hive_log_maxfilesize" : "256"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-atlas-application.properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "atlas.hook.hive.keepAliveTime" : "10",
            "atlas.hook.hive.maxThreads" : "5",
            "atlas.hook.hive.minThreads" : "5",
            "atlas.hook.hive.numRetries" : "3",
            "atlas.hook.hive.queueSize" : "1000",
            "atlas.hook.hive.synchronous" : "false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "beeline-log4j2",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nstatus = INFO\nname = BeelineLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.hive.log.level = {{hive_log_level}}\nproperty.hive.root.logger = console\n\n# list of all appenders\nappenders = console\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\n\n# list of all loggers\nloggers = HiveConnection\n\n# HiveConnection logs useful info for dynamic service discovery\nlogger.HiveConnection.name = org.apache.hive.jdbc.HiveConnection\nlogger.HiveConnection.level = INFO\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}\n  "
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "alert_ldap_password" : "",
            "alert_ldap_username" : "",
            "content" : "\nexport HADOOP_USER_CLASSPATH_FIRST=true  #this prevents old metrics libs from mapreduce lib from bringing in old jar deps overriding HIVE_LIB\nif [ \"$SERVICE\" = \"cli\" ]; then\n  if [ -z \"$DEBUG\" ]; then\n    export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:+UseNUMA -XX:+UseParallelGC -XX:-UseGCOverheadLimit\"\n  else\n    export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:-UseGCOverheadLimit\"\n  fi\nfi\n\n# The heap size of the jvm stared by hive shell script can be controlled via:\n\nif [ \"$SERVICE\" = \"metastore\" ]; then\n  export HADOOP_HEAPSIZE={{hive_metastore_heapsize}} # Setting for HiveMetastore\nelse\n  export HADOOP_HEAPSIZE={{hive_heapsize}} # Setting for HiveServer2 and Client\nfi\n\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS  -Xmx${HADOOP_HEAPSIZE}m\"\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS{{heap_dump_opts}}\"\n\n# Larger heap size may be required when running queries over large number of files or partitions.\n# By default hive shell scripts use a heap size of 256 (MB).  Larger heap size would also be\n# appropriate for hive server (hwi etc).\n\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nHADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\nexport HIVE_HOME=${HIVE_HOME:-{{hive_home_dir}}}\n\n# Hive Configuration Directory can be controlled by:\nexport HIVE_CONF_DIR=${HIVE_CONF_DIR:-{{hive_config_dir}}}\n\n# Folder containing extra libraries required for hive compilation/execution can be controlled by:\nexport HIVE_AUX_JARS_PATH={{stack_root}}/current/ext/hive\nif [ \"${HIVE_AUX_JARS_PATH}\" != \"\" ]; then\n  if [ -f \"${HIVE_AUX_JARS_PATH}\" ] || [ -d \"${HIVE_AUX_JARS_PATH}\" ] ; then\n    export HIVE_AUX_JARS_PATH=${HIVE_AUX_JARS_PATH}\n  elif [ -d \"/usr/hdp/current/hive-webhcat/share/hcatalog\" ]; then\n    export HIVE_AUX_JARS_PATH=/usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar\n  fi\nelif [ -d \"/usr/hdp/current/hive-webhcat/share/hcatalog\" ]; then\n  export HIVE_AUX_JARS_PATH=/usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar\nfi\n\nexport METASTORE_PORT={{hive_metastore_port}}\n\n{% if sqla_db_used or lib_dir_available %}\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:{{jdbc_libs_dir}}\"\nexport JAVA_LIBRARY_PATH=\"$JAVA_LIBRARY_PATH:{{jdbc_libs_dir}}\"\n{% endif %}\n    ",
            "enable_heap_dump" : "false",
            "hcat_log_dir" : "/var/log/webhcat",
            "hcat_pid_dir" : "/var/run/webhcat",
            "hcat_user" : "hcat",
            "heap_dump_location" : "/tmp",
            "hive.atlas.hook" : "false",
            "hive.client.heapsize" : "512",
            "hive.heapsize" : "5376",
            "hive.log.level" : "INFO",
            "hive.metastore.heapsize" : "1792",
            "hive_ambari_database" : "MySQL",
            "hive_database" : "Existing MSSQL Server database with SQL authentication",
            "hive_database_name" : "v368131f056aa2d44e882781a655de2485ehivemetastore",
            "hive_database_type" : "mssql",
            "hive_exec_orc_storage_strategy" : "SPEED",
            "hive_existing_mssql_server_database" : "v368131f056aa2d44e882781a655de2485ehivemetastore",
            "hive_existing_mssql_server_host" : "rfkowvb6yq.database.windows.net",
            "hive_hostname" : "rfkowvb6yq.database.windows.net",
            "hive_log_dir" : "/var/log/hive",
            "hive_pid_dir" : "/var/run/hive",
            "hive_security_authorization" : "None",
            "hive_timeline_logging_enabled" : "true",
            "hive_txn_acid" : "off",
            "hive_user" : "hive",
            "hive_user_nofile_limit" : "32000",
            "hive_user_nproc_limit" : "16000",
            "webhcat_user" : "hcat"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-interactive-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "dfs.client.mmap.enabled" : "false",
            "dfs.short.circuit.shared.memory.watcher.interrupt.check.ms" : "0",
            "hive.auto.convert.join.noconditionaltask.size" : "1000000000",
            "hive.driver.parallel.compilation" : "true",
            "hive.druid.basePersistDirectory" : "",
            "hive.druid.bitmap.type" : "roaring",
            "hive.druid.broker.address.default" : "localhost:8082",
            "hive.druid.coordinator.address.default" : "localhost:8082",
            "hive.druid.http.read.timeout" : "PT10M",
            "hive.druid.indexer.memory.rownum.max" : "75000",
            "hive.druid.indexer.partition.size.max" : "1000000",
            "hive.druid.indexer.segments.granularity" : "DAY",
            "hive.druid.maxTries" : "5",
            "hive.druid.metadata.db.type" : "mysql",
            "hive.druid.metadata.password" : "SECRET:hive-interactive-site:1:hive.druid.metadata.password",
            "hive.druid.metadata.uri" : "jdbc:mysql://localhost:3355/druid",
            "hive.druid.metadata.username" : "druid",
            "hive.druid.passiveWaitTimeMs" : "30000",
            "hive.druid.select.distribute" : "true",
            "hive.druid.storage.storageDirectory" : "{{druid_storage_dir}}",
            "hive.druid.working.directory" : "/tmp/druid-indexing",
            "hive.exec.orc.split.strategy" : "HYBRID",
            "hive.execution.engine" : "tez",
            "hive.execution.mode" : "llap",
            "hive.llap.auto.allow.uber" : "false",
            "hive.llap.client.consistent.splits" : "true",
            "hive.llap.daemon.am.liveness.heartbeat.interval.ms" : "10000ms",
            "hive.llap.daemon.logger" : "query-routing",
            "hive.llap.daemon.num.executors" : "1",
            "hive.llap.daemon.queue.name" : "default",
            "hive.llap.daemon.rpc.port" : "0",
            "hive.llap.daemon.service.hosts" : "@llap0",
            "hive.llap.daemon.task.scheduler.enable.preemption" : "true",
            "hive.llap.daemon.vcpus.per.instance" : "${hive.llap.daemon.num.executors}",
            "hive.llap.daemon.yarn.container.mb" : "0",
            "hive.llap.daemon.yarn.shuffle.port" : "15551",
            "hive.llap.enable.grace.join.in.llap" : "false",
            "hive.llap.execution.mode" : "only",
            "hive.llap.io.enabled" : "true",
            "hive.llap.io.memory.mode" : "",
            "hive.llap.io.memory.size" : "0",
            "hive.llap.io.threadpool.size" : "2",
            "hive.llap.io.use.lrfu" : "true",
            "hive.llap.management.rpc.port" : "15004",
            "hive.llap.object.cache.enabled" : "true",
            "hive.llap.task.scheduler.locality.delay" : "-1",
            "hive.llap.zk.sm.connectionString" : "localhost:2181",
            "hive.map.aggr.hash.min.reduction" : "0.99",
            "hive.mapjoin.hybridgrace.hashtable" : "false",
            "hive.merge.nway.joins" : "false",
            "hive.metastore.event.listeners" : "",
            "hive.metastore.uris" : "",
            "hive.optimize.dynamic.partition.hashjoin" : "true",
            "hive.prewarm.enabled" : "false",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.tez.default.queues" : "default",
            "hive.server2.tez.initialize.default.sessions" : "true",
            "hive.server2.tez.sessions.custom.queue.allowed" : "ignore",
            "hive.server2.tez.sessions.per.default.queue" : "1",
            "hive.server2.tez.sessions.restricted.configs" : "hive.execution.mode,hive.execution.engine",
            "hive.server2.thrift.http.port" : "10501",
            "hive.server2.thrift.port" : "10500",
            "hive.server2.webui.port" : "10502",
            "hive.server2.webui.use.ssl" : "false",
            "hive.server2.zookeeper.namespace" : "hiveserver2-hive2",
            "hive.tez.bucket.pruning" : "true",
            "hive.tez.cartesian-product.enabled" : "true",
            "hive.tez.container.size" : "682",
            "hive.tez.exec.print.summary" : "true",
            "hive.tez.input.generate.consistent.splits" : "true",
            "hive.vectorized.execution.mapjoin.minmax.enabled" : "true",
            "hive.vectorized.execution.mapjoin.native.enabled" : "true",
            "hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled" : "true",
            "hive.vectorized.execution.reduce.enabled" : "true",
            "hive.vectorized.groupby.maxentries" : "1000000",
            "llap.shuffle.connection-keep-alive.enable" : "true",
            "llap.shuffle.connection-keep-alive.timeout" : "60"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hive-audit",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ranger.plugin.hive.ambari.cluster.name" : "{{cluster_name}}",
            "xasecure.audit.destination.hdfs" : "true",
            "xasecure.audit.destination.hdfs.batch.filespool.dir" : "/var/log/hive/audit/hdfs/spool",
            "xasecure.audit.destination.hdfs.dir" : "hdfs://NAMENODE_HOSTNAME:8020/ranger/audit",
            "xasecure.audit.destination.solr" : "false",
            "xasecure.audit.destination.solr.batch.filespool.dir" : "/var/log/hive/audit/solr/spool",
            "xasecure.audit.destination.solr.urls" : "",
            "xasecure.audit.destination.solr.zookeepers" : "NONE",
            "xasecure.audit.is.enabled" : "true",
            "xasecure.audit.provider.summary.enabled" : "false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hive-security",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ranger.plugin.hive.policy.cache.dir" : "/etc/ranger/{{repo_name}}/policycache",
            "ranger.plugin.hive.policy.pollIntervalMs" : "30000",
            "ranger.plugin.hive.policy.rest.ssl.config.file" : "/usr/hdp/current/{{ranger_hive_component}}/conf/conf.server/ranger-policymgr-ssl.xml",
            "ranger.plugin.hive.policy.rest.url" : "{{policymgr_mgr_url}}",
            "ranger.plugin.hive.policy.source.impl" : "org.apache.ranger.admin.client.RangerAdminRESTClient",
            "ranger.plugin.hive.service.name" : "{{repo_name}}",
            "ranger.plugin.hive.urlauth.filesystem.schemes" : "hdfs:,file:,wasb:,adl:",
            "xasecure.hive.update.xapolicies.on.grant.revoke" : "true"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553531375,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "HIVE",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=HIVE&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "llap-cli-log4j2",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nstatus = WARN\nname = LlapCliLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.hive.log.level = INFO\nproperty.hive.root.logger = console\nproperty.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}\nproperty.hive.log.file = llap-cli.log\nproperty.hive.llapstatus.consolelogger.level = INFO\n\n# list of all appenders\nappenders = console, RFA, llapstatusconsole,FilterLog\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nappender.FilterLog.type = FilterLog\nappender.FilterLog.name = FilterLog\nappender.FilterLog.layout.type = PatternLayout\n#appender.FilterLog.layout.pattern =   %d{ISO8601} %5p [%t] %c{2}: %m%n\nappender.FilterLog.layout.pattern = %d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\nappender.FilterLog.source=CentralFilteredHadoopServiceLogs\nappender.FilterLog.component=llapcli\nappender.FilterLog.whitelistFile=NA\nappender.FilterLog.osType=Linux\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %p %c{2}: %m%n\n\n# llapstatusconsole appender\nappender.llapstatusconsole.type = Console\nappender.llapstatusconsole.name = llapstatusconsole\nappender.llapstatusconsole.target = SYSTEM_OUT\nappender.llapstatusconsole.layout.type = PatternLayout\nappender.llapstatusconsole.layout.pattern = %m%n\n\nappender.RFA.type = RollingRandomAccessFile\nappender.RFA.name = RFA\nappender.RFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}\n\n# Use %pid in the filePattern to append process-id@host-name to the filename if you want separate log files for different CLI session\nappender.RFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}_%i\nappender.RFA.layout.type = PatternLayout\nappender.RFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n\nappender.RFA.policies.type = Policies\nappender.RFA.policies.size.type = SizeBasedTriggeringPolicy\nappender.RFA.policies.size.size = 1024MB\nappender.RFA.strategy.type = DefaultRolloverStrategy\nappender.RFA.strategy.max = 10\n\n# list of all loggers\nloggers = ZooKeeper, DataNucleus, Datastore, JPOX, HadoopConf, LlapStatusServiceDriverConsole\n\nlogger.ZooKeeper.name = org.apache.zookeeper\nlogger.ZooKeeper.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\nlogger.HadoopConf.name = org.apache.hadoop.conf.Configuration\nlogger.HadoopConf.level = ERROR\n\nlogger.LlapStatusServiceDriverConsole.name = LlapStatusServiceDriverConsole\nlogger.LlapStatusServiceDriverConsole.additivity = false\nlogger.LlapStatusServiceDriverConsole.level = ${sys:hive.llapstatus.consolelogger.level}\n\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root, RFA, FL\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}\nrootLogger.appenderRef.RFA.ref = RFA\nrootLogger.appenderRef.FL.ref = FilterLog\nlogger.LlapStatusServiceDriverConsole.appenderRefs = llapstatusconsole, RFA\nlogger.LlapStatusServiceDriverConsole.appenderRef.llapstatusconsole.ref = llapstatusconsole\nlogger.LlapStatusServiceDriverConsole.appenderRef.RFA.ref = RFA",
            "llap_cli_log_maxbackupindex" : "30",
            "llap_cli_log_maxfilesize" : "256"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "llap-daemon-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# This is the log4j2 properties file used by llap-daemons. There's several loggers defined, which\n# can be selected while configuring LLAP.\n# Based on the one selected - UI links etc need to be manipulated in the system.\n# Note: Some names and logic is common to this file and llap LogHelpers. Make sure to change that\n# as well, if changing this file.\n\nstatus = INFO\nname = LlapDaemonLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.llap.daemon.log.level = {{llap_log_level}}\nproperty.llap.daemon.root.logger = console\nproperty.llap.daemon.log.dir = .\nproperty.llap.daemon.log.file = llapdaemon.log\nproperty.llap.daemon.historylog.file = llapdaemon_history.log\nproperty.llap.daemon.log.maxfilesize = {{hive_llap_log_maxfilesize}}MB\nproperty.llap.daemon.log.maxbackupindex = {{hive_llap_log_maxbackupindex}}\n\n# list of all appenders\nappenders = console, RFA, HISTORYAPPENDER, query-routing\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{ISO8601} %5p [%t (%X{fragmentId})] %c{2}: %m%n\n\n# rolling file appender\nappender.RFA.type = RollingRandomAccessFile\nappender.RFA.name = RFA\nappender.RFA.fileName = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.log.file}\nappender.RFA.filePattern = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.log.file}_%d{yyyy-MM-dd-HH}_%i.done\nappender.RFA.layout.type = PatternLayout\nappender.RFA.layout.pattern = %d{ISO8601} %-5p [%t (%X{fragmentId})] %c: %m%n\nappender.RFA.policies.type = Policies\nappender.RFA.policies.time.type = TimeBasedTriggeringPolicy\nappender.RFA.policies.time.interval = 1\nappender.RFA.policies.time.modulate = true\nappender.RFA.policies.size.type = SizeBasedTriggeringPolicy\nappender.RFA.policies.size.size = ${sys:llap.daemon.log.maxfilesize}\nappender.RFA.strategy.type = DefaultRolloverStrategy\nappender.RFA.strategy.max = ${sys:llap.daemon.log.maxbackupindex}\n\n# history file appender\nappender.HISTORYAPPENDER.type = RollingRandomAccessFile\nappender.HISTORYAPPENDER.name = HISTORYAPPENDER\nappender.HISTORYAPPENDER.fileName = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.historylog.file}\nappender.HISTORYAPPENDER.filePattern = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.historylog.file}_%d{yyyy-MM-dd-HH}_%i.done\nappender.HISTORYAPPENDER.layout.type = PatternLayout\nappender.HISTORYAPPENDER.layout.pattern = %m%n\nappender.HISTORYAPPENDER.policies.type = Policies\nappender.HISTORYAPPENDER.policies.size.type = SizeBasedTriggeringPolicy\nappender.HISTORYAPPENDER.policies.size.size = ${sys:llap.daemon.log.maxfilesize}\nappender.HISTORYAPPENDER.policies.time.type = TimeBasedTriggeringPolicy\nappender.HISTORYAPPENDER.policies.time.interval = 1\nappender.HISTORYAPPENDER.policies.time.modulate = true\nappender.HISTORYAPPENDER.strategy.type = DefaultRolloverStrategy\nappender.HISTORYAPPENDER.strategy.max = ${sys:llap.daemon.log.maxbackupindex}\n\n# queryId based routing file appender\nappender.query-routing.type = Routing\nappender.query-routing.name = query-routing\nappender.query-routing.routes.type = Routes\nappender.query-routing.routes.pattern = $${ctx:queryId}\n#Purge polciy for query-based Routing Appender\nappender.query-routing.purgePolicy.type = LlapRoutingAppenderPurgePolicy\n# Note: Do not change this name without changing the corresponding entry in LlapConstants\nappender.query-routing.purgePolicy.name = llapLogPurgerQueryRouting\n# default route\nappender.query-routing.routes.route-default.type = Route\nappender.query-routing.routes.route-default.key = $${ctx:queryId}\nappender.query-routing.routes.route-default.ref = RFA\n# queryId based route\nappender.query-routing.routes.route-mdc.type = Route\nappender.query-routing.routes.route-mdc.file-mdc.type = LlapWrappedAppender\nappender.query-routing.routes.route-mdc.file-mdc.name = IrrelevantName-query-routing\nappender.query-routing.routes.route-mdc.file-mdc.app.type = RandomAccessFile\nappender.query-routing.routes.route-mdc.file-mdc.app.name = file-mdc\nappender.query-routing.routes.route-mdc.file-mdc.app.fileName = ${sys:llap.daemon.log.dir}/${ctx:queryId}-${ctx:dagId}.log\nappender.query-routing.routes.route-mdc.file-mdc.app.layout.type = PatternLayout\nappender.query-routing.routes.route-mdc.file-mdc.app.layout.pattern = %d{ISO8601} %5p [%t (%X{fragmentId})] %c{2}: %m%n\n\n# list of all loggers\nloggers = PerfLogger, EncodedReader, NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, HistoryLogger, LlapIoImpl, LlapIoOrc, LlapIoCache, LlapIoLocking, TezSM, TezSS, TezHC, LlapDaemon\n\nlogger.LlapDaemon.name = org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon\nlogger.LlapDaemon.level = INFO\n\n# shut up the Tez logs that log debug-level stuff on INFO\n\nlogger.TezSM.name = org.apache.tez.runtime.library.common.shuffle.impl.ShuffleManager.fetch\nlogger.TezSM.level = WARN\nlogger.TezSS.name = org.apache.tez.runtime.library.common.shuffle.orderedgrouped.ShuffleScheduler.fetch\nlogger.TezSS.level = WARN\nlogger.TezHC.name = org.apache.tez.http.HttpConnection.url\nlogger.TezHC.level = WARN\n\nlogger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger\nlogger.PerfLogger.level = DEBUG\n\nlogger.EncodedReader.name = org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl\nlogger.EncodedReader.level = INFO\n\nlogger.LlapIoImpl.name = LlapIoImpl\nlogger.LlapIoImpl.level = INFO\n\nlogger.LlapIoOrc.name = LlapIoOrc\nlogger.LlapIoOrc.level = WARN\n\nlogger.LlapIoCache.name = LlapIoCache\nlogger.LlapIoCache.level = WARN\n\nlogger.LlapIoLocking.name = LlapIoLocking\nlogger.LlapIoLocking.level = WARN\n\nlogger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn\nlogger.NIOServerCnxn.level = WARN\n\nlogger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO\nlogger.ClientCnxnSocketNIO.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\nlogger.HistoryLogger.name = org.apache.hadoop.hive.llap.daemon.HistoryLogger\nlogger.HistoryLogger.level = INFO\nlogger.HistoryLogger.additivity = false\nlogger.HistoryLogger.appenderRefs = HistoryAppender\nlogger.HistoryLogger.appenderRef.HistoryAppender.ref = HISTORYAPPENDER\n\n# root logger\nrootLogger.level = ${sys:llap.daemon.log.level}\nrootLogger.appenderRefs = root\nrootLogger.appenderRef.root.ref = ${sys:llap.daemon.root.logger}",
            "hive_llap_log_maxbackupindex" : "240",
            "hive_llap_log_maxfilesize" : "256"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "webhcat-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Define some default values that can be overridden by system properties\nwebhcat.root.logger = DEBUG, standard\nwebhcat.log.dir = .\nwebhcat.log.file = webhcat.log\n\nlog4j.rootLogger = ${webhcat.root.logger}, ETW, FullPIILogs\n\n# Logging Threshold\nlog4j.threshhold = DEBUG\n\nlog4j.appender.standard  =  org.apache.log4j.RollingFileAppender\nlog4j.appender.standard.File = ${webhcat.log.dir}/${webhcat.log.file}\nlog4j.appender.standard.MaxFileSize=1024MB\nlog4j.appender.standard.MaxBackupIndex=10\nlog4j.appender.standard.layout = org.apache.log4j.PatternLayout\nlog4j.appender.standard.layout.conversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Class logging settings\nlog4j.logger.com.sun.jersey = DEBUG, FullPIILogs\nlog4j.logger.com.sun.jersey.spi.container.servlet.WebComponent = ERROR, FullPIILogs\nlog4j.logger.org.apache.hadoop = INFO, FullPIILogs\nlog4j.logger.org.apache.hadoop.conf = WARN, FullPIILogs\nlog4j.logger.org.apache.zookeeper = WARN\nlog4j.logger.org.eclipse.jetty = INFO, FullPIILogs\n#EtwLog Appender\n\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=templeton\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=hive\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=INFO\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true",
            "webhcat_log_maxbackupindex" : "20",
            "webhcat_log_maxfilesize" : "256"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "HIVE_METASTORE:hive_metastore;HIVE_SERVER:hive_hiveserver2;WEBHCAT_SERVER:webhcat_server",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"hive_hiveserver2\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hive-env/hive_log_dir', '/var/log/hive')}}/hiveserver2.log\"\n    },\n    {\n      \"type\":\"hive_metastore\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/hive-env/hive_log_dir', '/var/log/hive')}}/hivemetastore.log\"\n    },\n    {\n      \"type\": \"webhcat_server\",\n      \"rowntype\":\"service\",\n      \"path\":\"{{default('configurations/hive-env/hcat_log_dir', '/var/log/webhcat')}}/webhcat.log\"\n    }\n  ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"hive_hiveserver2\",\n            \"hive_metastore\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\]:%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\\\(%{JAVAFILE:file}:%{JAVAMETHOD:method}\\\\(%{INT:line_number}\\\\)\\\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     },\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"webhcat_server\"\n          ]\n         }\n       },\n      \"log4j_format\":\" %-5p | %d{DATE} | %c | %m%n\",\n      \"multiline_pattern\":\"^(%{SPACE}%{LOGLEVEL:level}%{CUSTOM_SEPARATOR}%{CUSTOM_DATESTAMP:logtime})\",\n      \"message_pattern\":\"(?m)^%{SPACE}%{LOGLEVEL:level}%{CUSTOM_SEPARATOR}%{CUSTOM_DATESTAMP:logtime}%{CUSTOM_SEPARATOR}%{JAVACLASS:file}%{CUSTOM_SEPARATOR}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"dd MMM yyyy HH:mm:ss,SSS\"\n          }\n         },\n        \"level\":{\n           \"map_fieldvalue\":{\n             \"pre_value\":\"WARNING\",\n             \"post_value\":\"WARN\"\n            }\n        }\n       }\n     }\n   ]\n }",
            "service_name" : "Hive"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-interactive-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\nif [ \"$SERVICE\" = \"cli\" ]; then\n  if [ -z \"$DEBUG\" ]; then\n    export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:+UseParNewGC -XX:-UseGCOverheadLimit\"\n  else\n    export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:-UseGCOverheadLimit\"\n  fi\nfi\n\n# The heap size of the jvm stared by hive shell script can be controlled via:\n\nif [ \"$SERVICE\" = \"metastore\" ]; then\n  export HADOOP_HEAPSIZE={{hive_metastore_heapsize}} # Setting for HiveMetastore\nelse\n  export HADOOP_HEAPSIZE={{hive_interactive_heapsize}} # Setting for HiveServer2 and Client\nfi\n\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS  -Xmx${HADOOP_HEAPSIZE}m\"\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS{{heap_dump_opts}}\"\n\n# Larger heap size may be required when running queries over large number of files or partitions.\n# By default hive shell scripts use a heap size of 256 (MB).  Larger heap size would also be\n# appropriate for hive server (hwi etc).\n\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nHADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\n# Hive Configuration Directory can be controlled by:\nexport HIVE_CONF_DIR={{hive_server_interactive_conf_dir}}\n\n# Add additional hcatalog jars\nif [ \"${HIVE_AUX_JARS_PATH}\" != \"\" ]; then\n  export HIVE_AUX_JARS_PATH=${HIVE_AUX_JARS_PATH}\nelse\n  export HIVE_AUX_JARS_PATH=/usr/hdp/current/hive-server2-hive2/lib/hive-hcatalog-core.jar\nfi\n\nexport METASTORE_PORT={{hive_metastore_port}}\n\n# Spark assembly contains a conflicting copy of HiveConf from hive-1.2\nexport HIVE_SKIP_SPARK_ASSEMBLY=true",
            "enable_hive_interactive" : "false",
            "hive_aux_jars" : "",
            "hive_heapsize" : "512",
            "llap_app_name" : "llap0",
            "llap_extra_slider_opts" : "",
            "llap_headroom_space" : "12288",
            "llap_heap_size" : "0",
            "llap_java_opts" : "-XX:+AlwaysPreTouch {% if java_version > 7 %}-Xss512k -XX:+UseG1GC -XX:TLABSize=8m -XX:+ResizeTLAB -XX:+UseNUMA -XX:+AggressiveOpts -XX:InitiatingHeapOccupancyPercent=40 -XX:G1ReservePercent=20 -XX:MaxGCPauseMillis=200{% else %}-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC{% endif %}{{heap_dump_opts}}",
            "llap_log_level" : "INFO",
            "num_llap_nodes" : "1",
            "num_llap_nodes_for_llap_daemons" : "1",
            "num_retries_for_checking_llap_status" : "20",
            "slider_am_container_mb" : "341"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hiveserver2-interactive-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.async.log.enabled" : "false",
            "hive.metastore.metrics.enabled" : "true",
            "hive.service.metrics.hadoop2.component" : "hiveserver2",
            "hive.service.metrics.reporter" : "HADOOP2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-log4j2",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nstatus = TRACE\nname = HiveLog4j2\n#packages = org.apache.hadoop.hive.ql.log\n\n\n# list of properties\nproperty.hive.log.level = {{hive_log_level}}\nproperty.hive.root.logger = RFA\nproperty.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}\nproperty.hive.log.file = hive.log\n\n# list of all appenders\nappenders = console, RFA, FilterLog\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nappender.FilterLog.type = FilterLog\nappender.FilterLog.name = FilterLog\nappender.FilterLog.layout.type = PatternLayout\n#appender.FilterLog.layout.pattern =   %d{ISO8601} %5p [%t] %c{2}: %m%n\nappender.FilterLog.layout.pattern = %d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\nappender.FilterLog.source=CentralFilteredHadoopServiceLogs\nappender.FilterLog.component=hiveserver2interactive\nappender.FilterLog.whitelistFile=NA\nappender.FilterLog.osType=Linux\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\n\n# daily rolling file appender\nappender.RFA.type = RollingRandomAccessFile\nappender.RFA.name = RFA\nappender.RFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}\n# Use %pid in the filePattern to append process-id@host-name to the filename if you want separate log files for different CLI session\nappender.RFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}_%i\nappender.RFA.layout.type = PatternLayout\nappender.RFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n\nappender.RFA.policies.type = Policies\nappender.RFA.policies.time.type = SizeBasedTriggeringPolicy\n\n\nappender.RFA.strategy.type = DefaultRolloverStrategy\nappender.RFA.strategy.max = 10\nappender.RFA.policies.size.type = SizeBasedTriggeringPolicy\nappender.RFA.policies.size.size = 1024MB\n\n# list of all loggers\nloggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX\n\nlogger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn\nlogger.NIOServerCnxn.level = WARN\n\nlogger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO\nlogger.ClientCnxnSocketNIO.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root, FL\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}\nrootLogger.appenderRef.FL.ref = FilterLog",
            "hive2_log_maxbackupindex" : "30",
            "hive2_log_maxfilesize" : "256"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hive-policymgr-ssl",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "xasecure.policymgr.clientssl.keystore" : "/usr/hdp/current/hive-server2/conf/ranger-plugin-keystore.jks",
            "xasecure.policymgr.clientssl.keystore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.keystore.password" : "SECRET:ranger-hive-policymgr-ssl:2:xasecure.policymgr.clientssl.keystore.password",
            "xasecure.policymgr.clientssl.truststore" : "/usr/hdp/current/hive-server2/conf/ranger-plugin-truststore.jks",
            "xasecure.policymgr.clientssl.truststore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.truststore.password" : "SECRET:ranger-hive-policymgr-ssl:2:xasecure.policymgr.clientssl.truststore.password"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "parquet-logging",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Properties file which configures the operation of the JDK\n# logging facility.\n\n# The system will look for this config file, first using\n# a System property specified at startup:\n#\n# >java -Djava.util.logging.config.file=myLoggingConfigFilePath\n#\n# If this property is not specified, then the config file is\n# retrieved from its default location at:\n#\n# JDK_HOME/jre/lib/logging.properties\n\n# Global logging properties.\n# ------------------------------------------\n# The set of handlers to be loaded upon startup.\n# Comma-separated list of class names.\n# (? LogManager docs say no comma here, but JDK example has comma.)\n# handlers=java.util.logging.ConsoleHandler\norg.apache.parquet.handlers= java.util.logging.FileHandler\n\n# Default global logging level.\n# Loggers and Handlers may override this level\n.level=INFO\n\n# Handlers\n# -----------------------------------------\n\n# --- ConsoleHandler ---\n# Override of global logging level\njava.util.logging.ConsoleHandler.level=INFO\njava.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter\njava.util.logging.SimpleFormatter.format=[%1$tc] %4$s: %2$s - %5$s %6$s%n\n\n# --- FileHandler ---\n# Override of global logging level\njava.util.logging.FileHandler.level=ALL\n\n# Naming style for the output file:\n# (The output file is placed in the system temporary directory.\n# %u is used to provide unique identifier for the file.\n# For more information refer\n# https://docs.oracle.com/javase/7/docs/api/java/util/logging/FileHandler.html)\njava.util.logging.FileHandler.pattern=%t/parquet-%u.log\n\n# Limiting size of output file in bytes:\njava.util.logging.FileHandler.limit=50000000\n\n# Number of output files to cycle through, by appending an\n# integer to the base file name:\njava.util.logging.FileHandler.count=1\n\n# Style of output (Simple or XML):\njava.util.logging.FileHandler.formatter=java.util.logging.SimpleFormatter"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hcat-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n      # Licensed to the Apache Software Foundation (ASF) under one\n      # or more contributor license agreements. See the NOTICE file\n      # distributed with this work for additional information\n      # regarding copyright ownership. The ASF licenses this file\n      # to you under the Apache License, Version 2.0 (the\n      # \"License\"); you may not use this file except in compliance\n      # with the License. You may obtain a copy of the License at\n      #\n      # http://www.apache.org/licenses/LICENSE-2.0\n      #\n      # Unless required by applicable law or agreed to in writing, software\n      # distributed under the License is distributed on an \"AS IS\" BASIS,\n      # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n      # See the License for the specific language governing permissions and\n      # limitations under the License.\n\n      JAVA_HOME={{java64_home}}\n      HCAT_PID_DIR={{hcat_pid_dir}}/\n      HCAT_LOG_DIR={{hcat_log_dir}}/\n      HCAT_CONF_DIR={{hcat_conf_dir}}\n      HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n      #DBROOT is the path where the connector jars are downloaded\n      DBROOT={{hcat_dbroot}}\n      USER={{hcat_user}}\n      METASTORE_PORT={{hive_metastore_port}}"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ambari.hive.db.schema.name" : "hive",
            "atlas.hook.hive.maxThreads" : "1",
            "atlas.hook.hive.minThreads" : "1",
            "datanucleus.autoCreateSchema" : "false",
            "datanucleus.cache.level2.type" : "none",
            "datanucleus.connectionPool.maxIdle" : "2",
            "datanucleus.connectionPoolingType" : "HikariCP",
            "datanucleus.fixedDatastore" : "true",
            "fs.azure.delete.threads" : "128",
            "fs.azure.rename.threads" : "128",
            "hive.auto.convert.join" : "true",
            "hive.auto.convert.join.noconditionaltask" : "true",
            "hive.auto.convert.join.noconditionaltask.size" : "319039733",
            "hive.auto.convert.sortmerge.join" : "false",
            "hive.auto.convert.sortmerge.join.to.mapjoin" : "false",
            "hive.cbo.enable" : "true",
            "hive.cli.print.header" : "false",
            "hive.cluster.delegation.token.store.class" : "org.apache.hadoop.hive.thrift.ZooKeeperTokenStore",
            "hive.cluster.delegation.token.store.zookeeper.connectString" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "hive.cluster.delegation.token.store.zookeeper.znode" : "/hive/cluster/delegation",
            "hive.compactor.abortedtxn.threshold" : "1000",
            "hive.compactor.check.interval" : "300L",
            "hive.compactor.delta.num.threshold" : "10",
            "hive.compactor.delta.pct.threshold" : "0.1f",
            "hive.compactor.initiator.on" : "false",
            "hive.compactor.worker.threads" : "0",
            "hive.compactor.worker.timeout" : "86400L",
            "hive.compute.query.using.stats" : "true",
            "hive.conf.restricted.list" : "hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role",
            "hive.convert.join.bucket.mapjoin.tez" : "false",
            "hive.default.fileformat" : "TextFile",
            "hive.default.fileformat.managed" : "TextFile",
            "hive.enforce.bucketing" : "true",
            "hive.enforce.sorting" : "true",
            "hive.enforce.sortmergebucketmapjoin" : "true",
            "hive.exec.compress.intermediate" : "false",
            "hive.exec.compress.output" : "false",
            "hive.exec.dynamic.partition" : "true",
            "hive.exec.dynamic.partition.mode" : "nonstrict",
            "hive.exec.failure.hooks" : "org.apache.hadoop.hive.ql.hooks.ATSHook",
            "hive.exec.max.created.files" : "100000",
            "hive.exec.max.dynamic.partitions" : "5000",
            "hive.exec.max.dynamic.partitions.pernode" : "2000",
            "hive.exec.orc.compression.strategy" : "SPEED",
            "hive.exec.orc.default.compress" : "ZLIB",
            "hive.exec.orc.default.stripe.size" : "67108864",
            "hive.exec.orc.encoding.strategy" : "SPEED",
            "hive.exec.parallel" : "false",
            "hive.exec.parallel.thread.number" : "8",
            "hive.exec.post.hooks" : "org.apache.hadoop.hive.ql.hooks.ATSHook",
            "hive.exec.pre.hooks" : "org.apache.hadoop.hive.ql.hooks.ATSHook",
            "hive.exec.reducers.bytes.per.reducer" : "67108864",
            "hive.exec.reducers.max" : "1009",
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.exec.submit.local.task.via.child" : "true",
            "hive.exec.submitviachild" : "false",
            "hive.execution.engine" : "tez",
            "hive.exim.uri.scheme.whitelist" : "wasb,hdfs,pfile,wasbs,abfs,abfss,adls,adlss",
            "hive.fetch.task.aggr" : "false",
            "hive.fetch.task.conversion" : "more",
            "hive.fetch.task.conversion.threshold" : "1073741824",
            "hive.hmshandler.retry.attempts" : "5",
            "hive.hmshandler.retry.interval" : "1000",
            "hive.limit.optimize.enable" : "true",
            "hive.limit.pushdown.memory.usage" : "0.04",
            "hive.map.aggr" : "true",
            "hive.map.aggr.hash.force.flush.memory.threshold" : "0.9",
            "hive.map.aggr.hash.min.reduction" : "0.5",
            "hive.map.aggr.hash.percentmemory" : "0.5",
            "hive.mapjoin.bucket.cache.size" : "10000",
            "hive.mapjoin.optimized.hashtable" : "true",
            "hive.mapred.reduce.tasks.speculative.execution" : "false",
            "hive.merge.mapfiles" : "true",
            "hive.merge.mapredfiles" : "false",
            "hive.merge.orcfile.stripe.level" : "true",
            "hive.merge.rcfile.block.level" : "true",
            "hive.merge.size.per.task" : "256000000",
            "hive.merge.smallfiles.avgsize" : "16000000",
            "hive.merge.tezfiles" : "false",
            "hive.metastore.authorization.storage.checks" : "false",
            "hive.metastore.cache.pinobjtypes" : "Table,Database,Type,FieldSchema,Order",
            "hive.metastore.client.connect.retry.delay" : "5s",
            "hive.metastore.client.socket.timeout" : "1800s",
            "hive.metastore.connect.retries" : "5",
            "hive.metastore.execute.setugi" : "false",
            "hive.metastore.failure.retries" : "24",
            "hive.metastore.kerberos.keytab.file" : "/etc/security/keytabs/hive.service.keytab",
            "hive.metastore.kerberos.principal" : "hive/_HOST@EXAMPLE.COM",
            "hive.metastore.pre.event.listeners" : "org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener",
            "hive.metastore.sasl.enabled" : "false",
            "hive.metastore.schema.verification" : "true",
            "hive.metastore.server.max.threads" : "100000",
            "hive.metastore.uris" : "thrift://hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:9083,thrift://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:9083",
            "hive.metastore.warehouse.dir" : "/hive/warehouse",
            "hive.mv.files.thread" : "128",
            "hive.optimize.bucketmapjoin" : "true",
            "hive.optimize.bucketmapjoin.sortedmerge" : "false",
            "hive.optimize.constant.propagation" : "true",
            "hive.optimize.index.filter" : "true",
            "hive.optimize.metadataonly" : "true",
            "hive.optimize.null.scan" : "true",
            "hive.optimize.reducededuplication" : "true",
            "hive.optimize.reducededuplication.min.reducer" : "4",
            "hive.optimize.sort.dynamic.partition" : "true",
            "hive.orc.compute.splits.num.threads" : "10",
            "hive.orc.splits.include.file.footer" : "false",
            "hive.prewarm.enabled" : "false",
            "hive.prewarm.numcontainers" : "3",
            "hive.scratchdir.lock" : "true",
            "hive.security.authenticator.manager" : "org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator",
            "hive.security.authorization.enabled" : "false",
            "hive.security.authorization.manager" : "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory",
            "hive.security.metastore.authenticator.manager" : "org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator",
            "hive.security.metastore.authorization.auth.reads" : "true",
            "hive.security.metastore.authorization.manager" : "org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly",
            "hive.server2.allow.user.substitution" : "true",
            "hive.server2.authentication" : "NONE",
            "hive.server2.authentication.spnego.keytab" : "HTTP/_HOST@EXAMPLE.COM",
            "hive.server2.authentication.spnego.principal" : "/etc/security/keytabs/spnego.service.keytab",
            "hive.server2.clear.dangling.scratchdir" : "true",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.logging.operation.enabled" : "true",
            "hive.server2.logging.operation.log.location" : "/tmp/hive/operation_logs",
            "hive.server2.max.start.attempts" : "5",
            "hive.server2.support.dynamic.service.discovery" : "true",
            "hive.server2.table.type.mapping" : "CLASSIC",
            "hive.server2.tez.default.queues" : "default",
            "hive.server2.tez.initialize.default.sessions" : "false",
            "hive.server2.tez.sessions.per.default.queue" : "1",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10001",
            "hive.server2.thrift.max.worker.threads" : "500",
            "hive.server2.thrift.port" : "10000",
            "hive.server2.thrift.sasl.qop" : "auth",
            "hive.server2.transport.mode" : "http",
            "hive.server2.use.SSL" : "false",
            "hive.server2.zookeeper.namespace" : "hiveserver2",
            "hive.smbjoin.cache.rows" : "10000",
            "hive.start.cleanup.scratchdir" : "false",
            "hive.stats.autogather" : "true",
            "hive.stats.dbclass" : "fs",
            "hive.stats.fetch.column.stats" : "true",
            "hive.stats.fetch.partition.stats" : "true",
            "hive.support.concurrency" : "false",
            "hive.tez.auto.reducer.parallelism" : "true",
            "hive.tez.container.size" : "1536",
            "hive.tez.cpu.vcores" : "-1",
            "hive.tez.dynamic.partition.pruning" : "true",
            "hive.tez.dynamic.partition.pruning.max.data.size" : "104857600",
            "hive.tez.dynamic.partition.pruning.max.event.size" : "1048576",
            "hive.tez.exec.print.summary" : "true",
            "hive.tez.input.format" : "org.apache.hadoop.hive.ql.io.HiveInputFormat",
            "hive.tez.java.opts" : "-Xmx1024M -Xms1024M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",
            "hive.tez.log.level" : "INFO",
            "hive.tez.max.partition.factor" : "3f",
            "hive.tez.min.partition.factor" : "1f",
            "hive.tez.smb.number.waves" : "0.5",
            "hive.txn.manager" : "org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager",
            "hive.txn.max.open.batch" : "1000",
            "hive.txn.timeout" : "300",
            "hive.user.install.directory" : "/user",
            "hive.vectorized.execution.enabled" : "true",
            "hive.vectorized.execution.reduce.enabled" : "true",
            "hive.vectorized.groupby.checkinterval" : "4096",
            "hive.vectorized.groupby.flush.percent" : "0.1",
            "hive.vectorized.groupby.maxentries" : "100000",
            "hive.warehouse.subdir.inherit.perms" : "true",
            "hive.zookeeper.client.port" : "2181",
            "hive.zookeeper.namespace" : "hive_zookeeper_namespace",
            "hive.zookeeper.quorum" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "javax.jdo.option.ConnectionDriverName" : "com.microsoft.sqlserver.jdbc.SQLServerDriver",
            "javax.jdo.option.ConnectionPassword" : "SECRET:hive-site:2:javax.jdo.option.ConnectionPassword",
            "javax.jdo.option.ConnectionURL" : "jdbc:sqlserver://rfkowvb6yq.database.windows.net;databaseName=v368131f056aa2d44e882781a655de2485ehivemetastore;trustServerCertificate=false;encrypt=true;hostNameInCertificate=*.database.windows.net;",
            "javax.jdo.option.ConnectionUserName" : "v368131f056aa2d44e882781a655de2485ehivemetastoreLogin@rfkowvb6yq.database.windows.net",
            "tez.runtime.shuffle.connect.timeout" : "30000",
            "tez.shuffle-vertex-manager.max-src-fraction" : "0.95",
            "tez.shuffle-vertex-manager.min-src-fraction" : "0.9"
          },
          "properties_attributes" : {
            "hidden" : {
              "javax.jdo.option.ConnectionPassword" : "HIVE_CLIENT,WEBHCAT_SERVER,HCAT,CONFIG_DOWNLOAD"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hive-plugin-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "REPOSITORY_CONFIG_PASSWORD" : "SECRET:ranger-hive-plugin-properties:2:REPOSITORY_CONFIG_PASSWORD",
            "REPOSITORY_CONFIG_USERNAME" : "hive",
            "common.name.for.certificate" : "",
            "external_admin_password" : "",
            "external_admin_username" : "",
            "external_ranger_admin_password" : "",
            "external_ranger_admin_username" : "",
            "jdbc.driverClassName" : "org.apache.hive.jdbc.HiveDriver",
            "policy_user" : "ambari-qa"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "webhcat-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fs.azure.skip.metrics" : "true",
            "templeton.enable.job.reconnect.default" : "true",
            "templeton.exec.timeout" : "60000",
            "templeton.hadoop" : "/usr/hdp/${hdp.version}/hadoop/bin/hadoop",
            "templeton.hadoop.conf.dir" : "/etc/hadoop/conf",
            "templeton.hadoop.queue.name" : "default",
            "templeton.hcat" : "/usr/hdp/${hdp.version}/hive/bin/hcat",
            "templeton.hcat.home" : "hive.tar.gz/hive/hcatalog",
            "templeton.hive.archive" : "wasb:///hdp/apps/${hdp.version}/hive/hive.tar.gz",
            "templeton.hive.extra.files" : "/usr/hdp/${hdp.version}/tez/conf/tez-site.xml",
            "templeton.hive.home" : "hive.tar.gz/hive",
            "templeton.hive.path" : "hive.tar.gz/hive/bin/hive",
            "templeton.hive.properties" : "hive.metastore.local=false,hive.metastore.uris=thrift://hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:9083\\,thrift://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:9083,hive.user.install.directory=/user,hive.root.logger=WARN\\,console,hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.ATSHook,hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.ATSHook,hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.ATSHook",
            "templeton.jar" : "/usr/hdp/${hdp.version}/hive/share/webhcat/svr/lib/hive-webhcat-*.jar",
            "templeton.job.list.timeout" : "90",
            "templeton.job.status.timeout" : "90",
            "templeton.job.submit.timeout" : "90",
            "templeton.job.timeout.task.retry.count" : "10",
            "templeton.job.timeout.task.retry.interval" : "10",
            "templeton.jobs.listorder" : "lexicographicaldesc",
            "templeton.libjars" : "/usr/hdp/${hdp.version}/zookeeper/zookeeper.jar,/usr/hdp/${hdp.version}/hive/lib/hive-common.jar",
            "templeton.mapper.memory.mb" : "1024",
            "templeton.override.enabled" : "false",
            "templeton.parallellism.job.list" : "5",
            "templeton.parallellism.job.status" : "50",
            "templeton.parallellism.job.submit" : "15",
            "templeton.pig.archive" : "wasb:///hdp/apps/${hdp.version}/pig/pig.tar.gz",
            "templeton.pig.path" : "pig.tar.gz/pig/bin/pig",
            "templeton.port" : "30111",
            "templeton.python" : "${env.PYTHON_CMD}",
            "templeton.sqoop.archive" : "wasb:///hdp/apps/${hdp.version}/sqoop/sqoop.tar.gz",
            "templeton.sqoop.home" : "/usr/hdp/${hdp.version}/sqoop",
            "templeton.sqoop.path" : "/usr/hdp/${hdp.version}/sqoop/bin/sqoop",
            "templeton.storage.class" : "org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage",
            "templeton.streaming.jar" : "wasb:///hdp/apps/${hdp.version}/mapreduce/hadoop-streaming.jar",
            "templeton.zookeeper.hosts" : "zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "yarn.app.mapreduce.client.job.retry-interval" : "1000",
            "yarn.app.mapreduce.client.max-retries" : "2",
            "yarn.resourcemanager.connect.max-wait.ms" : "40000",
            "yarn.resourcemanager.connect.retry-interval.ms" : "10000"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "tez-interactive-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "tez.am.am-rm.heartbeat.interval-ms.max" : "10000",
            "tez.am.client.heartbeat.poll.interval.millis" : "6000",
            "tez.am.client.heartbeat.timeout.secs" : "90",
            "tez.am.node-blacklisting.enabled" : "false",
            "tez.am.resource.memory.mb" : "1536",
            "tez.am.task.listener.thread-count" : "1",
            "tez.am.task.reschedule.higher.priority" : "false",
            "tez.container.max.java.heap.fraction" : "-1",
            "tez.dag.recovery.enabled" : "false",
            "tez.grouping.node.local.only" : "true",
            "tez.history.logging.log.level" : "TASK_ATTEMPT",
            "tez.history.logging.taskattempt-filters" : "SERVICE_BUSY,EXTERNAL_PREEMPTION",
            "tez.history.logging.timeline.num-dags-per-group" : "5",
            "tez.lib.uris" : "/hdp/apps/${hdp.version}/tez_hive2/tez.tar.gz",
            "tez.runtime.enable.final-merge.in.output" : "false",
            "tez.runtime.io.sort.mb" : "512",
            "tez.runtime.pipelined-shuffle.enabled" : "false",
            "tez.runtime.pipelined.sorter.lazy-allocate.memory" : "true",
            "tez.runtime.report.partition.stats" : "true",
            "tez.runtime.shuffle.connect.timeout" : "30000",
            "tez.runtime.shuffle.fetch.buffer.percent" : "0.6",
            "tez.runtime.shuffle.fetch.verify-disk-checksum" : "false",
            "tez.runtime.shuffle.keep-alive.enabled" : "true",
            "tez.runtime.shuffle.memory.limit.percent" : "0.25",
            "tez.runtime.shuffle.parallel.copies" : "8",
            "tez.runtime.shuffle.read.timeout" : "30000",
            "tez.runtime.shuffle.ssl.enable" : "false",
            "tez.runtime.unordered.output.buffer.size-mb" : "100",
            "tez.session.am.dag.submit.timeout.secs" : "1209600",
            "tez.task.heartbeat.timeout.check-ms" : "15000",
            "tez.task.timeout-ms" : "90000"
          },
          "properties_attributes" : {
            "final" : {
              "tez.runtime.shuffle.ssl.enable" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-exec-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Define some default values that can be overridden by system properties\n\nhive.log.threshold=ALL\nhive.root.logger=INFO,FA\nhive.log.dir=${java.io.tmpdir}/${user.name}\nhive.query.id=hadoop\nhive.log.file=${hive.query.id}.log\n\n# Define the root logger to the system property \\\"hadoop.root.logger\\\".\nlog4j.rootLogger=${hive.root.logger}, EventCounter,FullPIILogs\n# Logging Threshold\nlog4j.threshhold=${hive.log.threshold}\n\n#\n# File Appender\n#\n\nlog4j.appender.FA=org.apache.log4j.FileAppender\nlog4j.appender.FA.File=${hive.log.dir}/${hive.log.file}\nlog4j.appender.FA.layout=org.apache.log4j.PatternLayout\n\n# Pattern format: Date LogLevel LoggerName LogMessage\n# Pattern format: Date LogLevel LoggerName LogMessage\n# Debugging Pattern format\nlog4j.appender.FA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\n\n\n#\n# console\n# Add \\\"console\\\" to rootlogger above if you want to use this\n#\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\n\n#custom logging levels\n#log4j.logger.xxx=DEBUG\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter\n\n\nlog4j.category.DataNucleus=WARN,FA,FullPIILogs\nlog4j.category.Datastore=WARN,FA,FullPIILogs\nlog4j.category.Datastore.Schema=WARN,FA,FullPIILogs\nlog4j.category.JPOX.Datastore=WARN,FA,FullPIILogs\nlog4j.category.JPOX.Plugin=WARN,FA,FullPIILogs\nlog4j.category.JPOX.MetaData=WARN,FA,FullPIILogs\nlog4j.category.JPOX.Query=WARN,FA,FullPIILogs\nlog4j.category.JPOX.General=WARN,FA,FullPIILogs\nlog4j.category.JPOX.Enhancer=WARN,FA,FullPIILogs\n\n\n# Silence useless ZK logs\nlog4j.logger.org.apache.zookeeper.server.NIOServerCnxn=ERROR,FA,FullPIILogs\nlog4j.logger.org.apache.zookeeper.ClientCnxnSocketNIO=ERROR,FA,FullPIILogs\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=hive\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=INFO\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hivemetastore-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.metastore.metrics.enabled" : "true",
            "hive.service.metrics.hadoop2.component" : "hivemetastore",
            "hive.service.metrics.reporter" : "HADOOP2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "webhcat-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The file containing the running pid\nPID_FILE={{webhcat_pid_file}}\n\nTEMPLETON_LOG_DIR={{templeton_log_dir}}/\n\n\nWEBHCAT_LOG_DIR={{templeton_log_dir}}/\n\n# The console error log\nERROR_LOG={{templeton_log_dir}}/webhcat-console-error.log\n\n# The console log\nCONSOLE_LOG={{templeton_log_dir}}/webhcat-console.log\n\n#TEMPLETON_JAR=templeton_jar_name\n\n#HADOOP_PREFIX=hadoop_prefix\n\n#HCAT_PREFIX=hive_prefix\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-exec-log4j2",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.status = INFO\n\nname = HiveExecLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.hive.log.level = {{hive_log_level}}\nproperty.hive.root.logger = FA\nproperty.hive.query.id = hadoop\nproperty.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}\nproperty.hive.log.file = ${sys:hive.query.id}.log\n\n# list of all appenders\nappenders = console, FA, FilterLog\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nappender.FilterLog.type = FilterLog\nappender.FilterLog.name = FilterLog\nappender.FilterLog.layout.type = PatternLayout\n#appender.FilterLog.layout.pattern =   %d{ISO8601} %5p [%t] %c{2}: %m%n\nappender.FilterLog.layout.pattern = %d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\nappender.FilterLog.source=CentralFilteredHadoopServiceLogs\nappender.FilterLog.component=hiveexec\nappender.FilterLog.whitelistFile=NA\nappender.FilterLog.osType=Linux\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\n\n# simple file appender\nappender.FA.type = File\nappender.FA.name = FA\nappender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}\nappender.FA.layout.type = PatternLayout\nappender.FA.layout.pattern = %d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\n\n# list of all loggers\nloggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX\n\nlogger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn\nlogger.NIOServerCnxn.level = WARN\n\nlogger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO\nlogger.ClientCnxnSocketNIO.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root, FL\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}\nrootLogger.appenderRef.FL.ref = FilterLog"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hiveserver2-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.conf.restricted.list" : "hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role",
            "hive.metastore.metrics.enabled" : "true",
            "hive.security.authenticator.manager" : "org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator",
            "hive.security.authorization.enabled" : "false",
            "hive.security.authorization.manager" : "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory",
            "hive.service.metrics.hadoop2.component" : "hiveserver2",
            "hive.service.metrics.reporter" : "HADOOP2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \\\"License\\\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhive.log.threshold=ALL\nhive.root.logger=DEBUG,RFA\nhive.log.dir=${java.io.tmpdir}/${user.name}\nhive.log.file=hive.log\n\nlog4jhive.log.maxfilesize=1024MB\nlog4jhive.log.maxbackupindex=10\n\n\nlog4j.rootLogger=${hive.root.logger}, EventCounter, ETW, FullPIILogs\nlog4j.threshold=${hive.log.threshold}\n\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hive.log.dir}/${hive.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jhive.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jhive.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\nlog4j.appender.console.encoding=UTF-8\n\n#custom logging levels\n#log4j.logger.xxx=DEBUG\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter\n\n\nlog4j.category.DataNucleus=WARN,RFA,ETW,FullPIILogs\nlog4j.category.Datastore=WARN,RFA,ETW,FullPIILogs\nlog4j.category.Datastore.Schema=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.Datastore=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.Plugin=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.MetaData=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.Query=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.General=WARN,RFA,ETW,FullPIILogs\nlog4j.category.JPOX.Enhancer=WARN,RFA,ETW,FullPIILogs\n\n# Silence useless ZK logs\nlog4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN,RFA\nlog4j.logger.org.apache.zookeeper.ClientCnxnSocketNIO=WARN,RFA\n#EtwLog Appender\n#sends HDP service logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=hive\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=hive\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=INFO\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true",
            "hive_log_maxbackupindex" : "30",
            "hive_log_maxfilesize" : "256"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-atlas-application.properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "atlas.hook.hive.keepAliveTime" : "10",
            "atlas.hook.hive.maxThreads" : "5",
            "atlas.hook.hive.minThreads" : "5",
            "atlas.hook.hive.numRetries" : "3",
            "atlas.hook.hive.queueSize" : "1000",
            "atlas.hook.hive.synchronous" : "false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "beeline-log4j2",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nstatus = INFO\nname = BeelineLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.hive.log.level = {{hive_log_level}}\nproperty.hive.root.logger = console\n\n# list of all appenders\nappenders = console\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\n\n# list of all loggers\nloggers = HiveConnection\n\n# HiveConnection logs useful info for dynamic service discovery\nlogger.HiveConnection.name = org.apache.hive.jdbc.HiveConnection\nlogger.HiveConnection.level = INFO\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\nexport HADOOP_USER_CLASSPATH_FIRST=true  #this prevents old metrics libs from mapreduce lib from bringing in old jar deps overriding HIVE_LIB\nif [ \"$SERVICE\" = \"cli\" ]; then\n  if [ -z \"$DEBUG\" ]; then\n    export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:+UseNUMA -XX:+UseParallelGC -XX:-UseGCOverheadLimit\"\n  else\n    export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:-UseGCOverheadLimit\"\n  fi\nfi\n\n# The heap size of the jvm stared by hive shell script can be controlled via:\n\nif [ \"$SERVICE\" = \"metastore\" ]; then\n  export HADOOP_HEAPSIZE={{hive_metastore_heapsize}} # Setting for HiveMetastore\nelse\n  export HADOOP_HEAPSIZE={{hive_heapsize}} # Setting for HiveServer2 and Client\nfi\n\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS  -Xmx${HADOOP_HEAPSIZE}m\"\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS{{heap_dump_opts}}\"\n\n# Larger heap size may be required when running queries over large number of files or partitions.\n# By default hive shell scripts use a heap size of 256 (MB).  Larger heap size would also be\n# appropriate for hive server (hwi etc).\n\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nHADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\nexport HIVE_HOME=${HIVE_HOME:-{{hive_home_dir}}}\n\n# Hive Configuration Directory can be controlled by:\nexport HIVE_CONF_DIR=${HIVE_CONF_DIR:-{{hive_config_dir}}}\n\n# Folder containing extra libraries required for hive compilation/execution can be controlled by:\nexport HIVE_AUX_JARS_PATH={{stack_root}}/current/ext/hive\nif [ \"${HIVE_AUX_JARS_PATH}\" != \"\" ]; then\n  if [ -f \"${HIVE_AUX_JARS_PATH}\" ] || [ -d \"${HIVE_AUX_JARS_PATH}\" ] ; then\n    export HIVE_AUX_JARS_PATH=${HIVE_AUX_JARS_PATH}\n  elif [ -d \"/usr/hdp/current/hive-webhcat/share/hcatalog\" ]; then\n    export HIVE_AUX_JARS_PATH=/usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar\n  fi\nelif [ -d \"/usr/hdp/current/hive-webhcat/share/hcatalog\" ]; then\n  export HIVE_AUX_JARS_PATH=/usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar\nfi\n\nexport METASTORE_PORT={{hive_metastore_port}}\n\n{% if sqla_db_used or lib_dir_available %}\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:{{jdbc_libs_dir}}\"\nexport JAVA_LIBRARY_PATH=\"$JAVA_LIBRARY_PATH:{{jdbc_libs_dir}}\"\n{% endif %}",
            "enable_heap_dump" : "false",
            "hcat_log_dir" : "/var/log/webhcat",
            "hcat_pid_dir" : "/var/run/webhcat",
            "hcat_user" : "hcat",
            "heap_dump_location" : "/tmp",
            "hive.atlas.hook" : "false",
            "hive.client.heapsize" : "512",
            "hive.heapsize" : "5376",
            "hive.log.level" : "INFO",
            "hive.metastore.heapsize" : "1792",
            "hive_ambari_database" : "MySQL",
            "hive_database" : "Existing MSSQL Server database with SQL authentication",
            "hive_database_name" : "v368131f056aa2d44e882781a655de2485ehivemetastore",
            "hive_database_type" : "mssql",
            "hive_exec_orc_storage_strategy" : "SPEED",
            "hive_existing_mssql_server_database" : "v368131f056aa2d44e882781a655de2485ehivemetastore",
            "hive_existing_mssql_server_host" : "rfkowvb6yq.database.windows.net",
            "hive_hostname" : "rfkowvb6yq.database.windows.net",
            "hive_log_dir" : "/var/log/hive",
            "hive_pid_dir" : "/var/run/hive",
            "hive_security_authorization" : "None",
            "hive_timeline_logging_enabled" : "true",
            "hive_txn_acid" : "off",
            "hive_user" : "hive",
            "hive_user_nofile_limit" : "32000",
            "hive_user_nproc_limit" : "16000",
            "webhcat_user" : "hcat"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "hive-interactive-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "dfs.client.mmap.enabled" : "false",
            "dfs.short.circuit.shared.memory.watcher.interrupt.check.ms" : "0",
            "hive.auto.convert.join.noconditionaltask.size" : "1000000000",
            "hive.driver.parallel.compilation" : "true",
            "hive.druid.basePersistDirectory" : "",
            "hive.druid.bitmap.type" : "roaring",
            "hive.druid.broker.address.default" : "localhost:8082",
            "hive.druid.coordinator.address.default" : "localhost:8082",
            "hive.druid.http.read.timeout" : "PT10M",
            "hive.druid.indexer.memory.rownum.max" : "75000",
            "hive.druid.indexer.partition.size.max" : "1000000",
            "hive.druid.indexer.segments.granularity" : "DAY",
            "hive.druid.maxTries" : "5",
            "hive.druid.metadata.db.type" : "mysql",
            "hive.druid.metadata.password" : "SECRET:hive-interactive-site:2:hive.druid.metadata.password",
            "hive.druid.metadata.uri" : "jdbc:mysql://localhost:3355/druid",
            "hive.druid.metadata.username" : "druid",
            "hive.druid.passiveWaitTimeMs" : "30000",
            "hive.druid.select.distribute" : "true",
            "hive.druid.storage.storageDirectory" : "{{druid_storage_dir}}",
            "hive.druid.working.directory" : "/tmp/druid-indexing",
            "hive.exec.orc.split.strategy" : "HYBRID",
            "hive.execution.engine" : "tez",
            "hive.execution.mode" : "llap",
            "hive.llap.auto.allow.uber" : "false",
            "hive.llap.client.consistent.splits" : "true",
            "hive.llap.daemon.am.liveness.heartbeat.interval.ms" : "10000ms",
            "hive.llap.daemon.logger" : "query-routing",
            "hive.llap.daemon.num.executors" : "1",
            "hive.llap.daemon.queue.name" : "default",
            "hive.llap.daemon.rpc.port" : "0",
            "hive.llap.daemon.service.hosts" : "@llap0",
            "hive.llap.daemon.task.scheduler.enable.preemption" : "true",
            "hive.llap.daemon.vcpus.per.instance" : "${hive.llap.daemon.num.executors}",
            "hive.llap.daemon.yarn.container.mb" : "0",
            "hive.llap.daemon.yarn.shuffle.port" : "15551",
            "hive.llap.enable.grace.join.in.llap" : "false",
            "hive.llap.execution.mode" : "only",
            "hive.llap.io.enabled" : "true",
            "hive.llap.io.memory.mode" : "",
            "hive.llap.io.memory.size" : "0",
            "hive.llap.io.threadpool.size" : "2",
            "hive.llap.io.use.lrfu" : "true",
            "hive.llap.management.rpc.port" : "15004",
            "hive.llap.object.cache.enabled" : "true",
            "hive.llap.task.scheduler.locality.delay" : "-1",
            "hive.llap.zk.sm.connectionString" : "zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "hive.map.aggr.hash.min.reduction" : "0.99",
            "hive.mapjoin.hybridgrace.hashtable" : "false",
            "hive.merge.nway.joins" : "false",
            "hive.metastore.event.listeners" : "",
            "hive.metastore.uris" : "",
            "hive.optimize.dynamic.partition.hashjoin" : "true",
            "hive.prewarm.enabled" : "false",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.tez.default.queues" : "default",
            "hive.server2.tez.initialize.default.sessions" : "true",
            "hive.server2.tez.sessions.custom.queue.allowed" : "ignore",
            "hive.server2.tez.sessions.per.default.queue" : "1",
            "hive.server2.tez.sessions.restricted.configs" : "hive.execution.mode,hive.execution.engine",
            "hive.server2.thrift.http.port" : "10501",
            "hive.server2.thrift.port" : "10500",
            "hive.server2.webui.port" : "10502",
            "hive.server2.webui.use.ssl" : "false",
            "hive.server2.zookeeper.namespace" : "hiveserver2-hive2",
            "hive.tez.bucket.pruning" : "true",
            "hive.tez.cartesian-product.enabled" : "true",
            "hive.tez.container.size" : "682",
            "hive.tez.exec.print.summary" : "true",
            "hive.tez.input.generate.consistent.splits" : "true",
            "hive.vectorized.execution.mapjoin.minmax.enabled" : "true",
            "hive.vectorized.execution.mapjoin.native.enabled" : "true",
            "hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled" : "true",
            "hive.vectorized.execution.reduce.enabled" : "true",
            "hive.vectorized.groupby.maxentries" : "1000000",
            "llap.shuffle.connection-keep-alive.enable" : "true",
            "llap.shuffle.connection-keep-alive.timeout" : "60"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hive-audit",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ranger.plugin.hive.ambari.cluster.name" : "{{cluster_name}}",
            "xasecure.audit.destination.hdfs" : "true",
            "xasecure.audit.destination.hdfs.batch.filespool.dir" : "/var/log/hive/audit/hdfs/spool",
            "xasecure.audit.destination.hdfs.dir" : "hdfs://NAMENODE_HOSTNAME:8020/ranger/audit",
            "xasecure.audit.destination.solr" : "false",
            "xasecure.audit.destination.solr.batch.filespool.dir" : "/var/log/hive/audit/solr/spool",
            "xasecure.audit.destination.solr.urls" : "",
            "xasecure.audit.destination.solr.zookeepers" : "NONE",
            "xasecure.audit.is.enabled" : "true",
            "xasecure.audit.provider.summary.enabled" : "false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-hive-security",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ranger.plugin.hive.policy.cache.dir" : "/etc/ranger/{{repo_name}}/policycache",
            "ranger.plugin.hive.policy.pollIntervalMs" : "30000",
            "ranger.plugin.hive.policy.rest.ssl.config.file" : "/usr/hdp/current/{{ranger_hive_component}}/conf/conf.server/ranger-policymgr-ssl.xml",
            "ranger.plugin.hive.policy.rest.url" : "{{policymgr_mgr_url}}",
            "ranger.plugin.hive.policy.source.impl" : "org.apache.ranger.admin.client.RangerAdminRESTClient",
            "ranger.plugin.hive.service.name" : "{{repo_name}}",
            "ranger.plugin.hive.urlauth.filesystem.schemes" : "hdfs:,file:,wasb:,adl:",
            "xasecure.hive.update.xapolicies.on.grant.revoke" : "true"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553552935,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "HIVE",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=IOCACHE&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "iocache-lds-log4j-properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\nlog4j.rootLogger=DEBUG, R, ETW, Anonymizer, FullPIILogs\n\nlog4j.appender.R=org.apache.log4j.RollingFileAppender\nlog4j.appender.R.File=/var/log/iocache/lds.log\nlog4j.appender.R.MaxFileSize=10MB\nlog4j.appender.R.MaxBackupIndex=2\nlog4j.appender.R.layout=org.apache.log4j.PatternLayout\nlog4j.appender.R.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss,SSS} %p %t %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=iocachelds\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=iocachelds\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=iocachelds\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true\n        "
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "iocache-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "alert_result_code_critical" : "CRITICAL",
            "alert_result_code_ok" : "OK",
            "alert_result_code_unknown" : "UNKNOWN",
            "iocache_bks_pid_file" : "/var/run/iocache-bookkeeperserver.pid",
            "iocache_controller_pid_file" : "/var/run/iocache-controller.pid",
            "iocache_lds_pid_file" : "/var/run/iocache-localdatatransferserver.pid"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "iocache-bks-log4j-properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\nlog4j.rootLogger=DEBUG, R, ETW, Anonymizer, FullPIILogs\n\nlog4j.appender.R=org.apache.log4j.RollingFileAppender\nlog4j.appender.R.File=/var/log/iocache/bks.log\nlog4j.appender.R.MaxFileSize=10MB\nlog4j.appender.R.MaxBackupIndex=2\nlog4j.appender.R.layout=org.apache.log4j.PatternLayout\nlog4j.appender.R.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss,SSS} %p %t %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=iocachebks\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=iocachebks\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=iocachebks\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true\n        "
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553539147,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "IOCACHE",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=IOCACHE&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "iocache-lds-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\nlog4j.rootLogger=DEBUG, R, ETW, Anonymizer, FullPIILogs\n\nlog4j.appender.R=org.apache.log4j.RollingFileAppender\nlog4j.appender.R.File=/var/log/iocache/lds.log\nlog4j.appender.R.MaxFileSize=10MB\nlog4j.appender.R.MaxBackupIndex=2\nlog4j.appender.R.layout=org.apache.log4j.PatternLayout\nlog4j.appender.R.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss,SSS} %p %t %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=iocachelds\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=iocachelds\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=iocachelds\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "iocache-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "alert_result_code_critical" : "CRITICAL",
            "alert_result_code_ok" : "OK",
            "alert_result_code_unknown" : "UNKNOWN",
            "iocache_bks_pid_file" : "/var/run/iocache-bookkeeperserver.pid",
            "iocache_controller_pid_file" : "/var/run/iocache-controller.pid",
            "iocache_lds_pid_file" : "/var/run/iocache-localdatatransferserver.pid"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "iocache-bks-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\nlog4j.rootLogger=DEBUG, R, ETW, Anonymizer, FullPIILogs\n\nlog4j.appender.R=org.apache.log4j.RollingFileAppender\nlog4j.appender.R.File=/var/log/iocache/bks.log\nlog4j.appender.R.MaxFileSize=10MB\nlog4j.appender.R.MaxBackupIndex=2\nlog4j.appender.R.layout=org.apache.log4j.PatternLayout\nlog4j.appender.R.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss,SSS} %p %t %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=iocachebks\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=iocachebks\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=iocachebks\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553554623,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "IOCACHE",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=JUPYTER&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "jupyter-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : { },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553516179,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "JUPYTER",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=MAPREDUCE2&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "mapred-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# export JAVA_HOME=/home/y/libexec/jdk1.6.0/\n\nexport HADOOP_JOB_HISTORYSERVER_HEAPSIZE={{jobhistory_heapsize}}\n\nexport HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA\n\nexport HADOOP_JOB_HISTORYSERVER_OPTS=\"-Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=historyserver\"\n#export HADOOP_MAPRED_LOG_DIR=\"\" # Where log files are stored.  $HADOOP_MAPRED_HOME/logs by default.\n#export HADOOP_JHS_LOGGER=INFO,RFA # Hadoop JobSummary logger.\n#export HADOOP_MAPRED_PID_DIR= # The pid files are stored. /tmp by default.\n#export HADOOP_MAPRED_IDENT_STRING= #A string representing this instance of hadoop. $USER by default\n#export HADOOP_MAPRED_NICENESS= #The scheduling priority for daemons. Defaults to 0.\nexport HADOOP_OPTS=\"-Dhdp.version=$HDP_VERSION $HADOOP_OPTS\"\n  ",
            "jobhistory_heapsize" : "900",
            "mapred_log_dir_prefix" : "/var/log/hadoop-mapreduce",
            "mapred_pid_dir_prefix" : "/var/run/hadoop-mapreduce",
            "mapred_user" : "mapred",
            "mapred_user_nofile_limit" : "32768",
            "mapred_user_nproc_limit" : "65536"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "mapred-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "mapreduce.admin.map.child.java.opts" : "-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}",
            "mapreduce.admin.reduce.child.java.opts" : "-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}",
            "mapreduce.admin.user.env" : "LD_LIBRARY_PATH=/usr/hdp/${hdp.version}/hadoop/lib/native:/usr/hdp/${hdp.version}/hadoop/lib/native/Linux-{{architecture}}-64:./mr-framework/hadoop/lib/native:./mr-framework/hadoop/lib/native/Linux-{{architecture}}-64",
            "mapreduce.am.max-attempts" : "5",
            "mapreduce.application.classpath" : "$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure:/usr/hdp/current/ext/hadoop/*",
            "mapreduce.application.framework.path" : "/hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework",
            "mapreduce.cluster.administrators" : " hadoop",
            "mapreduce.cluster.local.dir" : "/mnt/resource/hadoop/mapred/local",
            "mapreduce.fileoutputcommitter.algorithm.version" : "2",
            "mapreduce.framework.name" : "yarn",
            "mapreduce.job.counters.max" : "130",
            "mapreduce.job.emit-timeline-data" : "false",
            "mapreduce.job.queuename" : "default",
            "mapreduce.job.reduce.slowstart.completedmaps" : "0.05",
            "mapreduce.jobhistory.address" : "headnodehost:10020",
            "mapreduce.jobhistory.admin.address" : "headnodehost:10033",
            "mapreduce.jobhistory.always-scan-user-dir" : "True",
            "mapreduce.jobhistory.bind-host" : "0.0.0.0",
            "mapreduce.jobhistory.done-dir" : "/mr-history/done",
            "mapreduce.jobhistory.http.policy" : "HTTP_ONLY",
            "mapreduce.jobhistory.intermediate-done-dir" : "/mr-history/tmp",
            "mapreduce.jobhistory.recovery.enable" : "true",
            "mapreduce.jobhistory.recovery.store.class" : "org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService",
            "mapreduce.jobhistory.recovery.store.leveldb.path" : "/hadoop/mapreduce/jhs",
            "mapreduce.jobhistory.webapp.address" : "headnodehost:19888",
            "mapreduce.map.java.opts" : "-Xmx1024M -Xms1024M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",
            "mapreduce.map.log.level" : "INFO",
            "mapreduce.map.maxattempts" : "10",
            "mapreduce.map.memory.mb" : "1536",
            "mapreduce.map.output.compress" : "false",
            "mapreduce.map.sort.spill.percent" : "0.7",
            "mapreduce.map.speculative" : "false",
            "mapreduce.output.fileoutputformat.compress" : "false",
            "mapreduce.output.fileoutputformat.compress.type" : "BLOCK",
            "mapreduce.reduce.input.buffer.percent" : "0.0",
            "mapreduce.reduce.java.opts" : "-Xmx1024M -Xms1024M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",
            "mapreduce.reduce.log.level" : "INFO",
            "mapreduce.reduce.maxattempts" : "10",
            "mapreduce.reduce.memory.mb" : "1536",
            "mapreduce.reduce.shuffle.fetch.retry.enabled" : "1",
            "mapreduce.reduce.shuffle.fetch.retry.interval-ms" : "1000",
            "mapreduce.reduce.shuffle.fetch.retry.timeout-ms" : "30000",
            "mapreduce.reduce.shuffle.input.buffer.percent" : "0.7",
            "mapreduce.reduce.shuffle.merge.percent" : "0.66",
            "mapreduce.reduce.shuffle.parallelcopies" : "30",
            "mapreduce.reduce.speculative" : "false",
            "mapreduce.shuffle.port" : "13562",
            "mapreduce.shuffle.transferTo.allowed" : "true",
            "mapreduce.task.io.sort.factor" : "100",
            "mapreduce.task.io.sort.mb" : "614",
            "mapreduce.task.timeout" : "300000",
            "yarn.app.mapreduce.am.admin-command-opts" : "-Dhdp.version=${hdp.version}",
            "yarn.app.mapreduce.am.command-opts" : "-Xmx1024M -Xms1024M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",
            "yarn.app.mapreduce.am.log.level" : "INFO",
            "yarn.app.mapreduce.am.resource.mb" : "1536",
            "yarn.app.mapreduce.am.staging-dir" : "/mapreducestaging",
            "yarn.app.mapreduce.client.job.max-retries" : "30"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "mapred-logsearch-conf",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "component_mappings" : "HISTORYSERVER:mapred_historyserver",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"mapred_historyserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/mapred-env/mapred_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/mapred-env/mapred_user', 'mapred')}}/mapred-{{default('configurations/mapred-env/mapred_user', 'mapred')}}-historyserver*.log\"\n    }\n   ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"mapred_historyserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\\\(%{JAVAFILE:file}:%{JAVAMETHOD:method}\\\\(%{INT:line_number}\\\\)\\\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     }\n   ]\n }\n    ",
            "service_name" : "MapReduce"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553519841,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "MAPREDUCE2",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=MAPREDUCE2&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "mapred-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# export JAVA_HOME=/home/y/libexec/jdk1.6.0/\n\nexport HADOOP_JOB_HISTORYSERVER_HEAPSIZE={{jobhistory_heapsize}}\n\nexport HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA\n\nexport HADOOP_JOB_HISTORYSERVER_OPTS=\"-Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=historyserver\"\n#export HADOOP_MAPRED_LOG_DIR=\"\" # Where log files are stored.  $HADOOP_MAPRED_HOME/logs by default.\n#export HADOOP_JHS_LOGGER=INFO,RFA # Hadoop JobSummary logger.\n#export HADOOP_MAPRED_PID_DIR= # The pid files are stored. /tmp by default.\n#export HADOOP_MAPRED_IDENT_STRING= #A string representing this instance of hadoop. $USER by default\n#export HADOOP_MAPRED_NICENESS= #The scheduling priority for daemons. Defaults to 0.\nexport HADOOP_OPTS=\"-Dhdp.version=$HDP_VERSION $HADOOP_OPTS\"",
            "jobhistory_heapsize" : "900",
            "mapred_log_dir_prefix" : "/var/log/hadoop-mapreduce",
            "mapred_pid_dir_prefix" : "/var/run/hadoop-mapreduce",
            "mapred_user" : "mapred",
            "mapred_user_nofile_limit" : "32768",
            "mapred_user_nproc_limit" : "65536"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "mapred-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "mapreduce.admin.map.child.java.opts" : "-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}",
            "mapreduce.admin.reduce.child.java.opts" : "-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}",
            "mapreduce.admin.user.env" : "LD_LIBRARY_PATH=/usr/hdp/${hdp.version}/hadoop/lib/native:/usr/hdp/${hdp.version}/hadoop/lib/native/Linux-{{architecture}}-64:./mr-framework/hadoop/lib/native:./mr-framework/hadoop/lib/native/Linux-{{architecture}}-64",
            "mapreduce.am.max-attempts" : "5",
            "mapreduce.application.classpath" : "$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure:/usr/hdp/current/ext/hadoop/*",
            "mapreduce.application.framework.path" : "/hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework",
            "mapreduce.cluster.administrators" : " hadoop",
            "mapreduce.cluster.local.dir" : "/mnt/resource/hadoop/mapred/local",
            "mapreduce.fileoutputcommitter.algorithm.version" : "2",
            "mapreduce.framework.name" : "yarn",
            "mapreduce.job.counters.max" : "130",
            "mapreduce.job.emit-timeline-data" : "false",
            "mapreduce.job.queuename" : "default",
            "mapreduce.job.reduce.slowstart.completedmaps" : "0.05",
            "mapreduce.jobhistory.address" : "headnodehost:10020",
            "mapreduce.jobhistory.admin.address" : "headnodehost:10033",
            "mapreduce.jobhistory.always-scan-user-dir" : "True",
            "mapreduce.jobhistory.bind-host" : "0.0.0.0",
            "mapreduce.jobhistory.done-dir" : "/mr-history/done",
            "mapreduce.jobhistory.http.policy" : "HTTP_ONLY",
            "mapreduce.jobhistory.intermediate-done-dir" : "/mr-history/tmp",
            "mapreduce.jobhistory.recovery.enable" : "true",
            "mapreduce.jobhistory.recovery.store.class" : "org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService",
            "mapreduce.jobhistory.recovery.store.leveldb.path" : "/hadoop/mapreduce/jhs",
            "mapreduce.jobhistory.webapp.address" : "headnodehost:19888",
            "mapreduce.map.java.opts" : "-Xmx1024M -Xms1024M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",
            "mapreduce.map.log.level" : "INFO",
            "mapreduce.map.maxattempts" : "10",
            "mapreduce.map.memory.mb" : "1536",
            "mapreduce.map.output.compress" : "false",
            "mapreduce.map.sort.spill.percent" : "0.7",
            "mapreduce.map.speculative" : "false",
            "mapreduce.output.fileoutputformat.compress" : "false",
            "mapreduce.output.fileoutputformat.compress.type" : "BLOCK",
            "mapreduce.reduce.input.buffer.percent" : "0.0",
            "mapreduce.reduce.java.opts" : "-Xmx1024M -Xms1024M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",
            "mapreduce.reduce.log.level" : "INFO",
            "mapreduce.reduce.maxattempts" : "10",
            "mapreduce.reduce.memory.mb" : "1536",
            "mapreduce.reduce.shuffle.fetch.retry.enabled" : "1",
            "mapreduce.reduce.shuffle.fetch.retry.interval-ms" : "1000",
            "mapreduce.reduce.shuffle.fetch.retry.timeout-ms" : "30000",
            "mapreduce.reduce.shuffle.input.buffer.percent" : "0.7",
            "mapreduce.reduce.shuffle.merge.percent" : "0.66",
            "mapreduce.reduce.shuffle.parallelcopies" : "30",
            "mapreduce.reduce.speculative" : "false",
            "mapreduce.shuffle.port" : "13562",
            "mapreduce.shuffle.transferTo.allowed" : "true",
            "mapreduce.task.io.sort.factor" : "100",
            "mapreduce.task.io.sort.mb" : "614",
            "mapreduce.task.timeout" : "300000",
            "yarn.app.mapreduce.am.admin-command-opts" : "-Dhdp.version=${hdp.version}",
            "yarn.app.mapreduce.am.command-opts" : "-Xmx1024M -Xms1024M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",
            "yarn.app.mapreduce.am.log.level" : "INFO",
            "yarn.app.mapreduce.am.resource.mb" : "1536",
            "yarn.app.mapreduce.am.staging-dir" : "/mapreducestaging",
            "yarn.app.mapreduce.client.job.max-retries" : "30"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "mapred-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "HISTORYSERVER:mapred_historyserver",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"mapred_historyserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/mapred-env/mapred_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/mapred-env/mapred_user', 'mapred')}}/mapred-{{default('configurations/mapred-env/mapred_user', 'mapred')}}-historyserver*.log\"\n    }\n   ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"mapred_historyserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\\\(%{JAVAFILE:file}:%{JAVAMETHOD:method}\\\\(%{INT:line_number}\\\\)\\\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     }\n   ]\n }",
            "service_name" : "MapReduce"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553549661,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "MAPREDUCE2",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=OOZIE&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "oozie-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "oozie.action.retry.interval" : "30",
            "oozie.authentication.simple.anonymous.allowed" : "true",
            "oozie.authentication.type" : "simple",
            "oozie.base.url" : "http://hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:11000/oozie",
            "oozie.credentials.credentialclasses" : "hcat=org.apache.oozie.action.hadoop.HCatCredentials,hive2=org.apache.oozie.action.hadoop.Hive2Credentials",
            "oozie.db.schema.name" : "ooziemetastore",
            "oozie.service.AuthorizationService.security.enabled" : "false",
            "oozie.service.HadoopAccessorService.hadoop.configurations" : "*={{hadoop_conf_dir}}",
            "oozie.service.HadoopAccessorService.kerberos.enabled" : "false",
            "oozie.service.HadoopAccessorService.supported.filesystems" : "hdfs,hftp,webhdfs,asv,wasb,asvs,wasbs,abfs,abfss",
            "oozie.service.JPAService.connection.properties" : "lockTimeout=120000",
            "oozie.service.JPAService.create.db.schema" : "false",
            "oozie.service.JPAService.jdbc.driver" : "com.microsoft.sqlserver.jdbc.SQLServerDriver",
            "oozie.service.JPAService.jdbc.password" : "SECRET:oozie-site:1:oozie.service.JPAService.jdbc.password",
            "oozie.service.JPAService.jdbc.url" : "jdbc:sqlserver://rurrh7sfk0.database.windows.net;databaseName=v368131f056aa2d44e882781a655de2485eooziemetastore;sendStringParametersAsUnicode=false;trustServerCertificate=false;encrypt=true;hostNameInCertificate=*.database.windows.net;",
            "oozie.service.JPAService.jdbc.username" : "v368131f056aa2d44e882781a655de2485eooziemetastoreLogin",
            "oozie.service.JPAService.validate.db.connection" : "true",
            "oozie.service.JPAService.validate.db.connection.eviction.interval" : "45000",
            "oozie.service.JPAService.validate.db.connection.eviction.num" : "10",
            "oozie.service.PurgeService.older.than" : "3",
            "oozie.service.SparkConfigurationService.spark.configurations" : "*=spark-conf",
            "oozie.service.URIHandlerService.uri.handlers" : "org.apache.oozie.dependency.FSURIHandler,org.apache.oozie.dependency.HCatURIHandler",
            "oozie.service.coord.push.check.requeue.interval" : "30000",
            "oozie.services.ext" : "org.apache.oozie.service.JMSAccessorService,org.apache.oozie.service.PartitionDependencyManagerService,org.apache.oozie.service.HCatAccessorService, org.apache.oozie.service.ZKLocksService,org.apache.oozie.service.ZKXLogStreamingService,org.apache.oozie.service.ZKJobsConcurrencyService,org.apache.oozie.service.ZKUUIDService",
            "oozie.system.id" : "oozie-${user.name}",
            "oozie.systemmode" : "NORMAL",
            "oozie.zookeeper.connection.string" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "oozie.zookeeper.namespace" : "oozie"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "oozie-logsearch-conf",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "component_mappings" : "OOZIE_SERVER:oozie_app",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"oozie_app\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/oozie-env/oozie_log_dir', '/var/log/oozie')}}/oozie.log\"\n    }\n   ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"oozie_app\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %5p %c{1}:%L - SERVER[${oozie.instance.id}] %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{DATA:logger_name}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     }\n   ]\n }\n    ",
            "service_name" : "Oozie"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "oozie-log4j",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License. See accompanying LICENSE file.\n#\n\n# If the Java System property 'oozie.log.dir' is not defined at Oozie start up time\n# XLogService sets its value to '${oozie.home}/logs'\n\n# The appender that Oozie uses must be named 'oozie' (i.e. log4j.appender.oozie)\n\n# Using the RollingFileAppender with the OozieRollingPolicy will roll the log file every hour and retain up to MaxHistory number of\n# log files. If FileNamePattern ends with \".gz\" it will create gzip files.\nlog4j.appender.oozie=org.apache.log4j.rolling.RollingFileAppender\nlog4j.appender.oozie.RollingPolicy=org.apache.oozie.util.OozieRollingPolicy\nlog4j.appender.oozie.File=${oozie.log.dir}/oozie.log\nlog4j.appender.oozie.Append=true\nlog4j.appender.oozie.layout=org.apache.log4j.PatternLayout\nlog4j.appender.oozie.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - SERVER[${oozie.instance.id}] %m%n\n# The FileNamePattern must end with \"-%d{yyyy-MM-dd-HH}.gz\" or \"-%d{yyyy-MM-dd-HH}\" and also start with the \n# value of log4j.appender.oozie.File\nlog4j.appender.oozie.RollingPolicy.FileNamePattern=${log4j.appender.oozie.File}-%d{yyyy-MM-dd-HH}\n# The MaxHistory controls how many log files will be retained (720 hours / 24 hours per day = 30 days); -1 to disable\nlog4j.appender.oozie.RollingPolicy.MaxHistory=720\n\n\n\nlog4j.appender.oozieError=org.apache.log4j.rolling.RollingFileAppender\nlog4j.appender.oozieError.RollingPolicy=org.apache.oozie.util.OozieRollingPolicy\nlog4j.appender.oozieError.File=${oozie.log.dir}/oozie-error.log\nlog4j.appender.oozieError.Append=true\nlog4j.appender.oozieError.layout=org.apache.log4j.PatternLayout\nlog4j.appender.oozieError.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - SERVER[${oozie.instance.id}] %m%n\n# The FileNamePattern must end with \"-%d{yyyy-MM-dd-HH}.gz\" or \"-%d{yyyy-MM-dd-HH}\" and also start with the\n# value of log4j.appender.oozieError.File\nlog4j.appender.oozieError.RollingPolicy.FileNamePattern=${log4j.appender.oozieError.File}-%d{yyyy-MM-dd-HH}\n# The MaxHistory controls how many log files will be retained (720 hours / 24 hours per day = 30 days); -1 to disable\nlog4j.appender.oozieError.RollingPolicy.MaxHistory=720\nlog4j.appender.oozieError.filter.1 = org.apache.log4j.varia.LevelMatchFilter\nlog4j.appender.oozieError.filter.1.levelToMatch = WARN\nlog4j.appender.oozieError.filter.2 = org.apache.log4j.varia.LevelMatchFilter\nlog4j.appender.oozieError.filter.2.levelToMatch = ERROR\nlog4j.appender.oozieError.filter.3 = org.apache.log4j.varia.LevelMatchFilter\nlog4j.appender.oozieError.filter.3.levelToMatch = FATAL\nlog4j.appender.oozieError.filter.4 = org.apache.log4j.varia.DenyAllFilter\n\n\n\n# Uncomment the below two lines to use the DailyRollingFileAppender instead\n# The DatePattern must end with either \"dd\" or \"HH\"\n#log4j.appender.oozie=org.apache.log4j.DailyRollingFileAppender\n#log4j.appender.oozie.DatePattern='.'yyyy-MM-dd-HH\n\nlog4j.appender.oozieops=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.oozieops.DatePattern='.'yyyy-MM-dd\nlog4j.appender.oozieops.File=${oozie.log.dir}/oozie-ops.log\nlog4j.appender.oozieops.Append=true\nlog4j.appender.oozieops.layout=org.apache.log4j.PatternLayout\nlog4j.appender.oozieops.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - %m%n\n\nlog4j.appender.oozieinstrumentation=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.oozieinstrumentation.DatePattern='.'yyyy-MM-dd\nlog4j.appender.oozieinstrumentation.File=${oozie.log.dir}/oozie-instrumentation.log\nlog4j.appender.oozieinstrumentation.Append=true\nlog4j.appender.oozieinstrumentation.layout=org.apache.log4j.PatternLayout\nlog4j.appender.oozieinstrumentation.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - %m%n\n\nlog4j.appender.oozieaudit=org.apache.log4j.rolling.RollingFileAppender\nlog4j.appender.oozieaudit.RollingPolicy=org.apache.oozie.util.OozieRollingPolicy\nlog4j.appender.oozieaudit.File=${oozie.log.dir}/oozie-audit.log\nlog4j.appender.oozieaudit.Append=true\nlog4j.appender.oozieaudit.layout=org.apache.log4j.PatternLayout\nlog4j.appender.oozieaudit.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - %m%n\nlog4j.appender.oozieaudit.RollingPolicy.FileNamePattern=${log4j.appender.oozieaudit.File}.%d{yyyy-MM-dd}\nlog4j.appender.oozieaudit.RollingPolicy.MaxHistory=30\n\n\nlog4j.appender.openjpa=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.openjpa.DatePattern='.'yyyy-MM-dd\nlog4j.appender.openjpa.File=${oozie.log.dir}/oozie-jpa.log\nlog4j.appender.openjpa.Append=true\nlog4j.appender.openjpa.layout=org.apache.log4j.PatternLayout\nlog4j.appender.openjpa.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - %m%n\nlog4j.category.openjpa.Tool=INFO\nlog4j.category.openjpa.Runtime=INFO\nlog4j.category.openjpa.Remote=WARN\nlog4j.category.openjpa.DataCache=WARN\nlog4j.category.openjpa.MetaData=WARN\nlog4j.category.openjpa.Enhance=WARN\nlog4j.category.openjpa.Query=WARN\nlog4j.category.openjpa.jdbc.SQL=WARN\nlog4j.category.openjpa.jdbc.JDBC=WARN\nlog4j.category.openjpa.jdbc.Schema=WARN\n\nlog4j.appender.jetty=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.jetty.DatePattern='.'yyyy-MM-dd\nlog4j.appender.jetty.File=${oozie.log.dir}/jetty.log\nlog4j.appender.jetty.Append=true\nlog4j.appender.jetty.layout=org.apache.log4j.PatternLayout\nlog4j.appender.jetty.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - %m%n\n\nlog4j.appender.none=org.apache.log4j.varia.NullAppender\n\n# Explicitly switch off root logger: anything interesting goes\n# already either to jetty or one of the oozie appenders\nlog4j.rootLogger=NONE, none\nlog4j.logger.org.eclipse.jetty=INFO, jetty\nlog4j.logger.openjpa=INFO, openjpa\nlog4j.logger.oozieops=INFO, oozieops\nlog4j.logger.oozieinstrumentation=ALL, oozieinstrumentation\nlog4j.logger.oozieaudit=ALL, oozieaudit\nlog4j.logger.org.apache.oozie=INFO, oozie, oozieError\nlog4j.logger.org.apache.hadoop=WARN, oozie\nlog4j.logger.org.mortbay=WARN, oozie\nlog4j.logger.org.hsqldb=WARN, oozie\nlog4j.logger.org.apache.hadoop.security.authentication.server=WARN, oozie",
            "oozie_log_maxhistory" : "720"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "oozie-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n#!/bin/bash\n\nif [ -d \"/usr/lib/bigtop-tomcat\" ]; then\n  export OOZIE_CONFIG=${OOZIE_CONFIG:-/etc/oozie/conf}\n  export CATALINA_BASE=${CATALINA_BASE:-{{oozie_server_dir}}}\n  export CATALINA_TMPDIR=${CATALINA_TMPDIR:-/var/tmp/oozie}\n  export OOZIE_CATALINA_HOME=/usr/lib/bigtop-tomcat\nfi\n\n#Set JAVA HOME\nexport JAVA_HOME={{java_home}}\n\nexport JRE_HOME=${JAVA_HOME}\n\n# Set Oozie specific environment variables here.\n\n# Settings for the Embedded Tomcat that runs Oozie\n# Java System properties for Oozie should be specified in this variable\n#\n# export CATALINA_OPTS=\n\n# Oozie configuration file to load from Oozie configuration directory\n#\n# export OOZIE_CONFIG_FILE=oozie-site.xml\n\n# Oozie logs directory\n#\nexport OOZIE_LOG={{oozie_log_dir}}\n\n# Oozie pid directory\n#\nexport CATALINA_PID={{pid_file}}\n\n#Location of the data for oozie\nexport OOZIE_DATA={{oozie_data_dir}}\n\n# Oozie Log4J configuration file to load from Oozie configuration directory\n#\n# export OOZIE_LOG4J_FILE=oozie-log4j.properties\n\n# Reload interval of the Log4J configuration file, in seconds\n#\n# export OOZIE_LOG4J_RELOAD=10\n\n# The port Oozie server runs\n#\nexport OOZIE_HTTP_PORT={{oozie_server_port}}\n\n# The admin port Oozie server runs\n#\nexport OOZIE_ADMIN_PORT={{oozie_server_admin_port}}\n\n# The host name Oozie server runs on\n#\n# export OOZIE_HTTP_HOSTNAME=`hostname -f`\n\n# The base URL for callback URLs to Oozie\n#\n# export OOZIE_BASE_URL=\"http://${OOZIE_HTTP_HOSTNAME}:${OOZIE_HTTP_PORT}/oozie\"\nexport JAVA_LIBRARY_PATH={{hadoop_lib_home}}/native/Linux-amd64-64\n\n# At least 1 minute of retry time to account for server downtime during\n# upgrade/downgrade\nexport OOZIE_CLIENT_OPTS=\"${OOZIE_CLIENT_OPTS} -Doozie.connection.retry.count=5 \"\n\n# This is needed so that Oozie does not run into OOM or GC Overhead limit\n# exceeded exceptions. If the oozie server is handling large number of\n# workflows/coordinator jobs, the memory settings may need to be revised\nexport CATALINA_OPTS=\"${CATALINA_OPTS} -Xmx2048m -XX:MaxPermSize=256m -Dwhitelist.filename=core-whitelist.res,oozie-whitelist.res -Dcomponent=oozie\"",
            "oozie_admin_port" : "11001",
            "oozie_admin_users" : "{oozie_user}, oozie-admin",
            "oozie_data_dir" : "/hadoop/oozie/data",
            "oozie_database" : "Existing MSSQL Server database with SQL authentication",
            "oozie_database_type" : "mssql",
            "oozie_existing_mssql_server_database" : "v368131f056aa2d44e882781a655de2485eooziemetastore",
            "oozie_existing_mssql_server_host" : "rurrh7sfk0.database.windows.net",
            "oozie_heapsize" : "2048",
            "oozie_hostname" : "rurrh7sfk0.database.windows.net",
            "oozie_log_dir" : "/var/log/oozie",
            "oozie_permsize" : "256",
            "oozie_pid_dir" : "/var/run/oozie",
            "oozie_tmp_dir" : "/var/tmp/oozie",
            "oozie_user" : "oozie",
            "oozie_user_nofile_limit" : "32000",
            "oozie_user_nproc_limit" : "16000",
            "service_check_job_name" : "no-op"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553516624,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "OOZIE",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=OOZIE&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "oozie-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "oozie.action.retry.interval" : "30",
            "oozie.authentication.simple.anonymous.allowed" : "true",
            "oozie.authentication.type" : "simple",
            "oozie.base.url" : "http://hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:11000/oozie",
            "oozie.credentials.credentialclasses" : "hcat=org.apache.oozie.action.hadoop.HCatCredentials,hive2=org.apache.oozie.action.hadoop.Hive2Credentials",
            "oozie.db.schema.name" : "ooziemetastore",
            "oozie.service.AuthorizationService.security.enabled" : "false",
            "oozie.service.HadoopAccessorService.hadoop.configurations" : "*={{hadoop_conf_dir}}",
            "oozie.service.HadoopAccessorService.kerberos.enabled" : "false",
            "oozie.service.HadoopAccessorService.supported.filesystems" : "hdfs,hftp,webhdfs,asv,wasb,asvs,wasbs,abfs,abfss",
            "oozie.service.JPAService.connection.properties" : "lockTimeout=120000",
            "oozie.service.JPAService.create.db.schema" : "false",
            "oozie.service.JPAService.jdbc.driver" : "com.microsoft.sqlserver.jdbc.SQLServerDriver",
            "oozie.service.JPAService.jdbc.password" : "SECRET:oozie-site:2:oozie.service.JPAService.jdbc.password",
            "oozie.service.JPAService.jdbc.url" : "jdbc:sqlserver://rurrh7sfk0.database.windows.net;databaseName=v368131f056aa2d44e882781a655de2485eooziemetastore;sendStringParametersAsUnicode=false;trustServerCertificate=false;encrypt=true;hostNameInCertificate=*.database.windows.net;",
            "oozie.service.JPAService.jdbc.username" : "v368131f056aa2d44e882781a655de2485eooziemetastoreLogin",
            "oozie.service.JPAService.validate.db.connection" : "true",
            "oozie.service.JPAService.validate.db.connection.eviction.interval" : "45000",
            "oozie.service.JPAService.validate.db.connection.eviction.num" : "10",
            "oozie.service.PurgeService.older.than" : "3",
            "oozie.service.SparkConfigurationService.spark.configurations" : "*=spark-conf",
            "oozie.service.URIHandlerService.uri.handlers" : "org.apache.oozie.dependency.FSURIHandler,org.apache.oozie.dependency.HCatURIHandler",
            "oozie.service.coord.push.check.requeue.interval" : "30000",
            "oozie.services.ext" : "org.apache.oozie.service.JMSAccessorService,org.apache.oozie.service.PartitionDependencyManagerService,org.apache.oozie.service.HCatAccessorService, org.apache.oozie.service.ZKLocksService,org.apache.oozie.service.ZKXLogStreamingService,org.apache.oozie.service.ZKJobsConcurrencyService,org.apache.oozie.service.ZKUUIDService",
            "oozie.system.id" : "oozie-${user.name}",
            "oozie.systemmode" : "NORMAL",
            "oozie.zookeeper.connection.string" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "oozie.zookeeper.namespace" : "oozie"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "oozie-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "OOZIE_SERVER:oozie_app",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"oozie_app\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/oozie-env/oozie_log_dir', '/var/log/oozie')}}/oozie.log\"\n    }\n   ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"oozie_app\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %5p %c{1}:%L - SERVER[${oozie.instance.id}] %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{DATA:logger_name}:%{INT:line_number}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     }\n   ]\n }",
            "service_name" : "Oozie"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "oozie-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License. See accompanying LICENSE file.\n#\n\n# If the Java System property 'oozie.log.dir' is not defined at Oozie start up time\n# XLogService sets its value to '${oozie.home}/logs'\n\n# The appender that Oozie uses must be named 'oozie' (i.e. log4j.appender.oozie)\n\n# Using the RollingFileAppender with the OozieRollingPolicy will roll the log file every hour and retain up to MaxHistory number of\n# log files. If FileNamePattern ends with \".gz\" it will create gzip files.\nlog4j.appender.oozie=org.apache.log4j.rolling.RollingFileAppender\nlog4j.appender.oozie.RollingPolicy=org.apache.oozie.util.OozieRollingPolicy\nlog4j.appender.oozie.File=${oozie.log.dir}/oozie.log\nlog4j.appender.oozie.Append=true\nlog4j.appender.oozie.layout=org.apache.log4j.PatternLayout\nlog4j.appender.oozie.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - SERVER[${oozie.instance.id}] %m%n\n# The FileNamePattern must end with \"-%d{yyyy-MM-dd-HH}.gz\" or \"-%d{yyyy-MM-dd-HH}\" and also start with the \n# value of log4j.appender.oozie.File\nlog4j.appender.oozie.RollingPolicy.FileNamePattern=${log4j.appender.oozie.File}-%d{yyyy-MM-dd-HH}\n# The MaxHistory controls how many log files will be retained (720 hours / 24 hours per day = 30 days); -1 to disable\nlog4j.appender.oozie.RollingPolicy.MaxHistory=720\n\n\n\nlog4j.appender.oozieError=org.apache.log4j.rolling.RollingFileAppender\nlog4j.appender.oozieError.RollingPolicy=org.apache.oozie.util.OozieRollingPolicy\nlog4j.appender.oozieError.File=${oozie.log.dir}/oozie-error.log\nlog4j.appender.oozieError.Append=true\nlog4j.appender.oozieError.layout=org.apache.log4j.PatternLayout\nlog4j.appender.oozieError.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - SERVER[${oozie.instance.id}] %m%n\n# The FileNamePattern must end with \"-%d{yyyy-MM-dd-HH}.gz\" or \"-%d{yyyy-MM-dd-HH}\" and also start with the\n# value of log4j.appender.oozieError.File\nlog4j.appender.oozieError.RollingPolicy.FileNamePattern=${log4j.appender.oozieError.File}-%d{yyyy-MM-dd-HH}\n# The MaxHistory controls how many log files will be retained (720 hours / 24 hours per day = 30 days); -1 to disable\nlog4j.appender.oozieError.RollingPolicy.MaxHistory=720\nlog4j.appender.oozieError.filter.1 = org.apache.log4j.varia.LevelMatchFilter\nlog4j.appender.oozieError.filter.1.levelToMatch = WARN\nlog4j.appender.oozieError.filter.2 = org.apache.log4j.varia.LevelMatchFilter\nlog4j.appender.oozieError.filter.2.levelToMatch = ERROR\nlog4j.appender.oozieError.filter.3 = org.apache.log4j.varia.LevelMatchFilter\nlog4j.appender.oozieError.filter.3.levelToMatch = FATAL\nlog4j.appender.oozieError.filter.4 = org.apache.log4j.varia.DenyAllFilter\n\n\n\n# Uncomment the below two lines to use the DailyRollingFileAppender instead\n# The DatePattern must end with either \"dd\" or \"HH\"\n#log4j.appender.oozie=org.apache.log4j.DailyRollingFileAppender\n#log4j.appender.oozie.DatePattern='.'yyyy-MM-dd-HH\n\nlog4j.appender.oozieops=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.oozieops.DatePattern='.'yyyy-MM-dd\nlog4j.appender.oozieops.File=${oozie.log.dir}/oozie-ops.log\nlog4j.appender.oozieops.Append=true\nlog4j.appender.oozieops.layout=org.apache.log4j.PatternLayout\nlog4j.appender.oozieops.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - %m%n\n\nlog4j.appender.oozieinstrumentation=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.oozieinstrumentation.DatePattern='.'yyyy-MM-dd\nlog4j.appender.oozieinstrumentation.File=${oozie.log.dir}/oozie-instrumentation.log\nlog4j.appender.oozieinstrumentation.Append=true\nlog4j.appender.oozieinstrumentation.layout=org.apache.log4j.PatternLayout\nlog4j.appender.oozieinstrumentation.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - %m%n\n\nlog4j.appender.oozieaudit=org.apache.log4j.rolling.RollingFileAppender\nlog4j.appender.oozieaudit.RollingPolicy=org.apache.oozie.util.OozieRollingPolicy\nlog4j.appender.oozieaudit.File=${oozie.log.dir}/oozie-audit.log\nlog4j.appender.oozieaudit.Append=true\nlog4j.appender.oozieaudit.layout=org.apache.log4j.PatternLayout\nlog4j.appender.oozieaudit.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - %m%n\nlog4j.appender.oozieaudit.RollingPolicy.FileNamePattern=${log4j.appender.oozieaudit.File}.%d{yyyy-MM-dd}\nlog4j.appender.oozieaudit.RollingPolicy.MaxHistory=30\n\n\nlog4j.appender.openjpa=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.openjpa.DatePattern='.'yyyy-MM-dd\nlog4j.appender.openjpa.File=${oozie.log.dir}/oozie-jpa.log\nlog4j.appender.openjpa.Append=true\nlog4j.appender.openjpa.layout=org.apache.log4j.PatternLayout\nlog4j.appender.openjpa.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - %m%n\nlog4j.category.openjpa.Tool=INFO\nlog4j.category.openjpa.Runtime=INFO\nlog4j.category.openjpa.Remote=WARN\nlog4j.category.openjpa.DataCache=WARN\nlog4j.category.openjpa.MetaData=WARN\nlog4j.category.openjpa.Enhance=WARN\nlog4j.category.openjpa.Query=WARN\nlog4j.category.openjpa.jdbc.SQL=WARN\nlog4j.category.openjpa.jdbc.JDBC=WARN\nlog4j.category.openjpa.jdbc.Schema=WARN\n\nlog4j.appender.jetty=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.jetty.DatePattern='.'yyyy-MM-dd\nlog4j.appender.jetty.File=${oozie.log.dir}/jetty.log\nlog4j.appender.jetty.Append=true\nlog4j.appender.jetty.layout=org.apache.log4j.PatternLayout\nlog4j.appender.jetty.layout.ConversionPattern=%d{ISO8601} %5p %c{1}:%L - %m%n\n\nlog4j.appender.none=org.apache.log4j.varia.NullAppender\n\n# Explicitly switch off root logger: anything interesting goes\n# already either to jetty or one of the oozie appenders\nlog4j.rootLogger=NONE, none\nlog4j.logger.org.eclipse.jetty=INFO, jetty\nlog4j.logger.openjpa=INFO, openjpa\nlog4j.logger.oozieops=INFO, oozieops\nlog4j.logger.oozieinstrumentation=ALL, oozieinstrumentation\nlog4j.logger.oozieaudit=ALL, oozieaudit\nlog4j.logger.org.apache.oozie=INFO, oozie, oozieError\nlog4j.logger.org.apache.hadoop=WARN, oozie\nlog4j.logger.org.mortbay=WARN, oozie\nlog4j.logger.org.hsqldb=WARN, oozie\nlog4j.logger.org.apache.hadoop.security.authentication.server=WARN, oozie",
            "oozie_log_maxhistory" : "720"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "oozie-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#!/bin/bash\n\nif [ -d \"/usr/lib/bigtop-tomcat\" ]; then\n  export OOZIE_CONFIG=${OOZIE_CONFIG:-/etc/oozie/conf}\n  export CATALINA_BASE=${CATALINA_BASE:-{{oozie_server_dir}}}\n  export CATALINA_TMPDIR=${CATALINA_TMPDIR:-/var/tmp/oozie}\n  export OOZIE_CATALINA_HOME=/usr/lib/bigtop-tomcat\nfi\n\n#Set JAVA HOME\nexport JAVA_HOME={{java_home}}\n\nexport JRE_HOME=${JAVA_HOME}\n\n# Set Oozie specific environment variables here.\n\n# Settings for the Embedded Tomcat that runs Oozie\n# Java System properties for Oozie should be specified in this variable\n#\n# export CATALINA_OPTS=\n\n# Oozie configuration file to load from Oozie configuration directory\n#\n# export OOZIE_CONFIG_FILE=oozie-site.xml\n\n# Oozie logs directory\n#\nexport OOZIE_LOG={{oozie_log_dir}}\n\n# Oozie pid directory\n#\nexport CATALINA_PID={{pid_file}}\n\n#Location of the data for oozie\nexport OOZIE_DATA={{oozie_data_dir}}\n\n# Oozie Log4J configuration file to load from Oozie configuration directory\n#\n# export OOZIE_LOG4J_FILE=oozie-log4j.properties\n\n# Reload interval of the Log4J configuration file, in seconds\n#\n# export OOZIE_LOG4J_RELOAD=10\n\n# The port Oozie server runs\n#\nexport OOZIE_HTTP_PORT={{oozie_server_port}}\n\n# The admin port Oozie server runs\n#\nexport OOZIE_ADMIN_PORT={{oozie_server_admin_port}}\n\n# The host name Oozie server runs on\n#\n# export OOZIE_HTTP_HOSTNAME=`hostname -f`\n\n# The base URL for callback URLs to Oozie\n#\n# export OOZIE_BASE_URL=\"http://${OOZIE_HTTP_HOSTNAME}:${OOZIE_HTTP_PORT}/oozie\"\nexport JAVA_LIBRARY_PATH={{hadoop_lib_home}}/native/Linux-amd64-64\n\n# At least 1 minute of retry time to account for server downtime during\n# upgrade/downgrade\nexport OOZIE_CLIENT_OPTS=\"${OOZIE_CLIENT_OPTS} -Doozie.connection.retry.count=5 \"\n\n# This is needed so that Oozie does not run into OOM or GC Overhead limit\n# exceeded exceptions. If the oozie server is handling large number of\n# workflows/coordinator jobs, the memory settings may need to be revised\nexport CATALINA_OPTS=\"${CATALINA_OPTS} -Xmx2048m -XX:MaxPermSize=256m -Dwhitelist.filename=core-whitelist.res,oozie-whitelist.res -Dcomponent=oozie\"",
            "oozie_admin_port" : "11001",
            "oozie_admin_users" : "{oozie_user}, oozie-admin",
            "oozie_data_dir" : "/hadoop/oozie/data",
            "oozie_database" : "Existing MSSQL Server database with SQL authentication",
            "oozie_database_type" : "mssql",
            "oozie_existing_mssql_server_database" : "v368131f056aa2d44e882781a655de2485eooziemetastore",
            "oozie_existing_mssql_server_host" : "rurrh7sfk0.database.windows.net",
            "oozie_heapsize" : "2048m",
            "oozie_hostname" : "rurrh7sfk0.database.windows.net",
            "oozie_log_dir" : "/var/log/oozie",
            "oozie_permsize" : "256m",
            "oozie_pid_dir" : "/var/run/oozie",
            "oozie_tmp_dir" : "/var/tmp/oozie",
            "oozie_user" : "oozie",
            "oozie_user_nofile_limit" : "32000",
            "oozie_user_nproc_limit" : "16000",
            "service_check_job_name" : "no-op"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553548271,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "OOZIE",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=PIG&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "pig-properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Pig default configuration file. All values can be overwritten by pig.properties and command line arguments.\n# see bin/pig -help\n\n# brief logging (no timestamps)\nbrief=false\n\n# debug level, INFO is default\ndebug=INFO\n\n# verbose print all log messages to screen (default to print only INFO and above to screen)\nverbose=false\n\n# exectype local|mapreduce|tez, mapreduce is default\nexectype=tez\n\n# Enable insertion of information about script into hadoop job conf \npig.script.info.enabled=true\n\n# Do not spill temp files smaller than this size (bytes)\npig.spill.size.threshold=5000000\n\n# EXPERIMENT: Activate garbage collection when spilling a file bigger than this size (bytes)\n# This should help reduce the number of files being spilled.\npig.spill.gc.activation.size=40000000\n\n# the following two parameters are to help estimate the reducer number\npig.exec.reducers.bytes.per.reducer=1000000000\npig.exec.reducers.max=999\n\n# Temporary location to store the intermediate data.\npig.temp.dir=/tmp/\n\n# Threshold for merging FRJoin fragment files\npig.files.concatenation.threshold=100\npig.optimistic.files.concatenation=false;\n\npig.disable.counter=false\n\nhcat.bin=/usr/bin/hcat\n\n    "
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "pig-log4j",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n#\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n#\n#\n\n# ***** Set root logger level to DEBUG and its only appender to A.\nlog4j.logger.org.apache.pig=info, A\n\n# ***** A is set to be a ConsoleAppender.\nlog4j.appender.A=org.apache.log4j.ConsoleAppender\n# ***** A uses PatternLayout.\nlog4j.appender.A.layout=org.apache.log4j.PatternLayout\nlog4j.appender.A.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n\n    "
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "pig-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\nJAVA_HOME={{java64_home}}\nHADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\nif [ -d \"/usr/lib/tez\" ]; then\n  PIG_OPTS=\"$PIG_OPTS -Dmapreduce.framework.name=yarn\"\nfi\n    "
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553523861,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "PIG",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=PIG&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "pig-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Pig default configuration file. All values can be overwritten by pig.properties and command line arguments.\n# see bin/pig -help\n\n# brief logging (no timestamps)\nbrief=false\n\n# debug level, INFO is default\ndebug=INFO\n\n# verbose print all log messages to screen (default to print only INFO and above to screen)\nverbose=false\n\n# exectype local|mapreduce|tez, mapreduce is default\nexectype=tez\n\n# Enable insertion of information about script into hadoop job conf \npig.script.info.enabled=true\n\n# Do not spill temp files smaller than this size (bytes)\npig.spill.size.threshold=5000000\n\n# EXPERIMENT: Activate garbage collection when spilling a file bigger than this size (bytes)\n# This should help reduce the number of files being spilled.\npig.spill.gc.activation.size=40000000\n\n# the following two parameters are to help estimate the reducer number\npig.exec.reducers.bytes.per.reducer=1000000000\npig.exec.reducers.max=999\n\n# Temporary location to store the intermediate data.\npig.temp.dir=/tmp/\n\n# Threshold for merging FRJoin fragment files\npig.files.concatenation.threshold=100\npig.optimistic.files.concatenation=false;\n\npig.disable.counter=false\n\nhcat.bin=/usr/bin/hcat"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "pig-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n#\n#\n\n# ***** Set root logger level to DEBUG and its only appender to A.\nlog4j.logger.org.apache.pig=info, A\n\n# ***** A is set to be a ConsoleAppender.\nlog4j.appender.A=org.apache.log4j.ConsoleAppender\n# ***** A uses PatternLayout.\nlog4j.appender.A.layout=org.apache.log4j.PatternLayout\nlog4j.appender.A.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "pig-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\nJAVA_HOME={{java64_home}}\nHADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\nif [ -d \"/usr/lib/tez\" ]; then\n  PIG_OPTS=\"$PIG_OPTS -Dmapreduce.framework.name=yarn\"\nfi"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553550663,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "PIG",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}\n",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout\n        "
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>\n        "
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}\n    ",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553546577,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553555608,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=3",
      "cluster_name" : "sparkpoc",
      "configurations" : [ ],
      "createtime" : 1535610629472,
      "group_id" : 2,
      "group_name" : "overridetry",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 3,
      "service_config_version_note" : null,
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=4",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1538660319758",
          "version" : 3,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n*.sink.prometheus.pushgateway-address-protocol=<prometheus pushgateway protocol> - defaults to http\n*.sink.prometheus.pushgateway-address=<prometheus pushgateway address> - defaults to 127.0.0.1:9091\n*.sink.prometheus.period=<period> - defaults to 10\n*.sink.prometheus.unit=< unit> - defaults to seconds (TimeUnit.SECONDS)\n*.sink.prometheus.pushgateway-enable-timestamp=<enable/disable metrics timestamp> - defaults to false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1538660320316,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 4,
      "service_config_version_note" : "Updated metrics.properties with prometheus",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=5",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1538663917348",
          "version" : 4,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1538663917471,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 5,
      "service_config_version_note" : "Removed prometheus",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=6",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1538664301170",
          "version" : 5,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n#*.sink.prometheus.pushgateway-address-protocol=<prometheus pushgateway protocol> - defaults to http\n#*.sink.prometheus.pushgateway-address=<prometheus pushgateway address> - defaults to 127.0.0.1:9091\n#*.sink.prometheus.period=<period> - defaults to 10\n#*.sink.prometheus.unit=TimeUnit - defaults to seconds (TimeUnit.SECONDS)\n#*.sink.prometheus.pushgateway-enable-timestamp=<enable/disable metrics timestamp> - defaults to false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1538664301289,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 6,
      "service_config_version_note" : "Added prometheus",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=7",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1538664638666",
          "version" : 6,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n*.sink.prometheus.pushgateway-address-protocol=http\n*.sink.prometheus.pushgateway-address=localhost:9091\n*.sink.prometheus.period=10\n*.sink.prometheus.unit=TimeUnit.SECONDS\n*.sink.prometheus.pushgateway-enable-timestamp=false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1538664638795,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 7,
      "service_config_version_note" : "Edited prometheus configs\n",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=8",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1538665066871",
          "version" : 7,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n*.sink.prometheus.pushgateway-address-protocol=http\n*.sink.prometheus.pushgateway-address=localhost:9091\n*.sink.prometheus.period=10\n*.sink.prometheus.unit=seconds\n*.sink.prometheus.pushgateway-enable-timestamp=false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1538665066984,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 8,
      "service_config_version_note" : "Changing seconds value",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=9",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1538912530677",
          "version" : 8,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n## Enable Prometheus for all instances by class name\n#*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n## Prometheus pushgateway address\n#*.sink.prometheus.pushgateway-address-protocol=http\n#*.sink.prometheus.pushgateway-address=localhost:9091\n#*.sink.prometheus.period=10\n#*.sink.prometheus.unit=seconds\n#*.sink.prometheus.pushgateway-enable-timestamp=false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1538912530768,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 9,
      "service_config_version_note" : "",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=10",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1538920750443",
          "version" : 9,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n*.sink.prometheus.pushgateway-address-protocol=http\n*.sink.prometheus.pushgateway-address=localhost:9091\n*.sink.prometheus.period=10\n*.sink.prometheus.unit=seconds\n*.sink.prometheus.pushgateway-enable-timestamp=false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1538920750508,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 10,
      "service_config_version_note" : "Prometheus is back",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=11",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1539178670041",
          "version" : 10,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n#*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n#*.sink.prometheus.pushgateway-address-protocol=http\n#*.sink.prometheus.pushgateway-address=localhost:9091\n#*.sink.prometheus.period=10\n#*.sink.prometheus.unit=seconds\n#*.sink.prometheus.pushgateway-enable-timestamp=false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1539178670156,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 11,
      "service_config_version_note" : "Removing prometheus",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=12",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1541919413205",
          "version" : 11,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n*.sink.prometheus.pushgateway-address-protocol=http\n*.sink.prometheus.pushgateway-address=localhost:9091\n*.sink.prometheus.period=10\n*.sink.prometheus.unit=seconds\n*.sink.prometheus.pushgateway-enable-timestamp=false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1541919413711,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 12,
      "service_config_version_note" : "Prom activated",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=13",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1541923016001",
          "version" : 12,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n*.sink.prometheus.pushgateway-address-protocol=http\n*.sink.prometheus.pushgateway-address=localhost:9091\n*.sink.prometheus.period=10\n*.sink.prometheus.unit=seconds\n*.sink.prometheus.pushgateway-enable-timestamp=false\n*.sink.prometheus.enable-jmx-collector=true"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1541923016128,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 13,
      "service_config_version_note" : "",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=14",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1541923927288",
          "version" : 13,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n*.sink.prometheus.pushgateway-address-protocol=http\n*.sink.prometheus.pushgateway-address=localhost:9091\n*.sink.prometheus.period=10\n*.sink.prometheus.unit=seconds\n*.sink.prometheus.pushgateway-enable-timestamp=false\n*.sink.prometheus.enable-jmx-collector=false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1541923927402,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 14,
      "service_config_version_note" : "jmx false",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=15",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1542715161034",
          "version" : 14,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n#*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n#*.sink.prometheus.pushgateway-address-protocol=http\n#*.sink.prometheus.pushgateway-address=localhost:9091\n#*.sink.prometheus.period=10\n#*.sink.prometheus.unit=seconds\n#*.sink.prometheus.pushgateway-enable-timestamp=false\n#*.sink.prometheus.enable-jmx-collector=false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1542715161311,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 15,
      "service_config_version_note" : "remove prom",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=16",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1542897487233",
          "version" : 15,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n*.sink.statsd.prefix=spark\n\n\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n#*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n#*.sink.prometheus.pushgateway-address-protocol=http\n#*.sink.prometheus.pushgateway-address=localhost:9091\n#*.sink.prometheus.period=10\n#*.sink.prometheus.unit=seconds\n#*.sink.prometheus.pushgateway-enable-timestamp=false\n#*.sink.prometheus.enable-jmx-collector=false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1542897487381,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 16,
      "service_config_version_note" : "Trying to add statsD",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=17",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1543239907706",
          "version" : 16,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n#*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n#*.sink.statsd.prefix=spark\n\n\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n#*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n#*.sink.prometheus.pushgateway-address-protocol=http\n#*.sink.prometheus.pushgateway-address=localhost:9091\n#*.sink.prometheus.period=10\n#*.sink.prometheus.unit=seconds\n#*.sink.prometheus.pushgateway-enable-timestamp=false\n#*.sink.prometheus.enable-jmx-collector=false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1543239907787,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 17,
      "service_config_version_note" : "Removing STATSD",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=18",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1543239907706",
          "version" : 16,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n#*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n#*.sink.statsd.prefix=spark\n\n\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n#*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n#*.sink.prometheus.pushgateway-address-protocol=http\n#*.sink.prometheus.pushgateway-address=localhost:9091\n#*.sink.prometheus.period=10\n#*.sink.prometheus.unit=seconds\n#*.sink.prometheus.pushgateway-enable-timestamp=false\n#*.sink.prometheus.enable-jmx-collector=false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1543940937418",
          "version" : 3,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\nlog4j.logger.fluentdLogger=info, fluentd\nlog4j.additivity.fluentdLogger=false\nlog4j.appender.fluentd=fluentd.log4jsupport.fluent4log4j\nlog4j.appender.fluentd.host=tomerHost\nlog4j.appender.fluentd.layout=fluentd.log4jsupport.GenevaLayout"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1543940937539,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 18,
      "service_config_version_note" : "log4j\n",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=19",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1543239907706",
          "version" : 16,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n#*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n#*.sink.statsd.prefix=spark\n\n\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n#*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n#*.sink.prometheus.pushgateway-address-protocol=http\n#*.sink.prometheus.pushgateway-address=localhost:9091\n#*.sink.prometheus.period=10\n#*.sink.prometheus.unit=seconds\n#*.sink.prometheus.pushgateway-enable-timestamp=false\n#*.sink.prometheus.enable-jmx-collector=false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1543947192130",
          "version" : 4,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\nlog4j.logger.fluentdLogger=info, fluentd\nlog4j.additivity.fluentdLogger=false\nlog4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\nlog4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1543947192227,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 19,
      "service_config_version_note" : "fdf",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=20",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1543239907706",
          "version" : 16,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n#*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n#*.sink.statsd.prefix=spark\n\n\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n# Enable Prometheus for all instances by class name\n#*.sink.prometheus.class=com.banzaicloud.spark.metrics.sink.PrometheusSink\n# Prometheus pushgateway address\n#*.sink.prometheus.pushgateway-address-protocol=http\n#*.sink.prometheus.pushgateway-address=localhost:9091\n#*.sink.prometheus.period=10\n#*.sink.prometheus.unit=seconds\n#*.sink.prometheus.pushgateway-enable-timestamp=false\n#*.sink.prometheus.enable-jmx-collector=false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1543948303421",
          "version" : 5,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n#log4j.logger.fluentdLogger=info, fluentd\nl#og4j.additivity.fluentdLogger=false\nl#og4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\nl#og4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1543948303523,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 20,
      "service_config_version_note" : "dfd",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=21",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1543948303421",
          "version" : 5,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n#log4j.logger.fluentdLogger=info, fluentd\nl#og4j.additivity.fluentdLogger=false\nl#og4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\nl#og4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1545560444568",
          "version" : 17,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n*.sink.statsd.host=localhost\n*.sink.statsd.port=8126\n\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1545560445182,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 21,
      "service_config_version_note" : "",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=22",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1543948303421",
          "version" : 5,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n#log4j.logger.fluentdLogger=info, fluentd\nl#og4j.additivity.fluentdLogger=false\nl#og4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\nl#og4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1545563690124",
          "version" : 18,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n#*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n#*.sink.statsd.host=localhost\n#*.sink.statsd.port=8126\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1545563689591,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 22,
      "service_config_version_note" : "",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=23",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1543948303421",
          "version" : 5,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n#log4j.logger.fluentdLogger=info, fluentd\nl#og4j.additivity.fluentdLogger=false\nl#og4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\nl#og4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1545567225415",
          "version" : 19,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n*.sink.statsd.host=localhost\n*.sink.statsd.port=8126\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1545567225551,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 23,
      "service_config_version_note" : "dfd",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=24",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1545567225415",
          "version" : 19,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n*.sink.statsd.host=localhost\n*.sink.statsd.port=8126\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1545641441013",
          "version" : 6,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n#log4j.logger.fluentdLogger=info, fluentd\n#log4j.additivity.fluentdLogger=false\n#log4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\n#log4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1545641441147,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 24,
      "service_config_version_note" : "Removing fluentd as logger",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=25",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1545641441013",
          "version" : 6,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n#log4j.logger.fluentdLogger=info, fluentd\n#log4j.additivity.fluentdLogger=false\n#log4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\n#log4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1545667159755",
          "version" : 20,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n#*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n#*.sink.statsd.host=localhost\n#*.sink.statsd.port=8126\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1545667159928,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 25,
      "service_config_version_note" : "removing statsd",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=26",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1545641441013",
          "version" : 6,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n#log4j.logger.fluentdLogger=info, fluentd\n#log4j.additivity.fluentdLogger=false\n#log4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\n#log4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1545819857213",
          "version" : 21,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n*.sink.statsd.host=localhost\n*.sink.statsd.port=8126\n*.sink.statsd.mdm_namespace=\"TomerTestNamespace\"\n\ndriver.source.wdatpsource.class=org.apache.spark.metrics.source.WdatpSource\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1545819857366,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 26,
      "service_config_version_note" : "Adding wdatp metrics",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=27",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1545819857213",
          "version" : 21,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n*.sink.statsd.host=localhost\n*.sink.statsd.port=8126\n*.sink.statsd.mdm_namespace=\"TomerTestNamespace\"\n\ndriver.source.wdatpsource.class=org.apache.spark.metrics.source.WdatpSource\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1550505579087",
          "version" : 7,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n##log4j.logger.fluentdLogger=info, fluentd\n#log4j.additivity.fluentdLogger=false\n#log4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\n#log4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1550505579692,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 27,
      "service_config_version_note" : "Double commenting",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=28",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1550505579087",
          "version" : 7,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n##log4j.logger.fluentdLogger=info, fluentd\n#log4j.additivity.fluentdLogger=false\n#log4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\n#log4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1551196373065",
          "version" : 22,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n#*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n#*.sink.statsd.host=localhost\n#*.sink.statsd.port=8126\n#*.sink.statsd.mdm_namespace=\"TomerTestNamespace\"\n\ndriver.source.wdatpsource.class=org.apache.spark.metrics.source.WdatpSource\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1551196373315,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 28,
      "service_config_version_note" : "Removing sink",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=29",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1551196373065",
          "version" : 22,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n#*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n#*.sink.statsd.host=localhost\n#*.sink.statsd.port=8126\n#*.sink.statsd.mdm_namespace=\"TomerTestNamespace\"\n\ndriver.source.wdatpsource.class=org.apache.spark.metrics.source.WdatpSource\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1551196433465",
          "version" : 8,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.out\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n##log4j.logger.fluentdLogger=info, fluentd\n#log4j.additivity.fluentdLogger=false\n#log4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\n#log4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1551196433635,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 29,
      "service_config_version_note" : "Setting logger to STDOUT",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=30",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1551196373065",
          "version" : 22,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n#*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n#*.sink.statsd.host=localhost\n#*.sink.statsd.port=8126\n#*.sink.statsd.mdm_namespace=\"TomerTestNamespace\"\n\ndriver.source.wdatpsource.class=org.apache.spark.metrics.source.WdatpSource\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1551248583866",
          "version" : 9,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console,wdatpWarn\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.out\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# WDATP Logger\nlog4j.appender.wdatpWarn.Threshold=WARN\nlog4j.appender.wdatpWarn=org.apache.log4j.ConsoleAppender\nlog4j.appender.wdatpWarn.target=System.err\nlog4j.appender.wdatpWarn.layout=org.apache.log4j.PatternLayout\nlog4j.appender.wdatpWarn.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n##log4j.logger.fluentdLogger=info, fluentd\n#log4j.additivity.fluentdLogger=false\n#log4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\n#log4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1551248584009,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 30,
      "service_config_version_note" : "Adding WDATP Logger",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=31",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1551196373065",
          "version" : 22,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n#*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n#*.sink.statsd.host=localhost\n#*.sink.statsd.port=8126\n#*.sink.statsd.mdm_namespace=\"TomerTestNamespace\"\n\ndriver.source.wdatpsource.class=org.apache.spark.metrics.source.WdatpSource\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "version1551256191921",
          "version" : 3,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,wdatpWarn,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n# WDATP Logger\nlog4j.appender.wdatpWarn=org.apache.log4j.ConsoleAppender\nlog4j.appender.wdatpWarn.target=System.err\nlog4j.appender.wdatpWarn.layout=org.apache.log4j.PatternLayout\nlog4j.appender.wdatpWarn.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.wdatpWarn.Threshold=WARN\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1551256191922",
          "version" : 10,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console,wdatpWarn\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.out\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# WDATP Logger\nlog4j.appender.wdatpWarn=org.apache.log4j.ConsoleAppender\nlog4j.appender.wdatpWarn.target=System.err\nlog4j.appender.wdatpWarn.layout=org.apache.log4j.PatternLayout\nlog4j.appender.wdatpWarn.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.wdatpWarn.Threshold=WARN\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n##log4j.logger.fluentdLogger=info, fluentd\n#log4j.additivity.fluentdLogger=false\n#log4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\n#log4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1551256192139,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 31,
      "service_config_version_note" : "Adding livy log",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=32",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1551196373065",
          "version" : 22,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n#*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n#*.sink.statsd.host=localhost\n#*.sink.statsd.port=8126\n#*.sink.statsd.mdm_namespace=\"TomerTestNamespace\"\n\ndriver.source.wdatpsource.class=org.apache.spark.metrics.source.WdatpSource\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "version1551256191921",
          "version" : 3,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,wdatpWarn,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n# WDATP Logger\nlog4j.appender.wdatpWarn=org.apache.log4j.ConsoleAppender\nlog4j.appender.wdatpWarn.target=System.err\nlog4j.appender.wdatpWarn.layout=org.apache.log4j.PatternLayout\nlog4j.appender.wdatpWarn.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.wdatpWarn.Threshold=WARN\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1551265471478",
          "version" : 11,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console,wdatpWarn\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.out\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# WDATP Logger\nlog4j.appender.wdatpWarn.Threshold=INFO\nlog4j.appender.wdatpWarn=org.apache.log4j.ConsoleAppender\nlog4j.appender.wdatpWarn.target=System.err\nlog4j.appender.wdatpWarn.layout=org.apache.log4j.PatternLayout\nlog4j.appender.wdatpWarn.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n##log4j.logger.fluentdLogger=info, fluentd\n#log4j.additivity.fluentdLogger=false\n#log4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\n#log4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1551265471618,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 32,
      "service_config_version_note" : "changing order of threshold",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=33",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1551196373065",
          "version" : 22,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n#*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n#*.sink.statsd.host=localhost\n#*.sink.statsd.port=8126\n#*.sink.statsd.mdm_namespace=\"TomerTestNamespace\"\n\ndriver.source.wdatpsource.class=org.apache.spark.metrics.source.WdatpSource\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "version1551265682812",
          "version" : 4,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,wdatpWarn,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1551265682813",
          "version" : 12,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, wdatp, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.out\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# WDATP Logger\nlog4j.appender.wdatp.Threshold=INFO\nlog4j.appender.wdatp=org.apache.log4j.ConsoleAppender\nlog4j.appender.wdatp.target=System.err\nlog4j.appender.wdatp.layout=org.apache.log4j.PatternLayout\nlog4j.appender.wdatp.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n##log4j.logger.fluentdLogger=info, fluentd\n#log4j.additivity.fluentdLogger=false\n#log4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\n#log4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1551265683018,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 33,
      "service_config_version_note" : "Changing logger configuration",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SPARK2&service_config_version=34",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "livy.environment" : "production",
            "livy.impersonation.enabled" : "true",
            "livy.repl.enableHiveContext" : "true",
            "livy.server.csrf_protection.enabled" : "true",
            "livy.server.port" : "8998",
            "livy.server.recovery.mode" : "recovery",
            "livy.server.recovery.state-store" : "zookeeper",
            "livy.server.recovery.state-store.url" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "livy.server.session.state-retain.sec" : "3600000",
            "livy.server.session.timeout" : "3600000",
            "livy.server.yarn.app-lookup-timeout" : "2m",
            "livy.spark.master" : "yarn-cluster"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_HOME=/usr/hdp/current/spark2-client\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport LIVY_LOG_DIR=/var/log/livy2\nexport LIVY_MAX_LOG_FILES=30\nexport LIVY_PID_DIR=/var/run/livy2\nexport LIVY_SERVER_JAVA_OPTS=\"-Xmx2g\"\nYARN_CLASSPATH=`yarn classpath`\nexport CLASSPATH=\"$YARN_CLASSPATH:/usr/lib/hdinsight-logging/*\"",
            "livy2_group" : "livy",
            "livy2_log_dir" : "/var/log/livy2",
            "livy2_pid_dir" : "/var/run/livy2",
            "livy2_user" : "livy",
            "spark_home" : "/usr/hdp/current/spark2-client"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-log4j-properties",
          "tag" : "version1551265682812",
          "version" : 4,
          "properties" : {
            "content" : "\n# The default Livy logging configuration.\nlog4j.rootLogger=INFO,console,wdatpWarn,ETW,Anonymizer,SparkMetrics,FullPIILogs\nlogFilter.filename=SparkLogFilters.xml\npatternGroup.filename=SparkPatternGroups.xml\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.org.eclipse.jetty=INFO\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparklivy\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparklivy\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# FilteredSparkMetrics Appender\nlog4j.appender.SparkMetrics=com.microsoft.log4jappender.FilteredSparkMetricsAppender\nlog4j.appender.SparkMetrics.source=CentralSparkEventsLog\nlog4j.appender.SparkMetrics.Threshold=INFO\nlog4j.appender.SparkMetrics.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparklivy\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "livy2-spark-blacklist",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n            #\n            # Configuration override / blacklist. Defines a list of properties that users are not allowed\n            # to override when starting Spark sessions.\n            #\n            # This file takes a list of property names (one per line). Empty lines and lines starting with \"#\"\n            # are ignored.\n            #\n\n            # Disallow overriding the master and the deploy mode.\n            spark.master\n            spark.submit.deployMode\n\n            # Disallow overriding the location of Spark cached jars.\n            spark.yarn.jar\n            spark.yarn.jars\n            spark.yarn.archive\n\n            # Don't allow users to override the RSC timeout.\n            livy.rsc.server.idle_timeout"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-defaults",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.instances" : "2",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.extraListeners" : "com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.kerberos.keytab" : "none",
            "spark.history.kerberos.principal" : "none",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.yarn.access.hadoopFileSystems" : "hdfs://mycluster",
            "spark.yarn.appMasterEnv.PYSPARK3_PYTHON" : "/usr/bin/anaconda/envs/py35/bin/python3",
            "spark.yarn.appMasterEnv.PYSPARK_PYTHON" : "/usr/bin/anaconda/bin/python",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.historyServer.address" : "{{spark_history_server_host}}:{{spark_history_ui_port}}",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "default",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "#!/usr/bin/env bash\n\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\nexport SPARK_LOG_DIR={{spark_log_dir}}\nexport SPARK_PID_DIR={{spark_pid_dir}}\nexport SPARK_MAJOR_VERSION=2\nSPARK_IDENT_STRING=$USER\nSPARK_NICENESS=0\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\nexport SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/usr/hdp/current/spark2-client/jars/*:/usr/lib/hdinsight-datalake/*:/usr/hdp/current/spark_llap/*:/usr/hdp/current/spark2-client/conf:\nexport JAVA_HOME={{java_home}}\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi\n\n# Tell pyspark (the shell) to use Anaconda Python.\nexport PYSPARK_PYTHON=${PYSPARK_PYTHON:-/usr/bin/anaconda/bin/python}",
            "hive_kerberos_keytab" : "{{hive_kerberos_keytab}}",
            "hive_kerberos_principal" : "{{hive_kerberos_principal}}",
            "spark_daemon_memory" : "1024",
            "spark_group" : "spark",
            "spark_log_dir" : "/var/log/spark2",
            "spark_pid_dir" : "/var/run/spark2",
            "spark_thrift_cmd_opts" : "",
            "spark_user" : "spark"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-hive-site-override",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hive.exec.scratchdir" : "hdfs://mycluster/tmp/hive",
            "hive.metastore.client.connect.retry.delay" : "5",
            "hive.metastore.client.socket.timeout" : "1800",
            "hive.server2.enable.doAs" : "false",
            "hive.server2.thrift.http.path" : "/",
            "hive.server2.thrift.http.port" : "10002",
            "hive.server2.thrift.port" : "10016",
            "hive.server2.transport.mode" : "http"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-log4j-properties",
          "tag" : "version1551266302458",
          "version" : 13,
          "properties" : {
            "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\nlog4jspark.root.logger=INFO,console\nlog4jspark.log.dir=.\nlog4jspark.log.file=spark.log\nlog4jspark.log.maxfilesize=1024MB\nlog4jspark.log.maxbackupindex=10\n\n# Define the root logger to the system property \"spark.root.logger\".\nlog4j.rootLogger=${log4jspark.root.logger}, wdatp, EventCounter\n\n# Set everything to be logged to the console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.out\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\nlog4j.appender.console.Threshold=INFO\n\n# WDATP Logger\nlog4j.appender.wdatp.Threshold=WARN\nlog4j.appender.wdatp=org.apache.log4j.ConsoleAppender\nlog4j.appender.wdatp.target=System.err\nlog4j.appender.wdatp.layout=org.apache.log4j.PatternLayout\nlog4j.appender.wdatp.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${log4jspark.log.dir}/${log4jspark.log.file}\nlog4j.appender.RFA.MaxFileSize=${log4jspark.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${log4jspark.log.maxbackupindex}\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${etwlogger.component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=${etwlogger.component}\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n##log4j.logger.fluentdLogger=info, fluentd\n#log4j.additivity.fluentdLogger=false\n#log4j.appender.fluentd=wdatp.infra.app.spark.logger.app.FluentdLogger\n#log4j.appender.fluentd.host=tomerHost"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "SPARK2_JOBHISTORYSERVER:spark2_jobhistory_server;SPARK2_THRIFTSERVER:spark2_thriftserver;LIVY2_SERVER:livy2_server",
            "content" : "\n{\n   \"input\":[\n      {\n       \"type\":\"spark2_jobhistory_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.deploy.history.HistoryServer*.out\"\n     },\n     {\n       \"type\":\"spark2_thriftserver\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/spark2-env/spark_log_dir', '/var/log/spark2')}}/spark-*-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\"\n     },\n     {\n       \"type\":\"livy2_server\",\n       \"rowtype\":\"service\",\n       \"path\":\"{{default('/configurations/livy2-env/livy2_log_dir', '/var/log/livy2')}}/livy-livy-server.out\"\n     }\n   ],\n   \"filter\":[\n      {\n          \"filter\":\"grok\",\n          \"conditions\":{\n            \"fields\":{\n              \"type\":[\n                \"spark2_jobhistory_server\",\n                \"spark2_thriftserver\",\n                \"livy2_server\"\n              ]\n             }\n          },\n          \"log4j_format\":\"\",\n          \"multiline_pattern\":\"^(%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level})\",\n          \"message_pattern\":\"(?m)^%{SPARK_DATESTAMP:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVAFILE:file}:%{SPACE}%{GREEDYDATA:log_message}\",\n          \"post_map_values\":{\n            \"logtime\":{\n              \"map_date\":{\n                \"target_date_pattern\":\"yy/MM/dd HH:mm:ss\"\n              }\n             },\n            \"level\":{\n              \"map_fieldvalue\":{\n                \"pre_value\":\"WARNING\",\n                \"post_value\":\"WARN\"\n              }\n             }\n           }\n      }\n   ]\n}",
            "service_name" : "Spark2"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-metrics-properties",
          "tag" : "version1551196373065",
          "version" : 22,
          "properties" : {
            "content" : "\n# syntax: [instance].sink|source.[name].[options]=[value]\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n #\n\n## List of available sinks and their properties. \n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=1\n#*.sink.console.unit=minutes\n\n# Master instance overlap polling period\n#master.sink.console.period=1\n#master.sink.console.unit=minutes\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/home/hdiuser/sparklogs\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n#*.sink.csv.unit=minutes\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=1\n#worker.sink.csv.unit=minutes\n\n# Enable Slf4JSink for all instances\n*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink\n\n# Polling period for Slf4jSink\n*.sink.slf4j.period=60\n*.sink.slf4j.unit=minutes\n\n\n# Enable StatsdSink for all instances by class name\n##*.sink.statsd.class=org.apache.spark.metrics.sink.MyStatsdSink\n##*.sink.statsd.prefix=spark\n\n#*.sink.statsd.class=org.apache.spark.metrics.sink.SparkAppStatsDSink\n#*.sink.statsd.host=localhost\n#*.sink.statsd.port=8126\n#*.sink.statsd.mdm_namespace=\"TomerTestNamespace\"\n\ndriver.source.wdatpsource.class=org.apache.spark.metrics.source.WdatpSource\n\n# Enable jvm source for instance master, worker, driver and executor\nmaster.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nworker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\ndriver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\nexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-fairscheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "fairscheduler_content" : "<?xml version=\"1.0\"?>\n            <allocations>\n            <pool name=\"default\">\n            <schedulingMode>FAIR</schedulingMode>\n            <weight>1</weight>\n            <minShare>2</minShare>\n            </pool>\n            </allocations>"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "spark2-thrift-sparkconf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "spark.cores.max" : "1",
            "spark.driver.cores" : "1",
            "spark.driver.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftdriver.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.driver.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.driver.memoryOverhead" : "384",
            "spark.dynamicAllocation.enabled" : "true",
            "spark.dynamicAllocation.executorIdleTimeout" : "1800s",
            "spark.dynamicAllocation.initialExecutors" : "0",
            "spark.dynamicAllocation.maxExecutors" : "1024",
            "spark.dynamicAllocation.minExecutors" : "0",
            "spark.eventLog.dir" : "wasb:///hdp/spark2-events",
            "spark.eventLog.enabled" : "true",
            "spark.executor.cores" : "2",
            "spark.executor.extraJavaOptions" : "-Dhdp.version={{hdp_full_version}} -Detwlogger.component=sparkthriftexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/spark -Dlog4jspark.log.file=sparkthriftexecutor.log -Djava.io.tmpdir=/var/tmp/spark -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC",
            "spark.executor.extraLibraryPath" : "{{spark_hadoop_lib_native}}",
            "spark.executor.memory" : "3072m",
            "spark.executor.memoryOverhead" : "384",
            "spark.hadoop.cacheConf" : "false",
            "spark.history.fs.logDirectory" : "wasb:///hdp/spark2-events",
            "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
            "spark.history.ui.port" : "18080",
            "spark.master" : "yarn",
            "spark.scheduler.allocation.file" : "/etc/sparkthrift/conf/sparkThriftFairScheduler.xml",
            "spark.scheduler.mode" : "FAIR",
            "spark.shuffle.service.enabled" : "true",
            "spark.ui.port" : "5040",
            "spark.yarn.am.memory" : "1g",
            "spark.yarn.containerLauncherMaxThreads" : "25",
            "spark.yarn.executor.failuresValidityInterval" : "2h",
            "spark.yarn.jars" : "local:///usr/hdp/current/spark2-client/jars/*",
            "spark.yarn.maxAppAttempts" : "1",
            "spark.yarn.preserve.staging.files" : "false",
            "spark.yarn.queue" : "thriftsvr",
            "spark.yarn.scheduler.heartbeat.interval-ms" : "5000",
            "spark.yarn.submit.file.replication" : "3"
          },
          "properties_attributes" : {
            "final" : {
              "spark.eventLog.enabled" : "true",
              "spark.eventLog.dir" : "true",
              "spark.history.fs.logDirectory" : "true"
            }
          }
        }
      ],
      "createtime" : 1551266302657,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 34,
      "service_config_version_note" : "Updating to WARN (wdatp loggeR)",
      "service_name" : "SPARK2",
      "stack_id" : "HDP-2.6",
      "user" : "admin"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SQOOP&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "sqoop-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Set Hadoop-specific environment variables here.\n\n#Set path to where bin/hadoop is available\n#Set path to where bin/hadoop is available\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\n#set the path to where bin/hbase is available\nexport HBASE_HOME=${HBASE_HOME:-{{hbase_home}}}\n\n#Set the path to where bin/hive is available\nexport HIVE_HOME=${HIVE_HOME:-{{hive_home}}}\n\n#Set the path for where zookeper config dir is\nexport ZOOCFGDIR=${ZOOCFGDIR:-/etc/zookeeper/conf}\n\n# add libthrift in hive to sqoop class path first so hive imports work\nexport SQOOP_USER_CLASSPATH=\"`ls ${HIVE_HOME}/lib/libthrift-*.jar 2> /dev/null`:${SQOOP_USER_CLASSPATH}\"\n    ",
            "jdbc_drivers" : " ",
            "sqoop.atlas.hook" : "false",
            "sqoop_user" : "sqoop"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "sqoop-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : { },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "sqoop-atlas-application.properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "atlas.jaas.KafkaClient.option.renewTicket" : "true",
            "atlas.jaas.KafkaClient.option.useTicketCache" : "true"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553515869,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "SQOOP",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=SQOOP&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "sqoop-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Set Hadoop-specific environment variables here.\n\n#Set path to where bin/hadoop is available\n#Set path to where bin/hadoop is available\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\n#set the path to where bin/hbase is available\nexport HBASE_HOME=${HBASE_HOME:-{{hbase_home}}}\n\n#Set the path to where bin/hive is available\nexport HIVE_HOME=${HIVE_HOME:-{{hive_home}}}\n\n#Set the path for where zookeper config dir is\nexport ZOOCFGDIR=${ZOOCFGDIR:-/etc/zookeeper/conf}\n\n# add libthrift in hive to sqoop class path first so hive imports work\nexport SQOOP_USER_CLASSPATH=\"`ls ${HIVE_HOME}/lib/libthrift-*.jar 2> /dev/null`:${SQOOP_USER_CLASSPATH}\"",
            "jdbc_drivers" : " ",
            "sqoop.atlas.hook" : "false",
            "sqoop_user" : "sqoop"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "sqoop-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : { },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "sqoop-atlas-application.properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "atlas.jaas.KafkaClient.option.renewTicket" : "true",
            "atlas.jaas.KafkaClient.option.useTicketCache" : "true"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553547995,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "SQOOP",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=TEZ&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "tez-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n# Tez specific configuration\nexport TEZ_CONF_DIR={{config_dir}}\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\n# The java implementation to use.\nexport JAVA_HOME={{java64_home}}\n    ",
            "enable_heap_dump" : "false",
            "heap_dump_location" : "/tmp",
            "tez_user" : "tez"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "tez-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "tez.am.am-rm.heartbeat.interval-ms.max" : "250",
            "tez.am.container.idle.release-timeout-max.millis" : "20000",
            "tez.am.container.idle.release-timeout-min.millis" : "10000",
            "tez.am.container.reuse.enabled" : "true",
            "tez.am.container.reuse.locality.delay-allocation-millis" : "0",
            "tez.am.container.reuse.non-local-fallback.enabled" : "false",
            "tez.am.container.reuse.rack-fallback.enabled" : "true",
            "tez.am.java.opts" : "-Xmx1024M -Xms1024M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",
            "tez.am.launch.cluster-default.cmd-opts" : "-server -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}",
            "tez.am.launch.cmd-opts" : "-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC{{heap_dump_opts}}",
            "tez.am.launch.env" : "LD_LIBRARY_PATH=/usr/hdp/${hdp.version}/hadoop/lib/native:/usr/hdp/${hdp.version}/hadoop/lib/native/Linux-{{architecture}}-64:./tezlib/lib/native:./tezlib/lib/native/Linux-{{architecture}}-64",
            "tez.am.log.level" : "INFO",
            "tez.am.max.app.attempts" : "2",
            "tez.am.maxtaskfailures.per.node" : "10",
            "tez.am.resource.memory.mb" : "1536",
            "tez.am.tez-ui.history-url.template" : "__HISTORY_URL_BASE__?viewPath=%2F%23%2Ftez-app%2F__APPLICATION_ID__",
            "tez.am.ui.history.url.scheme.check.enabled" : "false",
            "tez.am.view-acls" : "*",
            "tez.cluster.additional.classpath.prefix" : "/usr/lib/rubix/*:/usr/hdp/${hdp.version}/hadoop/hadoop-azure-2.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/usr/lib/hdinsight-logging/*",
            "tez.counters.max" : "10000",
            "tez.counters.max.groups" : "3000",
            "tez.dag.recovery.enabled" : "true",
            "tez.generate.debug.artifacts" : "false",
            "tez.grouping.max-size" : "1073741824",
            "tez.grouping.min-size" : "16777216",
            "tez.grouping.split-waves" : "1.7",
            "tez.history.logging.service.class" : "org.apache.tez.dag.history.logging.ats.ATSV15HistoryLoggingService",
            "tez.history.logging.timeline-cache-plugin.old-num-dags-per-group" : "5",
            "tez.history.logging.timeline.num-dags-per-group" : "5",
            "tez.lib.uris" : "/hdp/apps/${hdp.version}/tez/tez.tar.gz",
            "tez.runtime.compress" : "true",
            "tez.runtime.compress.codec" : "org.apache.hadoop.io.compress.SnappyCodec",
            "tez.runtime.convert.user-payload.to.history-text" : "false",
            "tez.runtime.io.sort.mb" : "614",
            "tez.runtime.optimize.local.fetch" : "true",
            "tez.runtime.pipelined.sorter.sort.threads" : "2",
            "tez.runtime.shuffle.fetch.buffer.percent" : "0.6",
            "tez.runtime.shuffle.memory.limit.percent" : "0.25",
            "tez.runtime.sorter.class" : "PIPELINED",
            "tez.runtime.unordered.output.buffer.size-mb" : "100",
            "tez.session.am.dag.submit.timeout.secs" : "300",
            "tez.session.client.timeout.secs" : "-1",
            "tez.shuffle-vertex-manager.max-src-fraction" : "0.4",
            "tez.shuffle-vertex-manager.min-src-fraction" : "0.2",
            "tez.staging-dir" : "/tezstaging/${user.name}/staging",
            "tez.task.am.heartbeat.counter.interval-ms.max" : "4000",
            "tez.task.generate.counters.per.io" : "true",
            "tez.task.get-task.sleep.interval-ms.max" : "200",
            "tez.task.launch.cluster-default.cmd-opts" : "-server -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}",
            "tez.task.launch.cmd-opts" : "-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC{{heap_dump_opts}}",
            "tez.task.launch.env" : "LD_LIBRARY_PATH=/usr/hdp/${hdp.version}/hadoop/lib/native:/usr/hdp/${hdp.version}/hadoop/lib/native/Linux-{{architecture}}-64:./tezlib/lib/native:./tezlib/lib/native/Linux-{{architecture}}-64",
            "tez.task.max-events-per-heartbeat" : "500",
            "tez.task.resource.memory.mb" : "1536",
            "tez.tez-ui.history-url.base" : "/#/main/view/TEZ/tez_cluster_instance",
            "tez.use.cluster.hadoop-libs" : "false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553519178,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "TEZ",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=TEZ&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "tez-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n# Tez specific configuration\nexport TEZ_CONF_DIR={{config_dir}}\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\n# The java implementation to use.\nexport JAVA_HOME={{java64_home}}",
            "enable_heap_dump" : "false",
            "heap_dump_location" : "/tmp",
            "tez_user" : "tez"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "tez-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "tez.am.am-rm.heartbeat.interval-ms.max" : "250",
            "tez.am.container.idle.release-timeout-max.millis" : "20000",
            "tez.am.container.idle.release-timeout-min.millis" : "10000",
            "tez.am.container.reuse.enabled" : "true",
            "tez.am.container.reuse.locality.delay-allocation-millis" : "0",
            "tez.am.container.reuse.non-local-fallback.enabled" : "false",
            "tez.am.container.reuse.rack-fallback.enabled" : "true",
            "tez.am.java.opts" : "-Xmx1024M -Xms1024M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",
            "tez.am.launch.cluster-default.cmd-opts" : "-server -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}",
            "tez.am.launch.cmd-opts" : "-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC{{heap_dump_opts}}",
            "tez.am.launch.env" : "LD_LIBRARY_PATH=/usr/hdp/${hdp.version}/hadoop/lib/native:/usr/hdp/${hdp.version}/hadoop/lib/native/Linux-{{architecture}}-64:./tezlib/lib/native:./tezlib/lib/native/Linux-{{architecture}}-64",
            "tez.am.log.level" : "INFO",
            "tez.am.max.app.attempts" : "2",
            "tez.am.maxtaskfailures.per.node" : "10",
            "tez.am.resource.memory.mb" : "1536",
            "tez.am.tez-ui.history-url.template" : "__HISTORY_URL_BASE__?viewPath=%2F%23%2Ftez-app%2F__APPLICATION_ID__",
            "tez.am.ui.history.url.scheme.check.enabled" : "false",
            "tez.am.view-acls" : "*",
            "tez.cluster.additional.classpath.prefix" : "/usr/lib/rubix/*:/usr/hdp/${hdp.version}/hadoop/hadoop-azure-2.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/usr/lib/hdinsight-logging/*",
            "tez.counters.max" : "10000",
            "tez.counters.max.groups" : "3000",
            "tez.dag.recovery.enabled" : "true",
            "tez.generate.debug.artifacts" : "false",
            "tez.grouping.max-size" : "1073741824",
            "tez.grouping.min-size" : "16777216",
            "tez.grouping.split-waves" : "1.7",
            "tez.history.logging.service.class" : "org.apache.tez.dag.history.logging.ats.ATSV15HistoryLoggingService",
            "tez.history.logging.timeline-cache-plugin.old-num-dags-per-group" : "5",
            "tez.history.logging.timeline.num-dags-per-group" : "5",
            "tez.lib.uris" : "/hdp/apps/${hdp.version}/tez/tez.tar.gz",
            "tez.runtime.compress" : "true",
            "tez.runtime.compress.codec" : "org.apache.hadoop.io.compress.SnappyCodec",
            "tez.runtime.convert.user-payload.to.history-text" : "false",
            "tez.runtime.io.sort.mb" : "614",
            "tez.runtime.optimize.local.fetch" : "true",
            "tez.runtime.pipelined.sorter.sort.threads" : "2",
            "tez.runtime.shuffle.fetch.buffer.percent" : "0.6",
            "tez.runtime.shuffle.memory.limit.percent" : "0.25",
            "tez.runtime.sorter.class" : "PIPELINED",
            "tez.runtime.unordered.output.buffer.size-mb" : "100",
            "tez.session.am.dag.submit.timeout.secs" : "300",
            "tez.session.client.timeout.secs" : "-1",
            "tez.shuffle-vertex-manager.max-src-fraction" : "0.4",
            "tez.shuffle-vertex-manager.min-src-fraction" : "0.2",
            "tez.staging-dir" : "/tezstaging/${user.name}/staging",
            "tez.task.am.heartbeat.counter.interval-ms.max" : "4000",
            "tez.task.generate.counters.per.io" : "true",
            "tez.task.get-task.sleep.interval-ms.max" : "200",
            "tez.task.launch.cluster-default.cmd-opts" : "-server -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}",
            "tez.task.launch.cmd-opts" : "-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC{{heap_dump_opts}}",
            "tez.task.launch.env" : "LD_LIBRARY_PATH=/usr/hdp/${hdp.version}/hadoop/lib/native:/usr/hdp/${hdp.version}/hadoop/lib/native/Linux-{{architecture}}-64:./tezlib/lib/native:./tezlib/lib/native/Linux-{{architecture}}-64",
            "tez.task.max-events-per-heartbeat" : "500",
            "tez.task.resource.memory.mb" : "1536",
            "tez.tez-ui.history-url.base" : "/#/main/view/TEZ/tez_cluster_instance",
            "tez.use.cluster.hadoop-libs" : "false"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553549485,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "TEZ",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=YARN&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-yarn-plugin-properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "REPOSITORY_CONFIG_PASSWORD" : "SECRET:ranger-yarn-plugin-properties:1:REPOSITORY_CONFIG_PASSWORD",
            "REPOSITORY_CONFIG_USERNAME" : "yarn",
            "common.name.for.certificate" : "",
            "external_admin_password" : "",
            "external_admin_username" : "",
            "external_ranger_admin_password" : "",
            "external_ranger_admin_username" : "",
            "hadoop.rpc.protection" : "",
            "policy_user" : "ambari-qa",
            "ranger-yarn-plugin-enabled" : "No"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "capacity-scheduler",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "yarn.scheduler.capacity.default.minimum-user-limit-percent" : "100",
            "yarn.scheduler.capacity.maximum-am-resource-percent" : "0.33",
            "yarn.scheduler.capacity.maximum-applications" : "10000",
            "yarn.scheduler.capacity.node-locality-delay" : "0",
            "yarn.scheduler.capacity.resource-calculator" : "org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator",
            "yarn.scheduler.capacity.root.accessible-node-labels" : "*",
            "yarn.scheduler.capacity.root.acl_administer_queue" : "*",
            "yarn.scheduler.capacity.root.capacity" : "100",
            "yarn.scheduler.capacity.root.default.acl_administer_jobs" : "*",
            "yarn.scheduler.capacity.root.default.acl_submit_applications" : "*",
            "yarn.scheduler.capacity.root.default.capacity" : "50",
            "yarn.scheduler.capacity.root.default.maximum-capacity" : "100",
            "yarn.scheduler.capacity.root.default.state" : "RUNNING",
            "yarn.scheduler.capacity.root.default.user-limit-factor" : "10",
            "yarn.scheduler.capacity.root.queues" : "default,thriftsvr",
            "yarn.scheduler.capacity.root.thriftsvr.capacity" : "50",
            "yarn.scheduler.capacity.root.thriftsvr.maximum-capacity" : "100",
            "yarn.scheduler.capacity.root.thriftsvr.user-limit-factor" : "10"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "yarn-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "apptimelineserver_heapsize" : "1024",
            "content" : "\nexport HADOOP_YARN_HOME={{hadoop_yarn_home}}\nexport YARN_LOG_DIR={{yarn_log_dir_prefix}}/$USER\nexport YARN_PID_DIR={{yarn_pid_dir_prefix}}/$USER\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\nexport JAVA_HOME={{java64_home}}\n\n# User for YARN daemons\nexport HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn}\n\n# resolve links - $0 may be a softlink\nexport YARN_CONF_DIR=\"${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}\"\n\n# some Java parameters\n# export JAVA_HOME=/home/y/libexec/jdk1.6.0/\nif [ \"$JAVA_HOME\" != \"\" ]; then\n  #echo \"run java in $JAVA_HOME\"\n  JAVA_HOME=$JAVA_HOME\nfi\n\nif [ \"$JAVA_HOME\" = \"\" ]; then\n  echo \"Error: JAVA_HOME is not set.\"\n  exit 1\nfi\n\nJAVA=$JAVA_HOME/bin/java\nJAVA_HEAP_MAX=-Xmx1000m\n\n# For setting YARN specific HEAP sizes please use this\n# Parameter and set appropriately\nYARN_HEAPSIZE={{yarn_heapsize}}\n\n# check envvars which might override default args\nif [ \"$YARN_HEAPSIZE\" != \"\" ]; then\n  JAVA_HEAP_MAX=\"-Xmx\"\"$YARN_HEAPSIZE\"\"m\"\nfi\n\n# Resource Manager specific parameters\n\n# Specify the max Heapsize for the ResourceManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1000.\n# This value will be overridden by an Xmx setting specified in either YARN_OPTS\n# and/or YARN_RESOURCEMANAGER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_RESOURCEMANAGER_HEAPSIZE={{resourcemanager_heapsize}}\n\n# Specify the JVM options to be used when starting the ResourceManager.\n# These options will be appended to the options specified as YARN_OPTS\n# and therefore may override any similar flags set in YARN_OPTS\n#export YARN_RESOURCEMANAGER_OPTS=\n\n# Node Manager specific parameters\n\n# Specify the max Heapsize for the NodeManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1000.\n# This value will be overridden by an Xmx setting specified in either YARN_OPTS\n# and/or YARN_NODEMANAGER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_NODEMANAGER_HEAPSIZE={{nodemanager_heapsize}}\n\n# Specify the max Heapsize for the HistoryManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1024.\n# This value will be overridden by an Xmx setting specified in either YARN_OPTS\n# and/or YARN_HISTORYSERVER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_HISTORYSERVER_HEAPSIZE={{apptimelineserver_heapsize}}\nexport YARN_TIMELINESERVER_HEAPSIZE={{apptimelineserver_heapsize}}\nexport YARN_TIMELINESERVER_OPTS=\"-Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=apptimelineserver\"\n\n# Specify the JVM options to be used when starting the NodeManager.\n# These options will be appended to the options specified as YARN_OPTS\n# and therefore may override any similar flags set in YARN_OPTS\nexport YARN_NODEMANAGER_OPTS=\"-Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=nodemanager\"\n\n# so that filenames w/ spaces are handled correctly in loops below\nIFS=\n\n\n# default log directory and file\nif [ \"$YARN_LOG_DIR\" = \"\" ]; then\n  YARN_LOG_DIR=\"$HADOOP_YARN_HOME/logs\"\nfi\nif [ \"$YARN_LOGFILE\" = \"\" ]; then\n  YARN_LOGFILE='yarn.log'\nfi\n\n# default policy file for service-level authorization\nif [ \"$YARN_POLICYFILE\" = \"\" ]; then\n  YARN_POLICYFILE=\"hadoop-policy.xml\"\nfi\n\n# restore ordinary behaviour\nunset IFS\n\n\nYARN_OPTS=\"$YARN_OPTS -Dhadoop.log.dir=$YARN_LOG_DIR\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.log.dir=$YARN_LOG_DIR\"\nYARN_OPTS=\"$YARN_OPTS -Dhadoop.log.file=$YARN_LOGFILE\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.log.file=$YARN_LOGFILE\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.home.dir=$YARN_COMMON_HOME\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.id.str=$YARN_IDENT_STRING\"\nYARN_OPTS=\"$YARN_OPTS -Dhadoop.root.logger=${YARN_ROOT_LOGGER:-INFO,console}\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.root.logger=${YARN_ROOT_LOGGER:-INFO,console}\"\nif [ \"x$JAVA_LIBRARY_PATH\" != \"x\" ]; then\n  YARN_OPTS=\"$YARN_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH\"\nfi\nYARN_OPTS=\"$YARN_OPTS -Dyarn.policy.file=$YARN_POLICYFILE\"\n\n{% if rm_security_opts is defined %}\nYARN_OPTS=\"{{rm_security_opts}} $YARN_OPTS\"\n{% endif %}",
            "is_supported_yarn_ranger" : "true",
            "min_user_id" : "1000",
            "nodemanager_heapsize" : "1024",
            "resourcemanager_heapsize" : "1024",
            "service_check.queue.name" : "default",
            "yarn_cgroups_enabled" : "false",
            "yarn_heapsize" : "1024",
            "yarn_log_dir_prefix" : "/var/log/hadoop-yarn",
            "yarn_pid_dir_prefix" : "/var/run/hadoop-yarn",
            "yarn_user" : "yarn",
            "yarn_user_nofile_limit" : "32768",
            "yarn_user_nproc_limit" : "65536"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-yarn-security",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ranger.plugin.yarn.policy.cache.dir" : "/etc/ranger/{{repo_name}}/policycache",
            "ranger.plugin.yarn.policy.pollIntervalMs" : "30000",
            "ranger.plugin.yarn.policy.rest.ssl.config.file" : "/etc/hadoop/conf/ranger-policymgr-ssl-yarn.xml",
            "ranger.plugin.yarn.policy.rest.url" : "{{policymgr_mgr_url}}",
            "ranger.plugin.yarn.policy.source.impl" : "org.apache.ranger.admin.client.RangerAdminRESTClient",
            "ranger.plugin.yarn.service.name" : "{{repo_name}}"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "yarn-site",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "hadoop.registry.rm.enabled" : "false",
            "hadoop.registry.zk.quorum" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "hadoop.security.instrumentation.requires.admin" : "false",
            "manage.include.files" : "false",
            "yarn.acl.enable" : "false",
            "yarn.admin.acl" : "",
            "yarn.app.mapreduce.am.create-intermediate-jh-base-dir" : "true",
            "yarn.app.mapreduce.am.job.recovery.enable" : "true",
            "yarn.application.classpath" : "$HADOOP_CONF_DIR,/usr/hdp/current/hadoop-client/*,/usr/hdp/current/hadoop-client/lib/*,/usr/hdp/current/hadoop-hdfs-client/*,/usr/hdp/current/hadoop-hdfs-client/lib/*,/usr/hdp/current/hadoop-yarn-client/*,/usr/hdp/current/hadoop-yarn-client/lib/*",
            "yarn.client.failover-proxy-provider" : "org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider",
            "yarn.client.nodemanager-connect.max-wait-ms" : "60000",
            "yarn.client.nodemanager-connect.retry-interval-ms" : "10000",
            "yarn.http.policy" : "HTTP_ONLY",
            "yarn.log-aggregation-enable" : "true",
            "yarn.log-aggregation.file-controller.IndexedFormat.class" : "org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController",
            "yarn.log-aggregation.file-controller.TFile.class" : "org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController",
            "yarn.log-aggregation.file-formats" : "IndexedFormat,TFile",
            "yarn.log-aggregation.retain-seconds" : "604800",
            "yarn.log.server.url" : "http://hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:19888/jobhistory/logs",
            "yarn.log.server.web-service.url" : "http://headnodehost:8188/ws/v1/applicationhistory",
            "yarn.node-labels.enabled" : "false",
            "yarn.node-labels.fs-store.retry-policy-spec" : "2000, 500",
            "yarn.node-labels.fs-store.root-dir" : "/system/yarn/node-labels",
            "yarn.nodemanager.address" : "0.0.0.0:30050",
            "yarn.nodemanager.admin-env" : "MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX",
            "yarn.nodemanager.aux-services" : "mapreduce_shuffle,spark_shuffle,spark2_shuffle",
            "yarn.nodemanager.aux-services.mapreduce_shuffle.class" : "org.apache.hadoop.mapred.ShuffleHandler",
            "yarn.nodemanager.aux-services.spark2_shuffle.class" : "org.apache.spark.network.yarn.YarnShuffleService",
            "yarn.nodemanager.aux-services.spark2_shuffle.classpath" : "{{stack_root}}/${hdp.version}/spark2/aux/*",
            "yarn.nodemanager.aux-services.spark_shuffle.class" : "org.apache.spark.network.yarn.YarnShuffleService",
            "yarn.nodemanager.aux-services.spark_shuffle.classpath" : "{{stack_root}}/${hdp.version}/spark/aux/*",
            "yarn.nodemanager.bind-host" : "0.0.0.0",
            "yarn.nodemanager.container-executor.class" : "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor",
            "yarn.nodemanager.container-metrics.unregister-delay-ms" : "60000",
            "yarn.nodemanager.container-monitor.interval-ms" : "3000",
            "yarn.nodemanager.delete.debug-delay-sec" : "600",
            "yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage" : "90",
            "yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb" : "1000",
            "yarn.nodemanager.disk-health-checker.min-healthy-disks" : "0.25",
            "yarn.nodemanager.health-checker.interval-ms" : "135000",
            "yarn.nodemanager.health-checker.script.timeout-ms" : "60000",
            "yarn.nodemanager.kill-escape.launch-command-line" : "slider-agent,LLAP",
            "yarn.nodemanager.kill-escape.user" : "hive",
            "yarn.nodemanager.linux-container-executor.cgroups.hierarchy" : "hadoop-yarn",
            "yarn.nodemanager.linux-container-executor.cgroups.mount" : "false",
            "yarn.nodemanager.linux-container-executor.cgroups.mount-path" : "/cgroup",
            "yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage" : "false",
            "yarn.nodemanager.linux-container-executor.group" : "hadoop",
            "yarn.nodemanager.linux-container-executor.resources-handler.class" : "org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler",
            "yarn.nodemanager.local-dirs" : "/mnt/resource/hadoop/yarn/local",
            "yarn.nodemanager.log-aggregation.compression-type" : "gz",
            "yarn.nodemanager.log-aggregation.debug-enabled" : "false",
            "yarn.nodemanager.log-aggregation.num-log-files-per-app" : "336",
            "yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds" : "3600",
            "yarn.nodemanager.log-dirs" : "/mnt/resource/hadoop/yarn/log",
            "yarn.nodemanager.log.retain-seconds" : "604800",
            "yarn.nodemanager.pmem-check-enabled" : "false",
            "yarn.nodemanager.recovery.dir" : "{{yarn_log_dir_prefix}}/nodemanager/recovery-state",
            "yarn.nodemanager.recovery.enabled" : "true",
            "yarn.nodemanager.remote-app-log-dir" : "/app-logs",
            "yarn.nodemanager.remote-app-log-dir-suffix" : "logs",
            "yarn.nodemanager.resource.cpu-vcores" : "7",
            "yarn.nodemanager.resource.memory-mb" : "12288",
            "yarn.nodemanager.resource.percentage-physical-cpu-limit" : "80",
            "yarn.nodemanager.vmem-check-enabled" : "false",
            "yarn.nodemanager.vmem-pmem-ratio" : "2.1",
            "yarn.nodemanager.webapp.address" : "0.0.0.0:30060",
            "yarn.resourcemanager.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8050",
            "yarn.resourcemanager.address.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8050",
            "yarn.resourcemanager.address.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8050",
            "yarn.resourcemanager.admin.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8141",
            "yarn.resourcemanager.admin.address.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8141",
            "yarn.resourcemanager.admin.address.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8141",
            "yarn.resourcemanager.am.max-attempts" : "5",
            "yarn.resourcemanager.bind-host" : "0.0.0.0",
            "yarn.resourcemanager.cluster-id" : "yarn-cluster",
            "yarn.resourcemanager.connect.max-wait.ms" : "7200000",
            "yarn.resourcemanager.connect.retry-interval.ms" : "30000",
            "yarn.resourcemanager.fs.state-store.retry-policy-spec" : "2000, 500",
            "yarn.resourcemanager.fs.state-store.uri" : " ",
            "yarn.resourcemanager.ha.automatic-failover.zk-base-path" : "/yarn-leader-election",
            "yarn.resourcemanager.ha.enabled" : "true",
            "yarn.resourcemanager.ha.rm-ids" : "rm1,rm2",
            "yarn.resourcemanager.hostname" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
            "yarn.resourcemanager.hostname.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
            "yarn.resourcemanager.hostname.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
            "yarn.resourcemanager.max-completed-applications" : "1000",
            "yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill" : "5000",
            "yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor" : "1",
            "yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round" : "0.1",
            "yarn.resourcemanager.nodes.exclude-path" : "/etc/hadoop/conf/yarn.exclude",
            "yarn.resourcemanager.recovery.enabled" : "true",
            "yarn.resourcemanager.resource-tracker.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8025",
            "yarn.resourcemanager.resource-tracker.address.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8025",
            "yarn.resourcemanager.resource-tracker.address.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8025",
            "yarn.resourcemanager.scheduler.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8030",
            "yarn.resourcemanager.scheduler.address.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8030",
            "yarn.resourcemanager.scheduler.address.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8030",
            "yarn.resourcemanager.scheduler.class" : "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
            "yarn.resourcemanager.scheduler.monitor.enable" : "true",
            "yarn.resourcemanager.scheduler.monitor.policies" : "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy",
            "yarn.resourcemanager.state-store.max-completed-applications" : "${yarn.resourcemanager.max-completed-applications}",
            "yarn.resourcemanager.store.class" : "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore",
            "yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size" : "10",
            "yarn.resourcemanager.system-metrics-publisher.enabled" : "true",
            "yarn.resourcemanager.webapp.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8088",
            "yarn.resourcemanager.webapp.address.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8088",
            "yarn.resourcemanager.webapp.address.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8088",
            "yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled" : "false",
            "yarn.resourcemanager.webapp.https.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8090",
            "yarn.resourcemanager.work-preserving-recovery.enabled" : "true",
            "yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms" : "10000",
            "yarn.resourcemanager.zk-acl" : "world:anyone:rwcda",
            "yarn.resourcemanager.zk-address" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "yarn.resourcemanager.zk-num-retries" : "1000",
            "yarn.resourcemanager.zk-retry-interval-ms" : "1000",
            "yarn.resourcemanager.zk-state-store.parent-path" : "/rmstore",
            "yarn.resourcemanager.zk-timeout-ms" : "10000",
            "yarn.scheduler.capacity.ordering-policy.priority-utilization.underutilized-preemption.enabled" : "false",
            "yarn.scheduler.maximum-allocation-mb" : "12288",
            "yarn.scheduler.maximum-allocation-vcores" : "7",
            "yarn.scheduler.minimum-allocation-mb" : "512",
            "yarn.scheduler.minimum-allocation-vcores" : "1",
            "yarn.timeline-service.address" : "headnodehost:10200",
            "yarn.timeline-service.bind-host" : "0.0.0.0",
            "yarn.timeline-service.client.best-effort" : "true",
            "yarn.timeline-service.client.fd-flush-interval-secs" : "5",
            "yarn.timeline-service.client.fd-retain-secs" : "90",
            "yarn.timeline-service.client.internal-timers-ttl-secs" : "150",
            "yarn.timeline-service.client.max-retries" : "30",
            "yarn.timeline-service.client.retry-interval-ms" : "1000",
            "yarn.timeline-service.enabled" : "true",
            "yarn.timeline-service.entity-file.fs-support-append" : "false",
            "yarn.timeline-service.entity-group-fs-store.active-dir" : "/atshistory/active",
            "yarn.timeline-service.entity-group-fs-store.app-cache-size" : "10",
            "yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds" : "3600",
            "yarn.timeline-service.entity-group-fs-store.done-dir" : "/atshistory/done",
            "yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes" : "org.apache.spark.deploy.history.yarn.plugin.SparkATSPlugin,org.apache.tez.dag.history.logging.ats.TimelineCachePluginImpl",
            "yarn.timeline-service.entity-group-fs-store.group-id-plugin-classpath" : "/usr/hdp/current/spark-client/hdpLib/*",
            "yarn.timeline-service.entity-group-fs-store.retain-seconds" : "604800",
            "yarn.timeline-service.entity-group-fs-store.scan-interval-seconds" : "15",
            "yarn.timeline-service.entity-group-fs-store.summary-store" : "org.apache.hadoop.yarn.server.timeline.SqlDBTimelineStore",
            "yarn.timeline-service.generic-application-history.hdinsight.filter-container-meta-info" : "true",
            "yarn.timeline-service.generic-application-history.save-non-am-container-meta-info" : "true",
            "yarn.timeline-service.generic-application-history.store-class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore",
            "yarn.timeline-service.http-authentication.simple.anonymous.allowed" : "true",
            "yarn.timeline-service.http-authentication.type" : "simple",
            "yarn.timeline-service.leveldb-state-store.path" : "/hadoop/yarn/timeline",
            "yarn.timeline-service.leveldb-timeline-store.path" : "/hadoop/yarn/timeline",
            "yarn.timeline-service.leveldb-timeline-store.read-cache-size" : "104857600",
            "yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size" : "10000",
            "yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size" : "10000",
            "yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms" : "43200000",
            "yarn.timeline-service.recovery.enabled" : "true",
            "yarn.timeline-service.sqldb-store.connection-password" : "G0oMKiWiTYsBndEypk9hrFVlbLsAI9eCnnnoPRbGW3KQk6NOYA39oNawNAjpaHEBnCe4DrpQKYPwfJZW4hIMbLphTb11XfKWSUAoMVCTIx5NDx",
            "yarn.timeline-service.sqldb-store.connection-url" : "jdbc:sqlserver://tonvo0x9dg.database.windows.net;databaseName=v368131f056aa2d44e882781a655de2485eAmbariDb;sendStringParametersAsUnicode=false;trustServerCertificate=false;encrypt=true;hostNameInCertificate=*.database.windows.net;",
            "yarn.timeline-service.sqldb-store.connection-username" : "v368131f056aa2d44e882781a655de2485eAmbariDbLogin@tonvo0x9dg.database.windows.net",
            "yarn.timeline-service.sqldb-store.driver" : "com.microsoft.sqlserver.jdbc.SQLServerDriver",
            "yarn.timeline-service.state-store-class" : "org.apache.hadoop.yarn.server.timeline.recovery.SqlDBTimelineStateStore",
            "yarn.timeline-service.store-class" : "org.apache.hadoop.yarn.server.timeline.HdInsightEntityGroupFSTimelineStore",
            "yarn.timeline-service.ttl-enable" : "true",
            "yarn.timeline-service.ttl-ms" : "604800000",
            "yarn.timeline-service.version" : "1.5",
            "yarn.timeline-service.webapp.address" : "headnodehost:8188",
            "yarn.timeline-service.webapp.https.address" : "headnodehost:8190"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "yarn-log4j",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n#Relative to Yarn Log Dir Prefix\nyarn.log.dir=.\n#\n# Job Summary Appender\n#\n# Use following logger to send summary to separate file defined by\n# hadoop.mapreduce.jobsummary.log.file rolled daily:\n# hadoop.mapreduce.jobsummary.logger=INFO,JSA\n#\nhadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}\nhadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log\nlog4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender\n# Set the ResourceManager summary log filename\nyarn.server.resourcemanager.appsummary.log.file=hadoop-mapreduce.jobsummary.log\n# Set the ResourceManager summary log level and appender\nyarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}\n#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY\n\n# To enable AppSummaryLogging for the RM,\n# set yarn.server.resourcemanager.appsummary.logger to\n# LEVEL,RMSUMMARY in hadoop-env.sh\n\n# Appender for ResourceManager Application Summary Log\n# Requires the following properties to be set\n#    - hadoop.log.dir (Hadoop Log directory)\n#    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)\n#    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)\nlog4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender\nlog4j.appender.RMSUMMARY.File=${yarn.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}\nlog4j.appender.RMSUMMARY.MaxFileSize={{yarn_rm_summary_log_max_backup_size}}MB\nlog4j.appender.RMSUMMARY.MaxBackupIndex={{yarn_rm_summary_log_number_of_backup_files}}\nlog4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.JSA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\nlog4j.appender.JSA.DatePattern=.yyyy-MM-dd\nlog4j.appender.JSA.layout=org.apache.log4j.PatternLayout\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false\n\n# Appender for viewing information for errors and warnings\nyarn.ewma.cleanupInterval=300\nyarn.ewma.messageAgeLimitSeconds=86400\nyarn.ewma.maxUniqueMessages=250\nlog4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender\nlog4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}\nlog4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}\nlog4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}\n\n# Audit logging for ResourceManager\nrm.audit.logger=${hadoop.root.logger}\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger=${rm.audit.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger=false\nlog4j.appender.RMAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.RMAUDIT.File=${yarn.log.dir}/rm-audit.log\nlog4j.appender.RMAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RMAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.RMAUDIT.DatePattern=.yyyy-MM-dd\n\n# Audit logging for NodeManager\nnm.audit.logger=${hadoop.root.logger}\nlog4j.logger.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger=${nm.audit.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger=false\nlog4j.appender.NMAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.NMAUDIT.File=${yarn.log.dir}/nm-audit.log\nlog4j.appender.NMAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.NMAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.NMAUDIT.DatePattern=.yyyy-MM-dd\n    ",
            "yarn_rm_summary_log_max_backup_size" : "256",
            "yarn_rm_summary_log_number_of_backup_files" : "20"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-yarn-audit",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "ranger.plugin.yarn.ambari.cluster.name" : "{{cluster_name}}",
            "xasecure.audit.destination.hdfs" : "true",
            "xasecure.audit.destination.hdfs.batch.filespool.dir" : "/var/log/hadoop/yarn/audit/hdfs/spool",
            "xasecure.audit.destination.hdfs.dir" : "hdfs://NAMENODE_HOSTNAME:8020/ranger/audit",
            "xasecure.audit.destination.solr" : "false",
            "xasecure.audit.destination.solr.batch.filespool.dir" : "/var/log/hadoop/yarn/audit/solr/spool",
            "xasecure.audit.destination.solr.urls" : "",
            "xasecure.audit.destination.solr.zookeepers" : "NONE",
            "xasecure.audit.is.enabled" : "true",
            "xasecure.audit.provider.summary.enabled" : "false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-yarn-policymgr-ssl",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "xasecure.policymgr.clientssl.keystore" : "/usr/hdp/current/hadoop-client/conf/ranger-yarn-plugin-keystore.jks",
            "xasecure.policymgr.clientssl.keystore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.keystore.password" : "SECRET:ranger-yarn-policymgr-ssl:1:xasecure.policymgr.clientssl.keystore.password",
            "xasecure.policymgr.clientssl.truststore" : "/usr/hdp/current/hadoop-client/conf/ranger-yarn-plugin-truststore.jks",
            "xasecure.policymgr.clientssl.truststore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.truststore.password" : "SECRET:ranger-yarn-policymgr-ssl:1:xasecure.policymgr.clientssl.truststore.password"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "yarn-logsearch-conf",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "component_mappings" : "RESOURCEMANAGER:yarn_resourcemanager,yarn_historyserver,yarn_jobsummary;NODEMANAGER:yarn_nodemanager;APP_TIMELINE_SERVER:yarn_timelineserver",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"yarn_nodemanager\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/yarn-env/yarn_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/yarn-env/yarn_user', 'yarn')}}/yarn-{{default('configurations/yarn-env/yarn_user', 'yarn')}}-nodemanager-*.log\"\n    },\n    {\n      \"type\":\"yarn_resourcemanager\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/yarn-env/yarn_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/yarn-env/yarn_user', 'yarn')}}/yarn-{{default('configurations/yarn-env/yarn_user', 'yarn')}}-resourcemanager-*.log\"\n    },\n    {\n      \"type\":\"yarn_timelineserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/yarn-env/yarn_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/yarn-env/yarn_user', 'yarn')}}/yarn-{{default('configurations/yarn-env/yarn_user', 'yarn')}}-timelineserver-*.log\"\n    },\n    {\n      \"type\":\"yarn_historyserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/yarn-env/yarn_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/yarn-env/yarn_user', 'yarn')}}/yarn-{{default('configurations/yarn-env/yarn_user', 'yarn')}}-historyserver-*.log\"\n    },\n    {\n      \"type\":\"yarn_jobsummary\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/yarn-env/yarn_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/yarn-env/yarn_user', 'yarn')}}/hadoop-mapreduce.jobsummary.log\"\n    }\n   ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"yarn_historyserver\",\n            \"yarn_jobsummary\",\n            \"yarn_nodemanager\",\n            \"yarn_resourcemanager\",\n            \"yarn_timelineserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\\\(%{JAVAFILE:file}:%{JAVAMETHOD:method}\\\\(%{INT:line_number}\\\\)\\\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     }\n   ]\n}\n    ",
            "service_name" : "YARN"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553522876,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "YARN",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=YARN&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-yarn-plugin-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "REPOSITORY_CONFIG_PASSWORD" : "SECRET:ranger-yarn-plugin-properties:2:REPOSITORY_CONFIG_PASSWORD",
            "REPOSITORY_CONFIG_USERNAME" : "yarn",
            "common.name.for.certificate" : "",
            "external_admin_password" : "",
            "external_admin_username" : "",
            "external_ranger_admin_password" : "",
            "external_ranger_admin_username" : "",
            "hadoop.rpc.protection" : "",
            "policy_user" : "ambari-qa",
            "ranger-yarn-plugin-enabled" : "No"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "capacity-scheduler",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "yarn.scheduler.capacity.default.minimum-user-limit-percent" : "100",
            "yarn.scheduler.capacity.maximum-am-resource-percent" : "0.33",
            "yarn.scheduler.capacity.maximum-applications" : "10000",
            "yarn.scheduler.capacity.node-locality-delay" : "0",
            "yarn.scheduler.capacity.resource-calculator" : "org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator",
            "yarn.scheduler.capacity.root.accessible-node-labels" : "*",
            "yarn.scheduler.capacity.root.acl_administer_queue" : "*",
            "yarn.scheduler.capacity.root.capacity" : "100",
            "yarn.scheduler.capacity.root.default.acl_administer_jobs" : "*",
            "yarn.scheduler.capacity.root.default.acl_submit_applications" : "*",
            "yarn.scheduler.capacity.root.default.capacity" : "50",
            "yarn.scheduler.capacity.root.default.maximum-capacity" : "100",
            "yarn.scheduler.capacity.root.default.state" : "RUNNING",
            "yarn.scheduler.capacity.root.default.user-limit-factor" : "10",
            "yarn.scheduler.capacity.root.queues" : "default,thriftsvr",
            "yarn.scheduler.capacity.root.thriftsvr.capacity" : "50",
            "yarn.scheduler.capacity.root.thriftsvr.maximum-capacity" : "100",
            "yarn.scheduler.capacity.root.thriftsvr.user-limit-factor" : "10"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "yarn-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "apptimelineserver_heapsize" : "1024",
            "content" : "\nexport HADOOP_YARN_HOME={{hadoop_yarn_home}}\nexport YARN_LOG_DIR={{yarn_log_dir_prefix}}/$USER\nexport YARN_PID_DIR={{yarn_pid_dir_prefix}}/$USER\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\nexport JAVA_HOME={{java64_home}}\n\n# User for YARN daemons\nexport HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn}\n\n# resolve links - $0 may be a softlink\nexport YARN_CONF_DIR=\"${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}\"\n\n# some Java parameters\n# export JAVA_HOME=/home/y/libexec/jdk1.6.0/\nif [ \"$JAVA_HOME\" != \"\" ]; then\n  #echo \"run java in $JAVA_HOME\"\n  JAVA_HOME=$JAVA_HOME\nfi\n\nif [ \"$JAVA_HOME\" = \"\" ]; then\n  echo \"Error: JAVA_HOME is not set.\"\n  exit 1\nfi\n\nJAVA=$JAVA_HOME/bin/java\nJAVA_HEAP_MAX=-Xmx1000m\n\n# For setting YARN specific HEAP sizes please use this\n# Parameter and set appropriately\nYARN_HEAPSIZE={{yarn_heapsize}}\n\n# check envvars which might override default args\nif [ \"$YARN_HEAPSIZE\" != \"\" ]; then\n  JAVA_HEAP_MAX=\"-Xmx\"\"$YARN_HEAPSIZE\"\"m\"\nfi\n\n# Resource Manager specific parameters\n\n# Specify the max Heapsize for the ResourceManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1000.\n# This value will be overridden by an Xmx setting specified in either YARN_OPTS\n# and/or YARN_RESOURCEMANAGER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_RESOURCEMANAGER_HEAPSIZE={{resourcemanager_heapsize}}\n\n# Specify the JVM options to be used when starting the ResourceManager.\n# These options will be appended to the options specified as YARN_OPTS\n# and therefore may override any similar flags set in YARN_OPTS\n#export YARN_RESOURCEMANAGER_OPTS=\n\n# Node Manager specific parameters\n\n# Specify the max Heapsize for the NodeManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1000.\n# This value will be overridden by an Xmx setting specified in either YARN_OPTS\n# and/or YARN_NODEMANAGER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_NODEMANAGER_HEAPSIZE={{nodemanager_heapsize}}\n\n# Specify the max Heapsize for the HistoryManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1024.\n# This value will be overridden by an Xmx setting specified in either YARN_OPTS\n# and/or YARN_HISTORYSERVER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_HISTORYSERVER_HEAPSIZE={{apptimelineserver_heapsize}}\nexport YARN_TIMELINESERVER_HEAPSIZE={{apptimelineserver_heapsize}}\nexport YARN_TIMELINESERVER_OPTS=\"-Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=apptimelineserver\"\n\n# Specify the JVM options to be used when starting the NodeManager.\n# These options will be appended to the options specified as YARN_OPTS\n# and therefore may override any similar flags set in YARN_OPTS\nexport YARN_NODEMANAGER_OPTS=\"-Dwhitelist.filename=core-whitelist.res,coremanual-whitelist.res -Dcomponent=nodemanager\"\n\n# so that filenames w/ spaces are handled correctly in loops below\nIFS=\n\n\n# default log directory and file\nif [ \"$YARN_LOG_DIR\" = \"\" ]; then\n  YARN_LOG_DIR=\"$HADOOP_YARN_HOME/logs\"\nfi\nif [ \"$YARN_LOGFILE\" = \"\" ]; then\n  YARN_LOGFILE='yarn.log'\nfi\n\n# default policy file for service-level authorization\nif [ \"$YARN_POLICYFILE\" = \"\" ]; then\n  YARN_POLICYFILE=\"hadoop-policy.xml\"\nfi\n\n# restore ordinary behaviour\nunset IFS\n\n\nYARN_OPTS=\"$YARN_OPTS -Dhadoop.log.dir=$YARN_LOG_DIR\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.log.dir=$YARN_LOG_DIR\"\nYARN_OPTS=\"$YARN_OPTS -Dhadoop.log.file=$YARN_LOGFILE\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.log.file=$YARN_LOGFILE\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.home.dir=$YARN_COMMON_HOME\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.id.str=$YARN_IDENT_STRING\"\nYARN_OPTS=\"$YARN_OPTS -Dhadoop.root.logger=${YARN_ROOT_LOGGER:-INFO,console}\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.root.logger=${YARN_ROOT_LOGGER:-INFO,console}\"\nif [ \"x$JAVA_LIBRARY_PATH\" != \"x\" ]; then\n  YARN_OPTS=\"$YARN_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH\"\nfi\nYARN_OPTS=\"$YARN_OPTS -Dyarn.policy.file=$YARN_POLICYFILE\"\n\n{% if rm_security_opts is defined %}\nYARN_OPTS=\"{{rm_security_opts}} $YARN_OPTS\"\n{% endif %}",
            "is_supported_yarn_ranger" : "true",
            "min_user_id" : "1000",
            "nodemanager_heapsize" : "1024",
            "resourcemanager_heapsize" : "1024",
            "service_check.queue.name" : "default",
            "yarn_cgroups_enabled" : "false",
            "yarn_heapsize" : "1024",
            "yarn_log_dir_prefix" : "/var/log/hadoop-yarn",
            "yarn_pid_dir_prefix" : "/var/run/hadoop-yarn",
            "yarn_user" : "yarn",
            "yarn_user_nofile_limit" : "32768",
            "yarn_user_nproc_limit" : "65536"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-yarn-security",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ranger.plugin.yarn.policy.cache.dir" : "/etc/ranger/{{repo_name}}/policycache",
            "ranger.plugin.yarn.policy.pollIntervalMs" : "30000",
            "ranger.plugin.yarn.policy.rest.ssl.config.file" : "/etc/hadoop/conf/ranger-policymgr-ssl-yarn.xml",
            "ranger.plugin.yarn.policy.rest.url" : "{{policymgr_mgr_url}}",
            "ranger.plugin.yarn.policy.source.impl" : "org.apache.ranger.admin.client.RangerAdminRESTClient",
            "ranger.plugin.yarn.service.name" : "{{repo_name}}"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "yarn-site",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "hadoop.registry.rm.enabled" : "false",
            "hadoop.registry.zk.quorum" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "hadoop.security.instrumentation.requires.admin" : "false",
            "manage.include.files" : "false",
            "yarn.acl.enable" : "false",
            "yarn.admin.acl" : "",
            "yarn.app.mapreduce.am.create-intermediate-jh-base-dir" : "true",
            "yarn.app.mapreduce.am.job.recovery.enable" : "true",
            "yarn.application.classpath" : "$HADOOP_CONF_DIR,/usr/hdp/current/hadoop-client/*,/usr/hdp/current/hadoop-client/lib/*,/usr/hdp/current/hadoop-hdfs-client/*,/usr/hdp/current/hadoop-hdfs-client/lib/*,/usr/hdp/current/hadoop-yarn-client/*,/usr/hdp/current/hadoop-yarn-client/lib/*",
            "yarn.client.failover-proxy-provider" : "org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider",
            "yarn.client.nodemanager-connect.max-wait-ms" : "60000",
            "yarn.client.nodemanager-connect.retry-interval-ms" : "10000",
            "yarn.http.policy" : "HTTP_ONLY",
            "yarn.log-aggregation-enable" : "true",
            "yarn.log-aggregation.file-controller.IndexedFormat.class" : "org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController",
            "yarn.log-aggregation.file-controller.TFile.class" : "org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController",
            "yarn.log-aggregation.file-formats" : "IndexedFormat,TFile",
            "yarn.log-aggregation.retain-seconds" : "604800",
            "yarn.log.server.url" : "http://hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:19888/jobhistory/logs",
            "yarn.log.server.web-service.url" : "http://headnodehost:8188/ws/v1/applicationhistory",
            "yarn.node-labels.enabled" : "false",
            "yarn.node-labels.fs-store.retry-policy-spec" : "2000, 500",
            "yarn.node-labels.fs-store.root-dir" : "/system/yarn/node-labels",
            "yarn.nodemanager.address" : "0.0.0.0:30050",
            "yarn.nodemanager.admin-env" : "MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX",
            "yarn.nodemanager.aux-services" : "mapreduce_shuffle,spark_shuffle,spark2_shuffle",
            "yarn.nodemanager.aux-services.mapreduce_shuffle.class" : "org.apache.hadoop.mapred.ShuffleHandler",
            "yarn.nodemanager.aux-services.spark2_shuffle.class" : "org.apache.spark.network.yarn.YarnShuffleService",
            "yarn.nodemanager.aux-services.spark2_shuffle.classpath" : "{{stack_root}}/${hdp.version}/spark2/aux/*",
            "yarn.nodemanager.aux-services.spark_shuffle.class" : "org.apache.spark.network.yarn.YarnShuffleService",
            "yarn.nodemanager.aux-services.spark_shuffle.classpath" : "{{stack_root}}/${hdp.version}/spark/aux/*",
            "yarn.nodemanager.bind-host" : "0.0.0.0",
            "yarn.nodemanager.container-executor.class" : "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor",
            "yarn.nodemanager.container-metrics.unregister-delay-ms" : "60000",
            "yarn.nodemanager.container-monitor.interval-ms" : "3000",
            "yarn.nodemanager.delete.debug-delay-sec" : "600",
            "yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage" : "90",
            "yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb" : "1000",
            "yarn.nodemanager.disk-health-checker.min-healthy-disks" : "0.25",
            "yarn.nodemanager.health-checker.interval-ms" : "135000",
            "yarn.nodemanager.health-checker.script.timeout-ms" : "60000",
            "yarn.nodemanager.kill-escape.launch-command-line" : "slider-agent,LLAP",
            "yarn.nodemanager.kill-escape.user" : "hive",
            "yarn.nodemanager.linux-container-executor.cgroups.hierarchy" : "hadoop-yarn",
            "yarn.nodemanager.linux-container-executor.cgroups.mount" : "false",
            "yarn.nodemanager.linux-container-executor.cgroups.mount-path" : "/cgroup",
            "yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage" : "false",
            "yarn.nodemanager.linux-container-executor.group" : "hadoop",
            "yarn.nodemanager.linux-container-executor.resources-handler.class" : "org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler",
            "yarn.nodemanager.local-dirs" : "/mnt/resource/hadoop/yarn/local",
            "yarn.nodemanager.log-aggregation.compression-type" : "gz",
            "yarn.nodemanager.log-aggregation.debug-enabled" : "false",
            "yarn.nodemanager.log-aggregation.num-log-files-per-app" : "336",
            "yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds" : "3600",
            "yarn.nodemanager.log-dirs" : "/mnt/resource/hadoop/yarn/log",
            "yarn.nodemanager.log.retain-seconds" : "604800",
            "yarn.nodemanager.pmem-check-enabled" : "false",
            "yarn.nodemanager.recovery.dir" : "{{yarn_log_dir_prefix}}/nodemanager/recovery-state",
            "yarn.nodemanager.recovery.enabled" : "true",
            "yarn.nodemanager.remote-app-log-dir" : "/app-logs",
            "yarn.nodemanager.remote-app-log-dir-suffix" : "logs",
            "yarn.nodemanager.resource.cpu-vcores" : "7",
            "yarn.nodemanager.resource.memory-mb" : "12288",
            "yarn.nodemanager.resource.percentage-physical-cpu-limit" : "80",
            "yarn.nodemanager.vmem-check-enabled" : "false",
            "yarn.nodemanager.vmem-pmem-ratio" : "2.1",
            "yarn.nodemanager.webapp.address" : "0.0.0.0:30060",
            "yarn.resourcemanager.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8050",
            "yarn.resourcemanager.address.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8050",
            "yarn.resourcemanager.address.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8050",
            "yarn.resourcemanager.admin.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8141",
            "yarn.resourcemanager.admin.address.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8141",
            "yarn.resourcemanager.admin.address.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8141",
            "yarn.resourcemanager.am.max-attempts" : "5",
            "yarn.resourcemanager.bind-host" : "0.0.0.0",
            "yarn.resourcemanager.cluster-id" : "yarn-cluster",
            "yarn.resourcemanager.connect.max-wait.ms" : "7200000",
            "yarn.resourcemanager.connect.retry-interval.ms" : "30000",
            "yarn.resourcemanager.fs.state-store.retry-policy-spec" : "2000, 500",
            "yarn.resourcemanager.fs.state-store.uri" : " ",
            "yarn.resourcemanager.ha.automatic-failover.zk-base-path" : "/yarn-leader-election",
            "yarn.resourcemanager.ha.enabled" : "true",
            "yarn.resourcemanager.ha.rm-ids" : "rm1,rm2",
            "yarn.resourcemanager.hostname" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
            "yarn.resourcemanager.hostname.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
            "yarn.resourcemanager.hostname.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net",
            "yarn.resourcemanager.max-completed-applications" : "1000",
            "yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill" : "5000",
            "yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor" : "1",
            "yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round" : "0.1",
            "yarn.resourcemanager.nodes.exclude-path" : "/etc/hadoop/conf/yarn.exclude",
            "yarn.resourcemanager.recovery.enabled" : "true",
            "yarn.resourcemanager.resource-tracker.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8025",
            "yarn.resourcemanager.resource-tracker.address.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8025",
            "yarn.resourcemanager.resource-tracker.address.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8025",
            "yarn.resourcemanager.scheduler.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8030",
            "yarn.resourcemanager.scheduler.address.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8030",
            "yarn.resourcemanager.scheduler.address.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8030",
            "yarn.resourcemanager.scheduler.class" : "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
            "yarn.resourcemanager.scheduler.monitor.enable" : "true",
            "yarn.resourcemanager.scheduler.monitor.policies" : "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy",
            "yarn.resourcemanager.state-store.max-completed-applications" : "${yarn.resourcemanager.max-completed-applications}",
            "yarn.resourcemanager.store.class" : "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore",
            "yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size" : "10",
            "yarn.resourcemanager.system-metrics-publisher.enabled" : "true",
            "yarn.resourcemanager.webapp.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8088",
            "yarn.resourcemanager.webapp.address.rm1" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8088",
            "yarn.resourcemanager.webapp.address.rm2" : "hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8088",
            "yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled" : "false",
            "yarn.resourcemanager.webapp.https.address" : "hn0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8090",
            "yarn.resourcemanager.work-preserving-recovery.enabled" : "true",
            "yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms" : "10000",
            "yarn.resourcemanager.zk-acl" : "world:anyone:rwcda",
            "yarn.resourcemanager.zk-address" : "zk0-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk2-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181,zk5-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:2181",
            "yarn.resourcemanager.zk-num-retries" : "1000",
            "yarn.resourcemanager.zk-retry-interval-ms" : "1000",
            "yarn.resourcemanager.zk-state-store.parent-path" : "/rmstore",
            "yarn.resourcemanager.zk-timeout-ms" : "10000",
            "yarn.scheduler.capacity.ordering-policy.priority-utilization.underutilized-preemption.enabled" : "false",
            "yarn.scheduler.maximum-allocation-mb" : "12288",
            "yarn.scheduler.maximum-allocation-vcores" : "7",
            "yarn.scheduler.minimum-allocation-mb" : "512",
            "yarn.scheduler.minimum-allocation-vcores" : "1",
            "yarn.timeline-service.address" : "headnodehost:10200",
            "yarn.timeline-service.bind-host" : "0.0.0.0",
            "yarn.timeline-service.client.best-effort" : "true",
            "yarn.timeline-service.client.fd-flush-interval-secs" : "5",
            "yarn.timeline-service.client.fd-retain-secs" : "90",
            "yarn.timeline-service.client.internal-timers-ttl-secs" : "150",
            "yarn.timeline-service.client.max-retries" : "30",
            "yarn.timeline-service.client.retry-interval-ms" : "1000",
            "yarn.timeline-service.enabled" : "true",
            "yarn.timeline-service.entity-file.fs-support-append" : "false",
            "yarn.timeline-service.entity-group-fs-store.active-dir" : "/atshistory/active",
            "yarn.timeline-service.entity-group-fs-store.app-cache-size" : "10",
            "yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds" : "3600",
            "yarn.timeline-service.entity-group-fs-store.done-dir" : "/atshistory/done",
            "yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes" : "org.apache.spark.deploy.history.yarn.plugin.SparkATSPlugin,org.apache.tez.dag.history.logging.ats.TimelineCachePluginImpl",
            "yarn.timeline-service.entity-group-fs-store.group-id-plugin-classpath" : "/usr/hdp/current/spark-client/hdpLib/*",
            "yarn.timeline-service.entity-group-fs-store.retain-seconds" : "604800",
            "yarn.timeline-service.entity-group-fs-store.scan-interval-seconds" : "15",
            "yarn.timeline-service.entity-group-fs-store.summary-store" : "org.apache.hadoop.yarn.server.timeline.SqlDBTimelineStore",
            "yarn.timeline-service.generic-application-history.hdinsight.filter-container-meta-info" : "true",
            "yarn.timeline-service.generic-application-history.save-non-am-container-meta-info" : "true",
            "yarn.timeline-service.generic-application-history.store-class" : "org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore",
            "yarn.timeline-service.http-authentication.simple.anonymous.allowed" : "true",
            "yarn.timeline-service.http-authentication.type" : "simple",
            "yarn.timeline-service.leveldb-state-store.path" : "/hadoop/yarn/timeline",
            "yarn.timeline-service.leveldb-timeline-store.path" : "/hadoop/yarn/timeline",
            "yarn.timeline-service.leveldb-timeline-store.read-cache-size" : "104857600",
            "yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size" : "10000",
            "yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size" : "10000",
            "yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms" : "43200000",
            "yarn.timeline-service.recovery.enabled" : "true",
            "yarn.timeline-service.sqldb-store.connection-password" : "G0oMKiWiTYsBndEypk9hrFVlbLsAI9eCnnnoPRbGW3KQk6NOYA39oNawNAjpaHEBnCe4DrpQKYPwfJZW4hIMbLphTb11XfKWSUAoMVCTIx5NDx",
            "yarn.timeline-service.sqldb-store.connection-url" : "jdbc:sqlserver://tonvo0x9dg.database.windows.net;databaseName=v368131f056aa2d44e882781a655de2485eAmbariDb;sendStringParametersAsUnicode=false;trustServerCertificate=false;encrypt=true;hostNameInCertificate=*.database.windows.net;",
            "yarn.timeline-service.sqldb-store.connection-username" : "v368131f056aa2d44e882781a655de2485eAmbariDbLogin@tonvo0x9dg.database.windows.net",
            "yarn.timeline-service.sqldb-store.driver" : "com.microsoft.sqlserver.jdbc.SQLServerDriver",
            "yarn.timeline-service.state-store-class" : "org.apache.hadoop.yarn.server.timeline.recovery.SqlDBTimelineStateStore",
            "yarn.timeline-service.store-class" : "org.apache.hadoop.yarn.server.timeline.HdInsightEntityGroupFSTimelineStore",
            "yarn.timeline-service.ttl-enable" : "true",
            "yarn.timeline-service.ttl-ms" : "604800000",
            "yarn.timeline-service.version" : "1.5",
            "yarn.timeline-service.webapp.address" : "headnodehost:8188",
            "yarn.timeline-service.webapp.https.address" : "headnodehost:8190"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "yarn-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#Relative to Yarn Log Dir Prefix\nyarn.log.dir=.\n#\n# Job Summary Appender\n#\n# Use following logger to send summary to separate file defined by\n# hadoop.mapreduce.jobsummary.log.file rolled daily:\n# hadoop.mapreduce.jobsummary.logger=INFO,JSA\n#\nhadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}\nhadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log\nlog4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender\n# Set the ResourceManager summary log filename\nyarn.server.resourcemanager.appsummary.log.file=hadoop-mapreduce.jobsummary.log\n# Set the ResourceManager summary log level and appender\nyarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}\n#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY\n\n# To enable AppSummaryLogging for the RM,\n# set yarn.server.resourcemanager.appsummary.logger to\n# LEVEL,RMSUMMARY in hadoop-env.sh\n\n# Appender for ResourceManager Application Summary Log\n# Requires the following properties to be set\n#    - hadoop.log.dir (Hadoop Log directory)\n#    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)\n#    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)\nlog4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender\nlog4j.appender.RMSUMMARY.File=${yarn.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}\nlog4j.appender.RMSUMMARY.MaxFileSize={{yarn_rm_summary_log_max_backup_size}}MB\nlog4j.appender.RMSUMMARY.MaxBackupIndex={{yarn_rm_summary_log_number_of_backup_files}}\nlog4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.JSA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\nlog4j.appender.JSA.DatePattern=.yyyy-MM-dd\nlog4j.appender.JSA.layout=org.apache.log4j.PatternLayout\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false\n\n# Appender for viewing information for errors and warnings\nyarn.ewma.cleanupInterval=300\nyarn.ewma.messageAgeLimitSeconds=86400\nyarn.ewma.maxUniqueMessages=250\nlog4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender\nlog4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}\nlog4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}\nlog4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}\n\n# Audit logging for ResourceManager\nrm.audit.logger=${hadoop.root.logger}\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger=${rm.audit.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger=false\nlog4j.appender.RMAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.RMAUDIT.File=${yarn.log.dir}/rm-audit.log\nlog4j.appender.RMAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RMAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.RMAUDIT.DatePattern=.yyyy-MM-dd\n\n# Audit logging for NodeManager\nnm.audit.logger=${hadoop.root.logger}\nlog4j.logger.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger=${nm.audit.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger=false\nlog4j.appender.NMAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.NMAUDIT.File=${yarn.log.dir}/nm-audit.log\nlog4j.appender.NMAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.NMAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.NMAUDIT.DatePattern=.yyyy-MM-dd",
            "yarn_rm_summary_log_max_backup_size" : "256",
            "yarn_rm_summary_log_number_of_backup_files" : "20"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-yarn-audit",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "ranger.plugin.yarn.ambari.cluster.name" : "{{cluster_name}}",
            "xasecure.audit.destination.hdfs" : "true",
            "xasecure.audit.destination.hdfs.batch.filespool.dir" : "/var/log/hadoop/yarn/audit/hdfs/spool",
            "xasecure.audit.destination.hdfs.dir" : "hdfs://NAMENODE_HOSTNAME:8020/ranger/audit",
            "xasecure.audit.destination.solr" : "false",
            "xasecure.audit.destination.solr.batch.filespool.dir" : "/var/log/hadoop/yarn/audit/solr/spool",
            "xasecure.audit.destination.solr.urls" : "",
            "xasecure.audit.destination.solr.zookeepers" : "NONE",
            "xasecure.audit.is.enabled" : "true",
            "xasecure.audit.provider.summary.enabled" : "false"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "ranger-yarn-policymgr-ssl",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "xasecure.policymgr.clientssl.keystore" : "/usr/hdp/current/hadoop-client/conf/ranger-yarn-plugin-keystore.jks",
            "xasecure.policymgr.clientssl.keystore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.keystore.password" : "SECRET:ranger-yarn-policymgr-ssl:2:xasecure.policymgr.clientssl.keystore.password",
            "xasecure.policymgr.clientssl.truststore" : "/usr/hdp/current/hadoop-client/conf/ranger-yarn-plugin-truststore.jks",
            "xasecure.policymgr.clientssl.truststore.credential.file" : "jceks://file{{credential_file}}",
            "xasecure.policymgr.clientssl.truststore.password" : "SECRET:ranger-yarn-policymgr-ssl:2:xasecure.policymgr.clientssl.truststore.password"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "yarn-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "RESOURCEMANAGER:yarn_resourcemanager,yarn_historyserver,yarn_jobsummary;NODEMANAGER:yarn_nodemanager;APP_TIMELINE_SERVER:yarn_timelineserver",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"yarn_nodemanager\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/yarn-env/yarn_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/yarn-env/yarn_user', 'yarn')}}/yarn-{{default('configurations/yarn-env/yarn_user', 'yarn')}}-nodemanager-*.log\"\n    },\n    {\n      \"type\":\"yarn_resourcemanager\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/yarn-env/yarn_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/yarn-env/yarn_user', 'yarn')}}/yarn-{{default('configurations/yarn-env/yarn_user', 'yarn')}}-resourcemanager-*.log\"\n    },\n    {\n      \"type\":\"yarn_timelineserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/yarn-env/yarn_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/yarn-env/yarn_user', 'yarn')}}/yarn-{{default('configurations/yarn-env/yarn_user', 'yarn')}}-timelineserver-*.log\"\n    },\n    {\n      \"type\":\"yarn_historyserver\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/yarn-env/yarn_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/yarn-env/yarn_user', 'yarn')}}/yarn-{{default('configurations/yarn-env/yarn_user', 'yarn')}}-historyserver-*.log\"\n    },\n    {\n      \"type\":\"yarn_jobsummary\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/yarn-env/yarn_log_dir_prefix', '/var/log/hadoop')}}/{{default('configurations/yarn-env/yarn_user', 'yarn')}}/hadoop-mapreduce.jobsummary.log\"\n    }\n   ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"yarn_historyserver\",\n            \"yarn_jobsummary\",\n            \"yarn_nodemanager\",\n            \"yarn_resourcemanager\",\n            \"yarn_timelineserver\"\n          ]\n         }\n       },\n      \"log4j_format\":\"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\",\n      \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n      \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\\\(%{JAVAFILE:file}:%{JAVAMETHOD:method}\\\\(%{INT:line_number}\\\\)\\\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     }\n   ]\n}",
            "service_name" : "YARN"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553550470,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "YARN",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=ZEPPELIN&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zeppelin-log4j-properties",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "log4j_properties_content" : "\nlog4j.rootLogger = INFO, dailyfile, ETW, Anonymizer, FullPIILogs\nlog4j.appender.stdout = org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout = org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=%5p [%d] ({%t} %F[%M]:%L) - %m%n\nlog4j.appender.dailyfile.DatePattern=.yyyy-MM-dd\nlog4j.appender.dailyfile.Threshold = INFO\nlog4j.appender.dailyfile = org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.dailyfile.File = ${zeppelin.log.file}\nlog4j.appender.dailyfile.layout = org.apache.log4j.PatternLayout\nlog4j.appender.dailyfile.layout.ConversionPattern=%5p [%d] ({%t} %F[%M]:%L) - %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparkzeppelin\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparkzeppelin\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparkzeppelin\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true\n"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zeppelin-config",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "zeppelin.anonymous.allowed" : "true",
            "zeppelin.config.fs.dir" : "file:///etc/zeppelin/conf/",
            "zeppelin.interpreter.config.upgrade" : "true",
            "zeppelin.interpreter.connect.timeout" : "30000",
            "zeppelin.interpreter.dir" : "interpreter",
            "zeppelin.interpreter.group.order" : "livy,md,sh,angular,jdbc",
            "zeppelin.interpreters" : "org.apache.zeppelin.livy.LivySparkInterpreter,org.apache.zeppelin.livy.LivyPySparkInterpreter,org.apache.zeppelin.livy.LivySparkSQLInterpreter,org.apache.zeppelin.markdown.Markdown,org.apache.zeppelin.shell.ShellInterpreter,org.apache.zeppelin.angular.AngularInterpreter,org.apache.zeppelin.jdbc.JDBCInterpreter",
            "zeppelin.notebook.dir" : "notebook",
            "zeppelin.notebook.homescreen" : " ",
            "zeppelin.notebook.homescreen.hide" : "false",
            "zeppelin.notebook.public" : "false",
            "zeppelin.notebook.s3.bucket" : "zeppelin",
            "zeppelin.notebook.s3.user" : "user",
            "zeppelin.notebook.storage" : "org.apache.zeppelin.notebook.repo.VFSNotebookRepo",
            "zeppelin.server.addr" : "0.0.0.0",
            "zeppelin.server.allowed.origins" : "*",
            "zeppelin.server.port" : "9995",
            "zeppelin.server.ssl.port" : "9995",
            "zeppelin.ssl" : "false",
            "zeppelin.ssl.client.auth" : "false",
            "zeppelin.ssl.key.manager.password" : "SECRET:zeppelin-config:1:zeppelin.ssl.key.manager.password",
            "zeppelin.ssl.keystore.password" : "SECRET:zeppelin-config:1:zeppelin.ssl.keystore.password",
            "zeppelin.ssl.keystore.path" : "conf/keystore",
            "zeppelin.ssl.keystore.type" : "JKS",
            "zeppelin.ssl.truststore.password" : "SECRET:zeppelin-config:1:zeppelin.ssl.truststore.password",
            "zeppelin.ssl.truststore.path" : "conf/truststore",
            "zeppelin.ssl.truststore.type" : "JKS",
            "zeppelin.websocket.max.text.message.size" : "1024000"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zeppelin-logsearch-conf",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "component_mappings" : "ZEPPELIN_MASTER:zeppelin",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"zeppelin\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/zeppelin-env/zeppelin_log_dir', '/var/log/zeppelin')}}/zeppelin-zeppelin-*.log\"\n    }\n   ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"zeppelin\"\n          ]\n         }\n       },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{TIMESTAMP_ISO8601:logtime}\\\\])\",\n      \"message_pattern\":\"(?m)^%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{TIMESTAMP_ISO8601:logtime}\\\\]%{SPACE}\\\\(\\\\{{\"{\"}}%{DATA:thread_name}\\\\{{\"}\"}}%{SPACE}%{JAVAFILE:file}\\\\[%{JAVAMETHOD:method}\\\\]:%{INT:line_number}\\\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     }\n   ]\n }\n    ",
            "service_name" : "Zeppelin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zeppelin-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "zeppelin.executor.instances" : "2",
            "zeppelin.executor.mem" : "512m",
            "zeppelin.server.kerberos.keytab" : "",
            "zeppelin.server.kerberos.principal" : "",
            "zeppelin.spark.jar.dir" : "/apps/zeppelin",
            "zeppelin_env_content" : "# export JAVA_HOME=\nexport JAVA_HOME={{java64_home}}\n# export MASTER=  # Spark master url. eg. spark://master_addr:7077. Leave empty if you want to use local mode.\nexport MASTER=yarn-client\nexport SPARK_YARN_JAR={{spark_jar}}\n# export ZEPPELIN_JAVA_OPTS   # Additional jvm options. for example, export ZEPPELIN_JAVA_OPTS=\"-Dspark.executor.memory=8g -Dspark.cores.max=16\"\nexport ZEPPELIN_JAVA_OPTS=\"-Dhdp.version={{full_stack_version}} -Dspark.executor.memory={{executor_mem}} -Dspark.executor.instances={{executor_instances}} -Dspark.yarn.queue={{spark_queue}}\"\n# export ZEPPELIN_MEM   # Zeppelin jvm mem options Default -Xms1024m -Xmx1024m -XX:MaxPermSize=512m\n# export ZEPPELIN_INTP_MEM   # zeppelin interpreter process jvm mem options. Default -Xms1024m -Xmx1024m -XX:MaxPermSize=512m\n# export ZEPPELIN_INTP_JAVA_OPTS  # zeppelin interpreter process jvm options.\n# export ZEPPELIN_SSL_PORT  # ssl port (used when ssl environment variable is set to true)\n\n# export ZEPPELIN_LOG_DIR  # Where log files are stored.  PWD by default.\nexport ZEPPELIN_LOG_DIR={{zeppelin_log_dir}}\n# export ZEPPELIN_PID_DIR  # The pid files are stored. ${ZEPPELIN_HOME}/run by default.\nexport ZEPPELIN_PID_DIR={{zeppelin_pid_dir}}\n# export ZEPPELIN_WAR_TEMPDIR # The location of jetty temporary directory.\n# export ZEPPELIN_NOTEBOOK_DIR # Where notebook saved\n# export ZEPPELIN_NOTEBOOK_HOMESCREEN   # Id of notebook to be displayed in homescreen. ex) 2A94M5J1Z\n# export ZEPPELIN_NOTEBOOK_HOMESCREEN_HIDE  # hide homescreen notebook from list when this value set to \"true\". default \"false\"\n# export ZEPPELIN_NOTEBOOK_S3_BUCKET # Bucket where notebook saved\n# export ZEPPELIN_NOTEBOOK_S3_ENDPOINT  # Endpoint of the bucket\n# export ZEPPELIN_NOTEBOOK_S3_USER  # User in bucket where notebook saved. For example bucket/user/notebook/2A94M5J1Z/note.json\n# export ZEPPELIN_IDENT_STRING   # A string representing this instance of zeppelin. $USER by default.\n# export ZEPPELIN_NICENESS   # The scheduling priority for daemons. Defaults to 0.\n# export ZEPPELIN_INTERPRETER_LOCALREPO   # Local repository for interpreter's additional dependency loading\n# export ZEPPELIN_NOTEBOOK_STORAGE   # Refers to pluggable notebook storage class, can have two classes simultaneously with a sync between them (e.g. local and remote).\n# export ZEPPELIN_NOTEBOOK_ONE_WAY_SYNC  # If there are multiple notebook storages, should we treat the first one as the only source of truth?\n# export ZEPPELIN_NOTEBOOK_PUBLIC  # Make notebook public by default when created, private otherwise\nexport ZEPPELIN_INTP_CLASSPATH_OVERRIDES=\"{{external_dependency_conf}}\"\n\n#### Spark interpreter configuration ####\n\n## Use provided spark installation ##\n## defining SPARK_HOME makes Zeppelin run spark interpreter process using spark-submit\n##\n# export SPARK_HOME   # (required) When it is defined, load it instead of Zeppelin embedded Spark libraries\n#export SPARK_HOME={{spark_home}}\n# export SPARK_SUBMIT_OPTIONS   # (optional) extra options to pass to spark submit. eg) \"--driver-memory 512M --executor-memory 1G\".\n# export SPARK_APP_NAME   # (optional) The name of spark application.\n\n## Use embedded spark binaries ##\n## without SPARK_HOME defined, Zeppelin still able to run spark interpreter process using embedded spark binaries.\n## however, it is not encouraged when you can define SPARK_HOME\n##\n# Options read in YARN client mode\n# export HADOOP_CONF_DIR    # yarn-site.xml is located in configuration directory in HADOOP_CONF_DIR.\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\n# Pyspark (supported with Spark 1.2.1 and above)\n# To configure pyspark, you need to set spark distribution's path to 'spark.home' property in Interpreter setting screen in Zeppelin GUI\n# export PYSPARK_PYTHON   # path to the python command. must be the same path on the driver(Zeppelin) and all workers.\n# export PYTHONPATH\n\nexport PYTHONPATH=\"${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.8.2.1-src.zip\"\nexport SPARK_YARN_USER_ENV=\"PYTHONPATH=${PYTHONPATH}\"\n\n## Spark interpreter options ##\n##\n# export ZEPPELIN_SPARK_USEHIVECONTEXT   # Use HiveContext instead of SQLContext if set true. true by default.\n# export ZEPPELIN_SPARK_CONCURRENTSQL   # Execute multiple SQL concurrently if set true. false by default.\n# export ZEPPELIN_SPARK_IMPORTIMPLICIT   # Import implicits, UDF collection, and sql if set true. true by default.\n# export ZEPPELIN_SPARK_MAXRESULT    # Max number of Spark SQL result to display. 1000 by default.\n# export ZEPPELIN_WEBSOCKET_MAX_TEXT_MESSAGE_SIZE   # Size in characters of the maximum text message to be received by websocket. Defaults to 1024000\n\n\n#### HBase interpreter configuration ####\n\n## To connect to HBase running on a cluster, either HBASE_HOME or HBASE_CONF_DIR must be set\n\n# export HBASE_HOME=   # (require) Under which HBase scripts and configuration should be\n# export HBASE_CONF_DIR=   # (optional) Alternatively, configuration directory can be set to point to the directory that has hbase-site.xml\n\n# export ZEPPELIN_IMPERSONATE_CMD   # Optional, when user want to run interpreter as end web user. eg) 'sudo -H -u ${ZEPPELIN_IMPERSONATE_USER} bash -c '\nexport ZEPPELIN_LIVY_HOST_URL=http://`hostname -f`:8998\nexport CLASSPATH=$CLASSPATH:/usr/lib/hdinsight-logging/*",
            "zeppelin_group" : "zeppelin",
            "zeppelin_log_dir" : "/var/log/zeppelin",
            "zeppelin_pid_dir" : "/var/run/zeppelin",
            "zeppelin_user" : "zeppelin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zeppelin-shiro-ini",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "shiro_ini_content" : "[users]\n# List of users with their password allowed to access Zeppelin.\n# To use a different strategy (LDAP / Database / ...) check the shiro doc at http://shiro.apache.org/configuration.html#Configuration-INISections\nadmin = admin, admin\nuser1 = user1, role1, role2\nuser2 = user2, role3\nuser3 = user3, role2\n\n# Sample LDAP configuration, for user Authentication, currently tested for single Realm\n[main]\n### A sample PAM configuration\n#pamRealm=org.apache.zeppelin.realm.PamRealm\n#pamRealm.service=sshd\n\n\nsessionManager = org.apache.shiro.web.session.mgt.DefaultWebSessionManager\n### If caching of user is required then uncomment below lines\ncacheManager = org.apache.shiro.cache.MemoryConstrainedCacheManager\nsecurityManager.cacheManager = $cacheManager\n\nsecurityManager.sessionManager = $sessionManager\n# 86,400,000 milliseconds = 24 hour\nsecurityManager.sessionManager.globalSessionTimeout = 86400000\nshiro.loginUrl = /api/login\n\n[roles]\nrole1 = *\nrole2 = *\nrole3 = *\nadmin = *\n\n[urls]\n# This section is used for url-based security.\n# You can secure interpreter, configuration and credential information by urls. Comment or uncomment the below urls that you want to hide.\n# anon means the access is anonymous.\n# authc means Form based Auth Security\n# To enfore security, comment the line below and uncomment the next one\n/api/version = anon\n#/api/interpreter/** = authc, roles[admin]\n#/api/configurations/** = authc, roles[admin]\n#/api/credential/** = authc, roles[admin]\n/** = anon\n#/** = authc\n"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553538046,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "ZEPPELIN",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=ZEPPELIN&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zeppelin-log4j-properties",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "log4j_properties_content" : "\nlog4j.rootLogger = INFO, dailyfile, ETW, Anonymizer, FullPIILogs\nlog4j.appender.stdout = org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout = org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=%5p [%d] ({%t} %F[%M]:%L) - %m%n\nlog4j.appender.dailyfile.DatePattern=.yyyy-MM-dd\nlog4j.appender.dailyfile.Threshold = INFO\nlog4j.appender.dailyfile = org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.dailyfile.File = ${zeppelin.log.file}\nlog4j.appender.dailyfile.layout = org.apache.log4j.PatternLayout\nlog4j.appender.dailyfile.layout.ConversionPattern=%5p [%d] ({%t} %F[%M]:%L) - %m%n\n\n#EtwLog Appender\n#sends Spark logs to customer storage account\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=sparkzeppelin\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# Anonymize Appender\n# Sends anonymized HDP service logs to our storage account\nlog4j.appender.Anonymizer.patternGroupResource=${patternGroup.filename}\nlog4j.appender.Anonymizer=com.microsoft.log4jappender.AnonymizeLogAppender\nlog4j.appender.Anonymizer.component=sparkzeppelin\nlog4j.appender.Anonymizer.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.Anonymizer.Threshold=DEBUG\nlog4j.appender.Anonymizer.logFilterResource=${logFilter.filename}\nlog4j.appender.Anonymizer.source=CentralAnonymizedLogs\nlog4j.appender.Anonymizer.OSType=Linux\n\n# Full PII log Appender\n# Sends  PII HDP service logs to our storage account\nlog4j.appender.FullPIILogs=com.microsoft.log4jappender.FullPIILogAppender\nlog4j.appender.FullPIILogs.component=sparkzeppelin\nlog4j.appender.FullPIILogs.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FullPIILogs.Threshold=DEBUG\nlog4j.appender.FullPIILogs.source=CentralFullServicePIILogs\nlog4j.appender.FullPIILogs.OSType=Linux\nlog4j.appender.FullPIILogs.SuffixHadoopEntryType=true"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zeppelin-config",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "zeppelin.anonymous.allowed" : "true",
            "zeppelin.config.fs.dir" : "file:///etc/zeppelin/conf/",
            "zeppelin.interpreter.config.upgrade" : "true",
            "zeppelin.interpreter.connect.timeout" : "30000",
            "zeppelin.interpreter.dir" : "interpreter",
            "zeppelin.interpreter.group.order" : "livy,md,sh,angular,jdbc",
            "zeppelin.interpreters" : "org.apache.zeppelin.livy.LivySparkInterpreter,org.apache.zeppelin.livy.LivyPySparkInterpreter,org.apache.zeppelin.livy.LivySparkSQLInterpreter,org.apache.zeppelin.markdown.Markdown,org.apache.zeppelin.shell.ShellInterpreter,org.apache.zeppelin.angular.AngularInterpreter,org.apache.zeppelin.jdbc.JDBCInterpreter",
            "zeppelin.notebook.dir" : "notebook",
            "zeppelin.notebook.homescreen" : " ",
            "zeppelin.notebook.homescreen.hide" : "false",
            "zeppelin.notebook.public" : "false",
            "zeppelin.notebook.s3.bucket" : "zeppelin",
            "zeppelin.notebook.s3.user" : "user",
            "zeppelin.notebook.storage" : "org.apache.zeppelin.notebook.repo.VFSNotebookRepo",
            "zeppelin.server.addr" : "0.0.0.0",
            "zeppelin.server.allowed.origins" : "*",
            "zeppelin.server.port" : "9995",
            "zeppelin.server.ssl.port" : "9995",
            "zeppelin.ssl" : "false",
            "zeppelin.ssl.client.auth" : "false",
            "zeppelin.ssl.key.manager.password" : "SECRET:zeppelin-config:2:zeppelin.ssl.key.manager.password",
            "zeppelin.ssl.keystore.password" : "SECRET:zeppelin-config:2:zeppelin.ssl.keystore.password",
            "zeppelin.ssl.keystore.path" : "conf/keystore",
            "zeppelin.ssl.keystore.type" : "JKS",
            "zeppelin.ssl.truststore.password" : "SECRET:zeppelin-config:2:zeppelin.ssl.truststore.password",
            "zeppelin.ssl.truststore.path" : "conf/truststore",
            "zeppelin.ssl.truststore.type" : "JKS",
            "zeppelin.websocket.max.text.message.size" : "1024000"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zeppelin-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "ZEPPELIN_MASTER:zeppelin",
            "content" : "\n{\n  \"input\":[\n    {\n      \"type\":\"zeppelin\",\n      \"rowtype\":\"service\",\n      \"path\":\"{{default('/configurations/zeppelin-env/zeppelin_log_dir', '/var/log/zeppelin')}}/zeppelin-zeppelin-*.log\"\n    }\n   ],\n  \"filter\":[\n    {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\n          \"type\":[\n            \"zeppelin\"\n          ]\n         }\n       },\n      \"log4j_format\":\"\",\n      \"multiline_pattern\":\"^(%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{TIMESTAMP_ISO8601:logtime}\\\\])\",\n      \"message_pattern\":\"(?m)^%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{TIMESTAMP_ISO8601:logtime}\\\\]%{SPACE}\\\\(\\\\{{\"{\"}}%{DATA:thread_name}\\\\{{\"}\"}}%{SPACE}%{JAVAFILE:file}\\\\[%{JAVAMETHOD:method}\\\\]:%{INT:line_number}\\\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n      \"post_map_values\":{\n        \"logtime\":{\n          \"map_date\":{\n            \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n          }\n         }\n       }\n     }\n   ]\n }",
            "service_name" : "Zeppelin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zeppelin-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "zeppelin.executor.instances" : "2",
            "zeppelin.executor.mem" : "512m",
            "zeppelin.server.kerberos.keytab" : "",
            "zeppelin.server.kerberos.principal" : "",
            "zeppelin.spark.jar.dir" : "/apps/zeppelin",
            "zeppelin_env_content" : "# export JAVA_HOME=\nexport JAVA_HOME={{java64_home}}\n# export MASTER=  # Spark master url. eg. spark://master_addr:7077. Leave empty if you want to use local mode.\nexport MASTER=yarn-client\nexport SPARK_YARN_JAR={{spark_jar}}\n# export ZEPPELIN_JAVA_OPTS   # Additional jvm options. for example, export ZEPPELIN_JAVA_OPTS=\"-Dspark.executor.memory=8g -Dspark.cores.max=16\"\nexport ZEPPELIN_JAVA_OPTS=\"-Dhdp.version={{full_stack_version}} -Dspark.executor.memory={{executor_mem}} -Dspark.executor.instances={{executor_instances}} -Dspark.yarn.queue={{spark_queue}}\"\n# export ZEPPELIN_MEM   # Zeppelin jvm mem options Default -Xms1024m -Xmx1024m -XX:MaxPermSize=512m\n# export ZEPPELIN_INTP_MEM   # zeppelin interpreter process jvm mem options. Default -Xms1024m -Xmx1024m -XX:MaxPermSize=512m\n# export ZEPPELIN_INTP_JAVA_OPTS  # zeppelin interpreter process jvm options.\n# export ZEPPELIN_SSL_PORT  # ssl port (used when ssl environment variable is set to true)\n\n# export ZEPPELIN_LOG_DIR  # Where log files are stored.  PWD by default.\nexport ZEPPELIN_LOG_DIR={{zeppelin_log_dir}}\n# export ZEPPELIN_PID_DIR  # The pid files are stored. ${ZEPPELIN_HOME}/run by default.\nexport ZEPPELIN_PID_DIR={{zeppelin_pid_dir}}\n# export ZEPPELIN_WAR_TEMPDIR # The location of jetty temporary directory.\n# export ZEPPELIN_NOTEBOOK_DIR # Where notebook saved\n# export ZEPPELIN_NOTEBOOK_HOMESCREEN   # Id of notebook to be displayed in homescreen. ex) 2A94M5J1Z\n# export ZEPPELIN_NOTEBOOK_HOMESCREEN_HIDE  # hide homescreen notebook from list when this value set to \"true\". default \"false\"\n# export ZEPPELIN_NOTEBOOK_S3_BUCKET # Bucket where notebook saved\n# export ZEPPELIN_NOTEBOOK_S3_ENDPOINT  # Endpoint of the bucket\n# export ZEPPELIN_NOTEBOOK_S3_USER  # User in bucket where notebook saved. For example bucket/user/notebook/2A94M5J1Z/note.json\n# export ZEPPELIN_IDENT_STRING   # A string representing this instance of zeppelin. $USER by default.\n# export ZEPPELIN_NICENESS   # The scheduling priority for daemons. Defaults to 0.\n# export ZEPPELIN_INTERPRETER_LOCALREPO   # Local repository for interpreter's additional dependency loading\n# export ZEPPELIN_NOTEBOOK_STORAGE   # Refers to pluggable notebook storage class, can have two classes simultaneously with a sync between them (e.g. local and remote).\n# export ZEPPELIN_NOTEBOOK_ONE_WAY_SYNC  # If there are multiple notebook storages, should we treat the first one as the only source of truth?\n# export ZEPPELIN_NOTEBOOK_PUBLIC  # Make notebook public by default when created, private otherwise\nexport ZEPPELIN_INTP_CLASSPATH_OVERRIDES=\"{{external_dependency_conf}}\"\n\n#### Spark interpreter configuration ####\n\n## Use provided spark installation ##\n## defining SPARK_HOME makes Zeppelin run spark interpreter process using spark-submit\n##\n# export SPARK_HOME   # (required) When it is defined, load it instead of Zeppelin embedded Spark libraries\n#export SPARK_HOME={{spark_home}}\n# export SPARK_SUBMIT_OPTIONS   # (optional) extra options to pass to spark submit. eg) \"--driver-memory 512M --executor-memory 1G\".\n# export SPARK_APP_NAME   # (optional) The name of spark application.\n\n## Use embedded spark binaries ##\n## without SPARK_HOME defined, Zeppelin still able to run spark interpreter process using embedded spark binaries.\n## however, it is not encouraged when you can define SPARK_HOME\n##\n# Options read in YARN client mode\n# export HADOOP_CONF_DIR    # yarn-site.xml is located in configuration directory in HADOOP_CONF_DIR.\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\n# Pyspark (supported with Spark 1.2.1 and above)\n# To configure pyspark, you need to set spark distribution's path to 'spark.home' property in Interpreter setting screen in Zeppelin GUI\n# export PYSPARK_PYTHON   # path to the python command. must be the same path on the driver(Zeppelin) and all workers.\n# export PYTHONPATH\n\nexport PYTHONPATH=\"${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.8.2.1-src.zip\"\nexport SPARK_YARN_USER_ENV=\"PYTHONPATH=${PYTHONPATH}\"\n\n## Spark interpreter options ##\n##\n# export ZEPPELIN_SPARK_USEHIVECONTEXT   # Use HiveContext instead of SQLContext if set true. true by default.\n# export ZEPPELIN_SPARK_CONCURRENTSQL   # Execute multiple SQL concurrently if set true. false by default.\n# export ZEPPELIN_SPARK_IMPORTIMPLICIT   # Import implicits, UDF collection, and sql if set true. true by default.\n# export ZEPPELIN_SPARK_MAXRESULT    # Max number of Spark SQL result to display. 1000 by default.\n# export ZEPPELIN_WEBSOCKET_MAX_TEXT_MESSAGE_SIZE   # Size in characters of the maximum text message to be received by websocket. Defaults to 1024000\n\n\n#### HBase interpreter configuration ####\n\n## To connect to HBase running on a cluster, either HBASE_HOME or HBASE_CONF_DIR must be set\n\n# export HBASE_HOME=   # (require) Under which HBase scripts and configuration should be\n# export HBASE_CONF_DIR=   # (optional) Alternatively, configuration directory can be set to point to the directory that has hbase-site.xml\n\n# export ZEPPELIN_IMPERSONATE_CMD   # Optional, when user want to run interpreter as end web user. eg) 'sudo -H -u ${ZEPPELIN_IMPERSONATE_USER} bash -c '\nexport ZEPPELIN_LIVY_HOST_URL=http://`hostname -f`:8998\nexport CLASSPATH=$CLASSPATH:/usr/lib/hdinsight-logging/*",
            "zeppelin_group" : "zeppelin",
            "zeppelin_log_dir" : "/var/log/zeppelin",
            "zeppelin_pid_dir" : "/var/run/zeppelin",
            "zeppelin_user" : "zeppelin"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zeppelin-shiro-ini",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "shiro_ini_content" : "[users]\n# List of users with their password allowed to access Zeppelin.\n# To use a different strategy (LDAP / Database / ...) check the shiro doc at http://shiro.apache.org/configuration.html#Configuration-INISections\nadmin = admin, admin\nuser1 = user1, role1, role2\nuser2 = user2, role3\nuser3 = user3, role2\n\n# Sample LDAP configuration, for user Authentication, currently tested for single Realm\n[main]\n### A sample PAM configuration\n#pamRealm=org.apache.zeppelin.realm.PamRealm\n#pamRealm.service=sshd\n\n\nsessionManager = org.apache.shiro.web.session.mgt.DefaultWebSessionManager\n### If caching of user is required then uncomment below lines\ncacheManager = org.apache.shiro.cache.MemoryConstrainedCacheManager\nsecurityManager.cacheManager = $cacheManager\n\nsecurityManager.sessionManager = $sessionManager\n# 86,400,000 milliseconds = 24 hour\nsecurityManager.sessionManager.globalSessionTimeout = 86400000\nshiro.loginUrl = /api/login\n\n[roles]\nrole1 = *\nrole2 = *\nrole3 = *\nadmin = *\n\n[urls]\n# This section is used for url-based security.\n# You can secure interpreter, configuration and credential information by urls. Comment or uncomment the below urls that you want to hide.\n# anon means the access is anonymous.\n# authc means Form based Auth Security\n# To enfore security, comment the line below and uncomment the next one\n/api/version = anon\n#/api/interpreter/** = authc, roles[admin]\n#/api/configurations/** = authc, roles[admin]\n#/api/credential/** = authc, roles[admin]\n/** = anon\n#/** = authc"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553554453,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "ZEPPELIN",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=ZOOKEEPER&service_config_version=1",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zoo.cfg",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "autopurge.purgeInterval" : "24",
            "autopurge.snapRetainCount" : "30",
            "clientPort" : "2181",
            "dataDir" : "/hadoop/zookeeper",
            "initLimit" : "10",
            "syncLimit" : "5",
            "tickTime" : "3000"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zookeeper-env",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\nexport JAVA_HOME={{java64_home}}\nexport ZOOKEEPER_HOME={{zk_home}}\nexport ZOO_LOG_DIR={{zk_log_dir}}\nexport ZOOPIDFILE={{zk_pid_file}}\nexport SERVER_JVMFLAGS=\"{{zk_server_heapsize}}  -Dwhitelist.filename=NA -Dcomponent=zookeeper-server\"\nexport JAVA=$JAVA_HOME/bin/java\nexport CLASSPATH=$CLASSPATH:/usr/share/zookeeper/*\nexport CLIENT_JVMFLAGS=\"-Dwhitelist.filename=NA -Dcomponent=zookeeper-client\"\n\n{% if security_enabled %}\nexport SERVER_JVMFLAGS=\"$SERVER_JVMFLAGS -Djava.security.auth.login.config={{zk_server_jaas_file}} \"\nexport CLIENT_JVMFLAGS=\"$CLIENT_JVMFLAGS -Djava.security.auth.login.config={{zk_client_jaas_file}}\"\n{% endif %}",
            "zk_log_dir" : "/var/log/zookeeper",
            "zk_pid_dir" : "/var/run/zookeeper",
            "zk_server_heapsize" : "1024",
            "zk_user" : "zookeeper"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zookeeper-log4j",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "content" : "\n#\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n#\n#\n\n#\n# ZooKeeper Logging Configuration\n#\n\n# DEFAULT: console appender only\nlog4j.rootLogger=INFO, CONSOLE, ETW, FilterLog\n\n# Example with rolling log file\n#log4j.rootLogger=DEBUG, CONSOLE, ROLLINGFILE\n\n# Example with rolling log file and tracing\n#log4j.rootLogger=TRACE, CONSOLE, ROLLINGFILE, TRACEFILE\n\n#\n# Log INFO level and above messages to the console\n#\nlog4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\nlog4j.appender.CONSOLE.Threshold=INFO\nlog4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}@%L] - %m%n\n\n#\n# Add ROLLINGFILE to rootLogger to get log file output\n#    Log DEBUG level and above messages to a log file\nlog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender\nlog4j.appender.ROLLINGFILE.Threshold=DEBUG\nlog4j.appender.ROLLINGFILE.File=zookeeper.log\n\n# Max log file size of 10MB\nlog4j.appender.ROLLINGFILE.MaxFileSize=10MB\n# uncomment the next line to limit number of backup files\n#log4j.appender.ROLLINGFILE.MaxBackupIndex=10\n\nlog4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.ROLLINGFILE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}@%L] - %m%n\n\n\n#\n# Add TRACEFILE to rootLogger to get log file output\n#  Log DEBUG level and above messages to a log file\nlog4j.appender.TRACEFILE=org.apache.log4j.FileAppender\nlog4j.appender.TRACEFILE.Threshold=TRACE\nlog4j.appender.TRACEFILE.File=zookeeper_trace.log\n\nlog4j.appender.TRACEFILE.layout=org.apache.log4j.PatternLayout\n### Notice we are including log4j's NDC here (%x)\nlog4j.appender.TRACEFILE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}@%L][%x] - %m%n\n\n\n#EtwLog Appender\n\n#sends HDP service logs to customer storage account\n\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=${component}\nlog4j.appender.FilterLog.whitelistFileName=${whitelist.filename}\nlog4j.appender.FilterLog.OSType=Linux\n",
            "zookeeper_log_max_backup_size" : "10",
            "zookeeper_log_number_of_backup_files" : "10"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zookeeper-logsearch-conf",
          "tag" : "INITIAL",
          "version" : 1,
          "properties" : {
            "component_mappings" : "ZOOKEEPER_SERVER:zookeeper",
            "content" : "\n{\n  \"input\":[\n    {\n     \"type\":\"zookeeper\",\n     \"rowtype\":\"service\",\n     \"path\":\"{{default('/configurations/zookeeper-env/zk_log_dir', '/var/log/zookeeper')}}/zookeeper*.log\"\n    }\n  ],\n  \"filter\":[\n   {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\"type\":[\"zookeeper\"]}\n      },\n     \"log4j_format\":\"%d{ISO8601} - %-5p [%t:%C{1}@%L] - %m%n\",\n     \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n     \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}-%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\@%{INT:line_number}\\\\]%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n     \"post_map_values\": {\n       \"logtime\": {\n         \"map_date\":{\n           \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n         }\n       }\n     }\n    }\n   ]\n}\n    ",
            "service_name" : "Zookeeper"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553520659,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : false,
      "service_config_version" : 1,
      "service_config_version_note" : null,
      "service_name" : "ZOOKEEPER",
      "stack_id" : "HDP-2.6",
      "user" : "hdinsightwatchdog"
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/configurations/service_config_versions?service_name=ZOOKEEPER&service_config_version=2",
      "cluster_name" : "sparkpoc",
      "configurations" : [
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zoo.cfg",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "autopurge.purgeInterval" : "24",
            "autopurge.snapRetainCount" : "30",
            "clientPort" : "2181",
            "dataDir" : "/hadoop/zookeeper",
            "initLimit" : "10",
            "syncLimit" : "5",
            "tickTime" : "3000"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zookeeper-env",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\nexport JAVA_HOME={{java64_home}}\nexport ZOOKEEPER_HOME={{zk_home}}\nexport ZOO_LOG_DIR={{zk_log_dir}}\nexport ZOOPIDFILE={{zk_pid_file}}\nexport SERVER_JVMFLAGS=\"{{zk_server_heapsize}}  -Dwhitelist.filename=NA -Dcomponent=zookeeper-server\"\nexport JAVA=$JAVA_HOME/bin/java\nexport CLASSPATH=$CLASSPATH:/usr/share/zookeeper/*\nexport CLIENT_JVMFLAGS=\"-Dwhitelist.filename=NA -Dcomponent=zookeeper-client\"\n\n{% if security_enabled %}\nexport SERVER_JVMFLAGS=\"$SERVER_JVMFLAGS -Djava.security.auth.login.config={{zk_server_jaas_file}} \"\nexport CLIENT_JVMFLAGS=\"$CLIENT_JVMFLAGS -Djava.security.auth.login.config={{zk_client_jaas_file}}\"\n{% endif %}",
            "zk_log_dir" : "/var/log/zookeeper",
            "zk_pid_dir" : "/var/run/zookeeper",
            "zk_server_heapsize" : "1024m",
            "zk_user" : "zookeeper"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zookeeper-log4j",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "content" : "\n#\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n#\n#\n\n#\n# ZooKeeper Logging Configuration\n#\n\n# DEFAULT: console appender only\nlog4j.rootLogger=INFO, CONSOLE, ETW, FilterLog\n\n# Example with rolling log file\n#log4j.rootLogger=DEBUG, CONSOLE, ROLLINGFILE\n\n# Example with rolling log file and tracing\n#log4j.rootLogger=TRACE, CONSOLE, ROLLINGFILE, TRACEFILE\n\n#\n# Log INFO level and above messages to the console\n#\nlog4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\nlog4j.appender.CONSOLE.Threshold=INFO\nlog4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}@%L] - %m%n\n\n#\n# Add ROLLINGFILE to rootLogger to get log file output\n#    Log DEBUG level and above messages to a log file\nlog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender\nlog4j.appender.ROLLINGFILE.Threshold=DEBUG\nlog4j.appender.ROLLINGFILE.File=zookeeper.log\n\n# Max log file size of 10MB\nlog4j.appender.ROLLINGFILE.MaxFileSize=10MB\n# uncomment the next line to limit number of backup files\n#log4j.appender.ROLLINGFILE.MaxBackupIndex=10\n\nlog4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.ROLLINGFILE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}@%L] - %m%n\n\n\n#\n# Add TRACEFILE to rootLogger to get log file output\n#  Log DEBUG level and above messages to a log file\nlog4j.appender.TRACEFILE=org.apache.log4j.FileAppender\nlog4j.appender.TRACEFILE.Threshold=TRACE\nlog4j.appender.TRACEFILE.File=zookeeper_trace.log\n\nlog4j.appender.TRACEFILE.layout=org.apache.log4j.PatternLayout\n### Notice we are including log4j's NDC here (%x)\nlog4j.appender.TRACEFILE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}@%L][%x] - %m%n\n\n\n#EtwLog Appender\n\n#sends HDP service logs to customer storage account\n\nlog4j.appender.ETW=com.microsoft.log4jappender.EtwAppender\nlog4j.appender.ETW.source=HadoopServiceLog\nlog4j.appender.ETW.component=${component}\nlog4j.appender.ETW.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.ETW.OSType=Linux\n\n# FilterLog Appender\n# Sends filtered HDP service logs to our storage account\nlog4j.appender.FilterLog=com.microsoft.log4jappender.FilterLogAppender\nlog4j.appender.FilterLog.layout=org.apache.log4j.TTCCLayout\nlog4j.appender.FilterLog.source=CentralFilteredHadoopServiceLogs\nlog4j.appender.FilterLog.component=${component}\nlog4j.appender.FilterLog.whitelistFileName=${whitelist.filename}\nlog4j.appender.FilterLog.OSType=Linux",
            "zookeeper_log_max_backup_size" : "10",
            "zookeeper_log_number_of_backup_files" : "10"
          },
          "properties_attributes" : { }
        },
        {
          "Config" : {
            "cluster_name" : "sparkpoc",
            "stack_id" : "HDP-2.6"
          },
          "type" : "zookeeper-logsearch-conf",
          "tag" : "TOPOLOGY_RESOLVED",
          "version" : 2,
          "properties" : {
            "component_mappings" : "ZOOKEEPER_SERVER:zookeeper",
            "content" : "\n{\n  \"input\":[\n    {\n     \"type\":\"zookeeper\",\n     \"rowtype\":\"service\",\n     \"path\":\"{{default('/configurations/zookeeper-env/zk_log_dir', '/var/log/zookeeper')}}/zookeeper*.log\"\n    }\n  ],\n  \"filter\":[\n   {\n      \"filter\":\"grok\",\n      \"conditions\":{\n        \"fields\":{\"type\":[\"zookeeper\"]}\n      },\n     \"log4j_format\":\"%d{ISO8601} - %-5p [%t:%C{1}@%L] - %m%n\",\n     \"multiline_pattern\":\"^(%{TIMESTAMP_ISO8601:logtime})\",\n     \"message_pattern\":\"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}-%{SPACE}%{LOGLEVEL:level}%{SPACE}\\\\[%{DATA:thread_name}\\\\@%{INT:line_number}\\\\]%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}\",\n     \"post_map_values\": {\n       \"logtime\": {\n         \"map_date\":{\n           \"target_date_pattern\":\"yyyy-MM-dd HH:mm:ss,SSS\"\n         }\n       }\n     }\n    }\n   ]\n}",
            "service_name" : "Zookeeper"
          },
          "properties_attributes" : { }
        }
      ],
      "createtime" : 1535553549890,
      "group_id" : -1,
      "group_name" : "Default",
      "hosts" : [ ],
      "is_cluster_compatible" : true,
      "is_current" : true,
      "service_config_version" : 2,
      "service_config_version_note" : null,
      "service_name" : "ZOOKEEPER",
      "stack_id" : "HDP-2.6",
      "user" : "internal"
    }
  ],
  "alert_definitions" : [
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/1",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 1,
        "label" : "Percent ZooKeeper Servers Available",
        "name" : "zookeeper_server_process_percent"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/2",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 2,
        "label" : "ZooKeeper Server Process",
        "name" : "zookeeper_server_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/3",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 3,
        "label" : "NameNode Host CPU Utilization",
        "name" : "namenode_cpu"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/4",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 4,
        "label" : "Secondary NameNode Process",
        "name" : "secondary_namenode_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/5",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 5,
        "label" : "HDFS Pending Deletion Blocks",
        "name" : "namenode_hdfs_pending_deletion_blocks"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/6",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 6,
        "label" : "NameNode Client RPC Queue Latency (Daily)",
        "name" : "namenode_client_rpc_queue_latency_daily"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/7",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 7,
        "label" : "NameNode High Availability Health",
        "name" : "namenode_ha_health"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/8",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 8,
        "label" : "DataNode Heap Usage",
        "name" : "datanode_heap_usage"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/9",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 9,
        "label" : "DataNode Health Summary",
        "name" : "datanode_health_summary"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/10",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 10,
        "label" : "DataNode Unmounted Data Dir",
        "name" : "datanode_unmounted_data_dir"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/11",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 11,
        "label" : "NameNode Service RPC Queue Latency (Daily)",
        "name" : "namenode_service_rpc_queue_latency_daily"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/12",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 12,
        "label" : "DataNode Process",
        "name" : "datanode_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/13",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 13,
        "label" : "NameNode Client RPC Processing Latency (Daily)",
        "name" : "namenode_client_rpc_processing_latency_daily"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/14",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 14,
        "label" : "NameNode Blocks Health",
        "name" : "namenode_hdfs_blocks_health"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/15",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 15,
        "label" : "NameNode Web UI",
        "name" : "namenode_webui"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/16",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 16,
        "label" : "DataNode Web UI",
        "name" : "datanode_webui"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/17",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 17,
        "label" : "DataNode Storage",
        "name" : "datanode_storage"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/18",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 18,
        "label" : "NameNode Service RPC Processing Latency (Hourly)",
        "name" : "namenode_service_rpc_processing_latency_hourly"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/19",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 19,
        "label" : "NFS Gateway Process",
        "name" : "nfsgateway_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/20",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 20,
        "label" : "Percent JournalNodes Available",
        "name" : "journalnode_process_percent"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/21",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 21,
        "label" : "HDFS Storage Capacity Usage (Daily)",
        "name" : "namenode_increase_in_storage_capacity_usage_daily"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/22",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 22,
        "label" : "NameNode Client RPC Queue Latency (Hourly)",
        "name" : "namenode_client_rpc_queue_latency_hourly"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/23",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 23,
        "label" : "Percent DataNodes With Available Space",
        "name" : "datanode_storage_percent"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/24",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 24,
        "label" : "NameNode Service RPC Processing Latency (Daily)",
        "name" : "namenode_service_rpc_processing_latency_daily"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/25",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 25,
        "label" : "HDFS Upgrade Finalized State",
        "name" : "upgrade_finalized_state"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/26",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 26,
        "label" : "NameNode Client RPC Processing Latency (Hourly)",
        "name" : "namenode_client_rpc_processing_latency_hourly"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/27",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 27,
        "label" : "HDFS Storage Capacity Usage (Weekly)",
        "name" : "namenode_increase_in_storage_capacity_usage_weekly"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/28",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 28,
        "label" : "JournalNode Web UI",
        "name" : "journalnode_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/29",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 29,
        "label" : "ZooKeeper Failover Controller Process",
        "name" : "hdfs_zookeeper_failover_controller_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/30",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 30,
        "label" : "NameNode Directory Status",
        "name" : "namenode_directory_status"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/31",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 31,
        "label" : "NameNode Service RPC Queue Latency (Hourly)",
        "name" : "namenode_service_rpc_queue_latency_hourly"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/32",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 32,
        "label" : "NameNode Heap Usage (Weekly)",
        "name" : "increase_nn_heap_usage_weekly"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/33",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 33,
        "label" : "Percent DataNodes Available",
        "name" : "datanode_process_percent"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/34",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 34,
        "label" : "HDFS Capacity Utilization",
        "name" : "namenode_hdfs_capacity_utilization"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/35",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 35,
        "label" : "NameNode RPC Latency",
        "name" : "namenode_rpc_latency"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/36",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 36,
        "label" : "NameNode Last Checkpoint",
        "name" : "namenode_last_checkpoint"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/37",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 37,
        "label" : "NameNode Heap Usage (Daily)",
        "name" : "increase_nn_heap_usage_daily"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/38",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 38,
        "label" : "Spark2 Livy Server",
        "name" : "livy2_server_status"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/39",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 39,
        "label" : "Spark2 History Server",
        "name" : "SPARK2_JOBHISTORYSERVER_PROCESS"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/40",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 40,
        "label" : "Spark2 Thrift Server",
        "name" : "spark2_thriftserver_status"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/41",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 41,
        "label" : "Zeppelin Server Status",
        "name" : "zeppelin_server_status"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/42",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 42,
        "label" : "Percent NodeManagers Available",
        "name" : "yarn_nodemanager_webui_percent"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/43",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 43,
        "label" : "NodeManager Health",
        "name" : "yarn_nodemanager_health"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/44",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 44,
        "label" : "ResourceManager Web UI",
        "name" : "yarn_resourcemanager_webui"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/45",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 45,
        "label" : "ResourceManager CPU Utilization",
        "name" : "yarn_resourcemanager_cpu"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/46",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 46,
        "label" : "NodeManager Web UI",
        "name" : "yarn_nodemanager_webui"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/47",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 47,
        "label" : "ResourceManager RPC Latency",
        "name" : "yarn_resourcemanager_rpc_latency"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/48",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 48,
        "label" : "NodeManager Health Summary",
        "name" : "nodemanager_health_summary"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/49",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 49,
        "label" : "App Timeline Web UI",
        "name" : "yarn_app_timeline_server_webui"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/50",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 50,
        "label" : "Hive Metastore Process",
        "name" : "hive_metastore_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/51",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 51,
        "label" : "HiveServer2 Interactive Process",
        "name" : "hive_server_interactive_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/52",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 52,
        "label" : "LLAP Application",
        "name" : "llap_application"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/53",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 53,
        "label" : "HiveServer2 Process",
        "name" : "hive_server_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/54",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 54,
        "label" : "WebHCat Server Status",
        "name" : "hive_webhcat_server_status"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/55",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 55,
        "label" : "Metrics Monitor Status",
        "name" : "ams_metrics_monitor_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/56",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 56,
        "label" : "Metrics Collector Process",
        "name" : "ams_metrics_collector_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/57",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 57,
        "label" : "Metrics Collector - HBase Master Process",
        "name" : "ams_metrics_collector_hbase_master_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/58",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 58,
        "label" : "Metrics Collector - HBase Master CPU Utilization",
        "name" : "ams_metrics_collector_hbase_master_cpu"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/59",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 59,
        "label" : "Percent Metrics Monitors Available",
        "name" : "metrics_monitor_process_percent"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/60",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 60,
        "label" : "Metrics Collector - Auto-Restart Status",
        "name" : "ams_metrics_collector_autostart"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/61",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 61,
        "label" : "Grafana Web UI",
        "name" : "grafana_webui"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/62",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 62,
        "label" : "IOCache Data Transfer Server Status",
        "name" : "cache_data_transfer_server_status"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/63",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 63,
        "label" : "IOCache Metadata Server Status",
        "name" : "cache_metadata_server_status"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/64",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 64,
        "label" : "Oozie Server Web UI",
        "name" : "oozie_server_webui"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/65",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 65,
        "label" : "Oozie Server Status",
        "name" : "oozie_server_status"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/66",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 66,
        "label" : "History Server Process",
        "name" : "mapreduce_history_server_process"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/67",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 67,
        "label" : "History Server Web UI",
        "name" : "mapreduce_history_server_webui"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/68",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 68,
        "label" : "History Server RPC Latency",
        "name" : "mapreduce_history_server_rpc_latency"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/69",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 69,
        "label" : "History Server CPU Utilization",
        "name" : "mapreduce_history_server_cpu"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/70",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 70,
        "label" : "Ambari Agent Distro/Conf Select Versions",
        "name" : "ambari_agent_version_select"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/71",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 71,
        "label" : "Host Disk Usage",
        "name" : "ambari_agent_disk_usage"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/72",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 72,
        "label" : "Ambari Agent Heartbeat",
        "name" : "ambari_server_agent_heartbeat"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/73",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 73,
        "label" : "Ambari Server Performance",
        "name" : "ambari_server_performance"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/74",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 74,
        "label" : "Component Version",
        "name" : "ambari_server_component_version"
      }
    },
    {
      "href" : "http://hn1-sparkp.pk3tke12z5uejpo3re1mqa2wyf.fx.internal.cloudapp.net:8080/api/v1/clusters/sparkpoc/alert_definitions/75",
      "AlertDefinition" : {
        "cluster_name" : "sparkpoc",
        "id" : 75,
        "label" : "Ambari Server Alerts",
        "name" : "ambari_server_stale_alerts"
      }
    }
  ],
  "artifacts" : [ ]
}